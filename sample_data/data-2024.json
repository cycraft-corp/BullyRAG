[
    {
        "title": "GroupContrast: Semantic-aware Self-supervised Representation Learning\n  for 3D Understanding",
        "published_time": "2024-03-14T17:59:59Z",
        "abstract": "  Self-supervised 3D representation learning aims to learn effective\nrepresentations from large-scale unlabeled point clouds. Most existing\napproaches adopt point discrimination as the pretext task, which assigns\nmatched points in two distinct views as positive pairs and unmatched points as\nnegative pairs. However, this approach often results in semantically identical\npoints having dissimilar representations, leading to a high number of false\nnegatives and introducing a \"semantic conflict\" problem. To address this issue,\nwe propose GroupContrast, a novel approach that combines segment grouping and\nsemantic-aware contrastive learning. Segment grouping partitions points into\nsemantically meaningful regions, which enhances semantic coherence and provides\nsemantic guidance for the subsequent contrastive representation learning.\nSemantic-aware contrastive learning augments the semantic information extracted\nfrom segment grouping and helps to alleviate the issue of \"semantic conflict\".\nWe conducted extensive experiments on multiple 3D scene understanding tasks.\nThe results demonstrate that GroupContrast learns semantically meaningful\nrepresentations and achieves promising transfer learning performance.\n"
    },
    {
        "title": "SCP-Diff: Photo-Realistic Semantic Image Synthesis with\n  Spatial-Categorical Joint Prior",
        "published_time": "2024-03-14T17:59:55Z",
        "abstract": "  Semantic image synthesis (SIS) shows good promises for sensor simulation.\nHowever, current best practices in this field, based on GANs, have not yet\nreached the desired level of quality. As latent diffusion models make\nsignificant strides in image generation, we are prompted to evaluate\nControlNet, a notable method for its dense control capabilities. Our\ninvestigation uncovered two primary issues with its results: the presence of\nweird sub-structures within large semantic areas and the misalignment of\ncontent with the semantic mask. Through empirical study, we pinpointed the\ncause of these problems as a mismatch between the noised training data\ndistribution and the standard normal prior applied at the inference stage. To\naddress this challenge, we developed specific noise priors for SIS,\nencompassing spatial, categorical, and a novel spatial-categorical joint prior\nfor inference. This approach, which we have named SCP-Diff, has yielded\nexceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on\nADE20K.The code and models can be accessed via the project page.\n"
    },
    {
        "title": "GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary\n  Robotic Grasping",
        "published_time": "2024-03-14T17:59:46Z",
        "abstract": "  Constructing a 3D scene capable of accommodating open-ended language queries,\nis a pivotal pursuit, particularly within the domain of robotics. Such\ntechnology facilitates robots in executing object manipulations based on human\nlanguage directives. To tackle this challenge, some research efforts have been\ndedicated to the development of language-embedded implicit fields. However,\nimplicit fields (e.g. NeRF) encounter limitations due to the necessity of\nprocessing a large number of input views for reconstruction, coupled with their\ninherent inefficiencies in inference. Thus, we present the GaussianGrasper,\nwhich utilizes 3D Gaussian Splatting to explicitly represent the scene as a\ncollection of Gaussian primitives. Our approach takes a limited set of RGB-D\nviews and employs a tile-based splatting technique to create a feature field.\nIn particular, we propose an Efficient Feature Distillation (EFD) module that\nemploys contrastive learning to efficiently and accurately distill language\nembeddings derived from foundational models. With the reconstructed geometry of\nthe Gaussian field, our method enables the pre-trained grasping model to\ngenerate collision-free grasp pose candidates. Furthermore, we propose a\nnormal-guided grasp module to select the best grasp pose. Through comprehensive\nreal-world experiments, we demonstrate that GaussianGrasper enables robots to\naccurately query and grasp objects with language instructions, providing a new\nsolution for language-guided manipulation tasks. Data and codes can be\navailable at https://github.com/MrSecant/GaussianGrasper.\n"
    },
    {
        "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models",
        "published_time": "2024-03-14T17:59:14Z",
        "abstract": "  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.\n"
    },
    {
        "title": "OneTracker: Unifying Visual Object Tracking with Foundation Models and\n  Efficient Tuning",
        "published_time": "2024-03-14T17:59:13Z",
        "abstract": "  Visual object tracking aims to localize the target object of each frame based\non its initial appearance in the first frame. Depending on the input modility,\ntracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and\nRGB+D) tracking. Despite the different input modalities, the core aspect of\ntracking is the temporal matching. Based on this common ground, we present a\ngeneral framework to unify various tracking tasks, termed as OneTracker.\nOneTracker first performs a large-scale pre-training on a RGB tracker called\nFoundation Tracker. This pretraining phase equips the Foundation Tracker with a\nstable ability to estimate the location of the target object. Then we regard\nother modality information as prompt and build Prompt Tracker upon Foundation\nTracker. Through freezing the Foundation Tracker and only adjusting some\nadditional trainable parameters, Prompt Tracker inhibits the strong\nlocalization ability from Foundation Tracker and achieves parameter-efficient\nfinetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of\nour general framework OneTracker, which is consisted of Foundation Tracker and\nPrompt Tracker, we conduct extensive experiments on 6 popular tracking tasks\nacross 11 benchmarks and our OneTracker outperforms other models and achieves\nstate-of-the-art performance.\n"
    },
    {
        "title": "Holo-Relighting: Controllable Volumetric Portrait Relighting from a\n  Single Image",
        "published_time": "2024-03-14T17:58:56Z",
        "abstract": "  At the core of portrait photography is the search for ideal lighting and\nviewpoint. The process often requires advanced knowledge in photography and an\nelaborate studio setup. In this work, we propose Holo-Relighting, a volumetric\nrelighting method that is capable of synthesizing novel viewpoints, and novel\nlighting from a single image. Holo-Relighting leverages the pretrained 3D GAN\n(EG3D) to reconstruct geometry and appearance from an input portrait as a set\nof 3D-aware features. We design a relighting module conditioned on a given\nlighting to process these features, and predict a relit 3D representation in\nthe form of a tri-plane, which can render to an arbitrary viewpoint through\nvolume rendering. Besides viewpoint and lighting control, Holo-Relighting also\ntakes the head pose as a condition to enable head-pose-dependent lighting\neffects. With these novel designs, Holo-Relighting can generate complex\nnon-Lambertian lighting effects (e.g., specular highlights and cast shadows)\nwithout using any explicit physical lighting priors. We train Holo-Relighting\nwith data captured with a light stage, and propose two data-rendering\ntechniques to improve the data quality for training the volumetric relighting\nsystem. Through quantitative and qualitative experiments, we demonstrate\nHolo-Relighting can achieve state-of-the-arts relighting quality with better\nphotorealism, 3D consistency and controllability.\n"
    },
    {
        "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
        "published_time": "2024-03-14T17:58:41Z",
        "abstract": "  Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.\n"
    },
    {
        "title": "Generalized Predictive Model for Autonomous Driving",
        "published_time": "2024-03-14T17:58:33Z",
        "abstract": "  In this paper, we introduce the first large-scale video prediction model in\nthe autonomous driving discipline. To eliminate the restriction of high-cost\ndata collection and empower the generalization ability of our model, we acquire\nmassive data from the web and pair it with diverse and high-quality text\ndescriptions. The resultant dataset accumulates over 2000 hours of driving\nvideos, spanning areas all over the world with diverse weather conditions and\ntraffic scenarios. Inheriting the merits from recent latent diffusion models,\nour model, dubbed GenAD, handles the challenging dynamics in driving scenes\nwith novel temporal reasoning blocks. We showcase that it can generalize to\nvarious unseen driving datasets in a zero-shot manner, surpassing general or\ndriving-specific video prediction counterparts. Furthermore, GenAD can be\nadapted into an action-conditioned prediction model or a motion planner,\nholding great potential for real-world driving applications.\n"
    },
    {
        "title": "Video Mamba Suite: State Space Model as a Versatile Alternative for\n  Video Understanding",
        "published_time": "2024-03-14T17:57:07Z",
        "abstract": "  Understanding videos is one of the fundamental directions in computer vision\nresearch, with extensive efforts dedicated to exploring various architectures\nsuch as RNN, 3D CNN, and Transformers. The newly proposed architecture of state\nspace model, e.g., Mamba, shows promising traits to extend its success in long\nsequence modeling to video modeling. To assess whether Mamba can be a viable\nalternative to Transformers in the video understanding domain, in this work, we\nconduct a comprehensive set of studies, probing different roles Mamba can play\nin modeling videos, while investigating diverse tasks where Mamba could exhibit\nsuperiority. We categorize Mamba into four roles for modeling videos, deriving\na Video Mamba Suite composed of 14 models/modules, and evaluating them on 12\nvideo understanding tasks. Our extensive experiments reveal the strong\npotential of Mamba on both video-only and video-language tasks while showing\npromising efficiency-performance trade-offs. We hope this work could provide\nvaluable data points and insights for future research on video understanding.\nCode is public: https://github.com/OpenGVLab/video-mamba-suite.\n"
    },
    {
        "title": "Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation",
        "published_time": "2024-03-14T17:57:04Z",
        "abstract": "  Recent years have witnessed the strong power of 3D generation models, which\noffer a new level of creative flexibility by allowing users to guide the 3D\ncontent generation process through a single image or natural language. However,\nit remains challenging for existing 3D generation methods to create\nsubject-driven 3D content across diverse prompts. In this paper, we introduce a\nnovel 3D customization method, dubbed Make-Your-3D that can personalize\nhigh-fidelity and consistent 3D content from only a single image of a subject\nwith text description within 5 minutes. Our key insight is to harmonize the\ndistributions of a multi-view diffusion model and an identity-specific 2D\ngenerative model, aligning them with the distribution of the desired 3D\nsubject. Specifically, we design a co-evolution framework to reduce the\nvariance of distributions, where each model undergoes a process of learning\nfrom the other through identity-aware optimization and subject-prior\noptimization, respectively. Extensive experiments demonstrate that our method\ncan produce high-quality, consistent, and subject-specific 3D content with\ntext-driven modifications that are unseen in subject image.\n"
    },
    {
        "title": "Score-Guided Diffusion for 3D Human Recovery",
        "published_time": "2024-03-14T17:56:14Z",
        "abstract": "  We present Score-Guided Human Mesh Recovery (ScoreHMR), an approach for\nsolving inverse problems for 3D human pose and shape reconstruction. These\ninverse problems involve fitting a human body model to image observations,\ntraditionally solved through optimization techniques. ScoreHMR mimics model\nfitting approaches, but alignment with the image observation is achieved\nthrough score guidance in the latent space of a diffusion model. The diffusion\nmodel is trained to capture the conditional distribution of the human model\nparameters given an input image. By guiding its denoising process with a\ntask-specific score, ScoreHMR effectively solves inverse problems for various\napplications without the need for retraining the task-agnostic diffusion model.\nWe evaluate our approach on three settings/applications. These are: (i)\nsingle-frame model fitting; (ii) reconstruction from multiple uncalibrated\nviews; (iii) reconstructing humans in video sequences. ScoreHMR consistently\noutperforms all optimization baselines on popular benchmarks across all\nsettings. We make our code and models available at the\nhttps://statho.github.io/ScoreHMR.\n"
    },
    {
        "title": "Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering",
        "published_time": "2024-03-14T17:55:33Z",
        "abstract": "  Visual text rendering poses a fundamental challenge for contemporary\ntext-to-image generation models, with the core problem lying in text encoder\ndeficiencies. To achieve accurate text rendering, we identify two crucial\nrequirements for text encoders: character awareness and alignment with glyphs.\nOur solution involves crafting a series of customized text encoder, Glyph-ByT5,\nby fine-tuning the character-aware ByT5 encoder using a meticulously curated\npaired glyph-text dataset. We present an effective method for integrating\nGlyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for\ndesign image generation. This significantly enhances text rendering accuracy,\nimproving it from less than $20\\%$ to nearly $90\\%$ on our design image\nbenchmark. Noteworthy is Glyph-SDXL's newfound ability for text paragraph\nrendering, achieving high spelling accuracy for tens to hundreds of characters\nwith automated multi-line layouts. Finally, through fine-tuning Glyph-SDXL with\na small set of high-quality, photorealistic images featuring visual text, we\nshowcase a substantial improvement in scene text rendering capabilities in\nopen-domain real images. These compelling outcomes aim to encourage further\nexploration in designing customized text encoders for diverse and challenging\ntasks.\n"
    },
    {
        "title": "PosSAM: Panoptic Open-vocabulary Segment Anything",
        "published_time": "2024-03-14T17:55:03Z",
        "abstract": "  In this paper, we introduce an open-vocabulary panoptic segmentation model\nthat effectively unifies the strengths of the Segment Anything Model (SAM) with\nthe vision-language CLIP model in an end-to-end framework. While SAM excels in\ngenerating spatially-aware masks, it's decoder falls short in recognizing\nobject class information and tends to oversegment without additional guidance.\nExisting approaches address this limitation by using multi-stage techniques and\nemploying separate models to generate class-aware prompts, such as bounding\nboxes or segmentation masks. Our proposed method, PosSAM is an end-to-end model\nwhich leverages SAM's spatially rich features to produce instance-aware masks\nand harnesses CLIP's semantically discriminative features for effective\ninstance classification. Specifically, we address the limitations of SAM and\npropose a novel Local Discriminative Pooling (LDP) module leveraging\nclass-agnostic SAM and class-aware CLIP features for unbiased open-vocabulary\nclassification. Furthermore, we introduce a Mask-Aware Selective Ensembling\n(MASE) algorithm that adaptively enhances the quality of generated masks and\nboosts the performance of open-vocabulary classification during inference for\neach image. We conducted extensive experiments to demonstrate our methods\nstrong generalization properties across multiple datasets, achieving\nstate-of-the-art performance with substantial improvements over SOTA\nopen-vocabulary panoptic segmentation methods. In both COCO to ADE20K and\nADE20K to COCO settings, PosSAM outperforms the previous state-of-the-art\nmethods by a large margin, 2.4 PQ and 4.6 PQ, respectively. Project Website:\nhttps://vibashan.github.io/possam-web/.\n"
    },
    {
        "title": "Explore In-Context Segmentation via Latent Diffusion Models",
        "published_time": "2024-03-14T17:52:31Z",
        "abstract": "  In-context segmentation has drawn more attention with the introduction of\nvision foundation models. Most existing approaches adopt metric learning or\nmasked image modeling to build the correlation between visual prompts and input\nimage queries. In this work, we explore this problem from a new perspective,\nusing one representative generation model, the latent diffusion model (LDM). We\nobserve a task gap between generation and segmentation in diffusion models, but\nLDM is still an effective minimalist for in-context segmentation. In\nparticular, we propose two meta-architectures and correspondingly design\nseveral output alignment and optimization strategies. We have conducted\ncomprehensive ablation studies and empirically found that the segmentation\nquality counts on output alignment and in-context instructions. Moreover, we\nbuild a new and fair in-context segmentation benchmark that includes both image\nand video datasets. Experiments validate the efficiency of our approach,\ndemonstrating comparable or even stronger results than previous specialist\nmodels or visual foundation models. Our study shows that LDMs can also achieve\ngood enough results for challenging in-context segmentation tasks.\n"
    },
    {
        "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
        "published_time": "2024-03-14T17:51:32Z",
        "abstract": "  In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, consisting of\nboth dense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.\n"
    },
    {
        "title": "Counterfactual contrastive learning: robust representations via causal\n  image synthesis",
        "published_time": "2024-03-14T17:47:01Z",
        "abstract": "  Contrastive pretraining is well-known to improve downstream task performance\nand model generalisation, especially in limited label settings. However, it is\nsensitive to the choice of augmentation pipeline. Positive pairs should\npreserve semantic information while destroying domain-specific information.\nStandard augmentation pipelines emulate domain-specific changes with\npre-defined photometric transformations, but what if we could simulate\nrealistic domain changes instead? In this work, we show how to utilise recent\nprogress in counterfactual image generation to this effect. We propose\nCF-SimCLR, a counterfactual contrastive learning approach which leverages\napproximate counterfactual inference for positive pair creation. Comprehensive\nevaluation across five datasets, on chest radiography and mammography,\ndemonstrates that CF-SimCLR substantially improves robustness to acquisition\nshift with higher downstream performance on both in- and out-of-distribution\ndata, particularly for domains which are under-represented during training.\n"
    },
    {
        "title": "Renovating Names in Open-Vocabulary Segmentation Benchmarks",
        "published_time": "2024-03-14T17:35:32Z",
        "abstract": "  Names are essential to both human cognition and vision-language models.\nOpen-vocabulary models utilize class names as text prompts to generalize to\ncategories unseen during training. However, name qualities are often overlooked\nand lack sufficient precision in existing datasets. In this paper, we address\nthis underexplored problem by presenting a framework for \"renovating\" names in\nopen-vocabulary segmentation benchmarks (RENOVATE). Through human study, we\ndemonstrate that the names generated by our model are more precise descriptions\nof the visual segments and hence enhance the quality of existing datasets by\nmeans of simple renaming. We further demonstrate that using our renovated names\nenables training of stronger open-vocabulary segmentation models. Using\nopen-vocabulary segmentation for name quality evaluation, we show that our\nrenovated names lead to up to 16% relative improvement from the original names\non various benchmarks across various state-of-the-art models. We provide our\ncode and relabelings for several popular segmentation datasets (ADE20K,\nCityscapes, PASCAL Context) to the research community.\n"
    },
    {
        "title": "The NeRFect Match: Exploring NeRF Features for Visual Localization",
        "published_time": "2024-03-14T17:11:49Z",
        "abstract": "  In this work, we propose the use of Neural Radiance Fields (NeRF) as a scene\nrepresentation for visual localization. Recently, NeRF has been employed to\nenhance pose regression and scene coordinate regression models by augmenting\nthe training database, providing auxiliary supervision through rendered images,\nor serving as an iterative refinement module. We extend its recognized\nadvantages -- its ability to provide a compact scene representation with\nrealistic appearances and accurate geometry -- by exploring the potential of\nNeRF's internal features in establishing precise 2D-3D matches for\nlocalization. To this end, we conduct a comprehensive examination of NeRF's\nimplicit knowledge, acquired through view synthesis, for matching under various\nconditions. This includes exploring different matching network architectures,\nextracting encoder features at multiple layers, and varying training\nconfigurations. Significantly, we introduce NeRFMatch, an advanced 2D-3D\nmatching function that capitalizes on the internal knowledge of NeRF learned\nvia view synthesis. Our evaluation of NeRFMatch on standard localization\nbenchmarks, within a structure-based pipeline, sets a new state-of-the-art for\nlocalization performance on Cambridge Landmarks.\n"
    },
    {
        "title": "Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text\n  Transformation",
        "published_time": "2024-03-14T17:03:04Z",
        "abstract": "  Multimodal large language models (MLLMs) have shown impressive reasoning\nabilities, which, however, are also more vulnerable to jailbreak attacks than\ntheir LLM predecessors. Although still capable of detecting unsafe responses,\nwe observe that safety mechanisms of the pre-aligned LLMs in MLLMs can be\neasily bypassed due to the introduction of image features. To construct robust\nMLLMs, we propose ECSO(Eyes Closed, Safety On), a novel training-free\nprotecting approach that exploits the inherent safety awareness of MLLMs, and\ngenerates safer responses via adaptively transforming unsafe images into texts\nto activate intrinsic safety mechanism of pre-aligned LLMs in MLLMs.\nExperiments on five state-of-the-art (SoTA) MLLMs demonstrate that our ECSO\nenhances model safety significantly (e.g., a 37.6% improvement on the\nMM-SafetyBench (SD+OCR), and 71.3% on VLSafe for the LLaVA-1.5-7B), while\nconsistently maintaining utility results on common MLLM benchmarks.\nFurthermore, we show that ECSO can be used as a data engine to generate\nsupervised-finetuning (SFT) data for MLLM alignment without extra human\nintervention.\n"
    },
    {
        "title": "Less is More: Data Value Estimation for Visual Instruction Tuning",
        "published_time": "2024-03-14T16:47:25Z",
        "abstract": "  Visual instruction tuning is the key to building multimodal large language\nmodels (MLLMs), which greatly improves the reasoning capabilities of large\nlanguage models (LLMs) in vision scenario. However, existing MLLMs mostly rely\non a mixture of multiple highly diverse visual instruction datasets for\ntraining (even more than a million instructions), which may introduce data\nredundancy. To investigate this issue, we conduct a series of empirical\nstudies, which reveal a significant redundancy within the visual instruction\ndatasets, and show that greatly reducing the amount of several instruction\ndataset even do not affect the performance. Based on the findings, we propose a\nnew data selection approach TIVE, to eliminate redundancy within visual\ninstruction data. TIVE first estimates the task-level and instance-level value\nof the visual instructions based on computed gradients. Then, according to the\nestimated values, TIVE determines the task proportion within the visual\ninstructions, and selects representative instances to compose a smaller visual\ninstruction subset for training. Experiments on LLaVA-1.5 show that our\napproach using only about 7.5% data can achieve comparable performance as the\nfull-data fine-tuned model across seven benchmarks, even surpassing it on four\nof the benchmarks. Our code and data will be publicly released.\n"
    },
    {
        "title": "CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition",
        "published_time": "2024-02-26T14:18:12Z",
        "abstract": "  To achieve greater accuracy, hypergraph matching algorithms require\nexponential increases in computational resources. Recent kd-tree-based\napproximate nearest neighbor (ANN) methods, despite the sparsity of their\ncompatibility tensor, still require exhaustive calculations for large-scale\ngraph matching. This work utilizes CUR tensor decomposition and introduces a\nnovel cascaded second and third-order hypergraph matching framework (CURSOR)\nfor efficient hypergraph matching. A CUR-based second-order graph matching\nalgorithm is used to provide a rough match, and then the core of CURSOR, a\nfiber-CUR-based tensor generation method, directly calculates entries of the\ncompatibility tensor by leveraging the initial second-order match result. This\nsignificantly decreases the time complexity and tensor density. A probability\nrelaxation labeling (PRL)-based matching algorithm, specifically suitable for\nsparse tensors, is developed. Experiment results on large-scale synthetic\ndatasets and widely-adopted benchmark sets demonstrate the superiority of\nCURSOR over existing methods. The tensor generation method in CURSOR can be\nintegrated seamlessly into existing hypergraph matching methods to improve\ntheir performance and lower their computational costs.\n"
    },
    {
        "title": "Cloud gap-filling with deep learning for improved grassland monitoring",
        "published_time": "2024-03-14T16:41:26Z",
        "abstract": "  Uninterrupted optical image time series are crucial for the timely monitoring\nof agricultural land changes. However, the continuity of such time series is\noften disrupted by clouds. In response to this challenge, we propose a deep\nlearning method that integrates cloud-free optical (Sentinel-2) observations\nand weather-independent (Sentinel-1) Synthetic Aperture Radar (SAR) data, using\na combined Convolutional Neural Network (CNN)-Recurrent Neural Network (RNN)\narchitecture to generate continuous Normalized Difference Vegetation Index\n(NDVI) time series. We emphasize the significance of observation continuity by\nassessing the impact of the generated time series on the detection of grassland\nmowing events. We focus on Lithuania, a country characterized by extensive\ncloud coverage, and compare our approach with alternative interpolation\ntechniques (i.e., linear, Akima, quadratic). Our method surpasses these\ntechniques, with an average MAE of 0.024 and R^2 of 0.92. It not only improves\nthe accuracy of event detection tasks by employing a continuous time series,\nbut also effectively filters out sudden shifts and noise originating from\ncloudy observations that cloud masks often fail to detect.\n"
    },
    {
        "title": "WeakSurg: Weakly supervised surgical instrument segmentation using\n  temporal equivariance and semantic continuity",
        "published_time": "2024-03-14T16:39:11Z",
        "abstract": "  Weakly supervised surgical instrument segmentation with only instrument\npresence labels has been rarely explored in surgical domain. To mitigate the\nhighly under-constrained challenges, we extend a two-stage weakly supervised\nsegmentation paradigm with temporal attributes from two perspectives. From a\ntemporal equivariance perspective, we propose a prototype-based temporal\nequivariance regulation loss to enhance pixel-wise consistency between adjacent\nfeatures. From a semantic continuity perspective, we propose a class-aware\ntemporal semantic continuity loss to constrain the semantic consistency between\na global view of target frame and local non-discriminative regions of adjacent\nreference frame. To the best of our knowledge, WeakSurg is the first\ninstrument-presence-only weakly supervised segmentation architecture to take\ntemporal information into account for surgical scenarios. Extensive experiments\nare validated on Cholec80, an open benchmark for phase and instrument\nrecognition. We annotate instance-wise instrument labels with fixed time-steps\nwhich are double checked by a clinician with 3-years experience. Our results\nshow that WeakSurg compares favorably with state-of-the-art methods not only on\nsemantic segmentation metrics but also on instance segmentation metrics.\n"
    },
    {
        "title": "Probabilistic Contrastive Learning for Long-Tailed Visual Recognition",
        "published_time": "2024-03-11T13:44:49Z",
        "abstract": "  Long-tailed distributions frequently emerge in real-world data, where a large\nnumber of minority categories contain a limited number of samples. Such\nimbalance issue considerably impairs the performance of standard supervised\nlearning algorithms, which are mainly designed for balanced training sets.\nRecent investigations have revealed that supervised contrastive learning\nexhibits promising potential in alleviating the data imbalance. However, the\nperformance of supervised contrastive learning is plagued by an inherent\nchallenge: it necessitates sufficiently large batches of training data to\nconstruct contrastive pairs that cover all categories, yet this requirement is\ndifficult to meet in the context of class-imbalanced data. To overcome this\nobstacle, we propose a novel probabilistic contrastive (ProCo) learning\nalgorithm that estimates the data distribution of the samples from each class\nin the feature space, and samples contrastive pairs accordingly. In fact,\nestimating the distributions of all classes using features in a small batch,\nparticularly for imbalanced data, is not feasible. Our key idea is to introduce\na reasonable and simple assumption that the normalized features in contrastive\nlearning follow a mixture of von Mises-Fisher (vMF) distributions on unit\nspace, which brings two-fold benefits. First, the distribution parameters can\nbe estimated using only the first sample moment, which can be efficiently\ncomputed in an online manner across different batches. Second, based on the\nestimated distribution, the vMF distribution allows us to sample an infinite\nnumber of contrastive pairs and derive a closed form of the expected\ncontrastive loss for efficient optimization. Our code is available at\nhttps://github.com/LeapLabTHU/ProCo.\n"
    },
    {
        "title": "Explorations in Texture Learning",
        "published_time": "2024-03-14T16:30:52Z",
        "abstract": "  In this work, we investigate \\textit{texture learning}: the identification of\ntextures learned by object classification models, and the extent to which they\nrely on these textures. We build texture-object associations that uncover new\ninsights about the relationships between texture and object classes in CNNs and\nfind three classes of results: associations that are strong and expected,\nstrong and not expected, and expected but not present. Our analysis\ndemonstrates that investigations in texture learning enable new methods for\ninterpretability and have the potential to uncover unexpected biases.\n"
    },
    {
        "title": "TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation\n  under Visual Corruptions",
        "published_time": "2024-03-04T12:20:29Z",
        "abstract": "  Robot navigation under visual corruption presents a formidable challenge. To\naddress this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav,\nfor point-goal navigation under visual corruptions. Our \"plug-and-play\" method\nincorporates a top-down decoder to a pre-trained navigation model. Firstly, the\npre-trained navigation model gets a corrupted image and extracts features.\nSecondly, the top-down decoder produces the reconstruction given the high-level\nfeatures extracted by the pre-trained model. Then, it feeds the reconstruction\nof a corrupted image back to the pre-trained model. Finally, the pre-trained\nmodel does forward pass again to output action. Despite being trained solely on\nclean images, the top-down decoder can reconstruct cleaner images from\ncorrupted ones without the need for gradient-based adaptation. The pre-trained\nnavigation model with our top-down decoder significantly enhances navigation\nperformance across almost all visual corruptions in our benchmarks. Our method\nimproves the success rate of point-goal navigation from the state-of-the-art\nresult of 46% to 94% on the most severe corruption. This suggests its potential\nfor broader application in robotic visual navigation. Project page:\nhttps://sites.google.com/view/tta-nav\n"
    },
    {
        "title": "Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data",
        "published_time": "2024-02-08T18:30:50Z",
        "abstract": "  In recent years, Transformers have become the de-facto architecture for\nsequence modeling on text and a variety of multi-dimensional data, such as\nimages and video. However, the use of self-attention layers in a Transformer\nincurs prohibitive compute and memory complexity that scales quadratically\nw.r.t. the sequence length. A recent architecture, Mamba, based on state space\nmodels has been shown to achieve comparable performance for modeling text\nsequences, while scaling linearly with the sequence length. In this work, we\npresent Mamba-ND, a generalized design extending the Mamba architecture to\narbitrary multi-dimensional data. Our design alternatively unravels the input\ndata across different dimensions following row-major orderings. We provide a\nsystematic comparison of Mamba-ND with several other alternatives, based on\nprior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.\nEmpirically, we show that Mamba-ND demonstrates performance competitive with\nthe state-of-the-art on a variety of multi-dimensional benchmarks, including\nImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather\nforecasting.\n"
    },
    {
        "title": "VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding",
        "published_time": "2024-03-14T16:13:00Z",
        "abstract": "  The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent\n"
    },
    {
        "title": "SkateFormer: Skeletal-Temporal Transformer for Human Action Recognition",
        "published_time": "2024-03-14T15:55:53Z",
        "abstract": "  Skeleton-based action recognition, which classifies human actions based on\nthe coordinates of joints and their connectivity within skeleton data, is\nwidely utilized in various scenarios. While Graph Convolutional Networks (GCNs)\nhave been proposed for skeleton data represented as graphs, they suffer from\nlimited receptive fields constrained by joint connectivity. To address this\nlimitation, recent advancements have introduced transformer-based methods.\nHowever, capturing correlations between all joints in all frames requires\nsubstantial memory resources. To alleviate this, we propose a novel approach\ncalled Skeletal-Temporal Transformer (SkateFormer) that partitions joints and\nframes based on different types of skeletal-temporal relation (Skate-Type) and\nperforms skeletal-temporal self-attention (Skate-MSA) within each partition. We\ncategorize the key skeletal-temporal relations for action recognition into a\ntotal of four distinct types. These types combine (i) two skeletal relation\ntypes based on physically neighboring and distant joints, and (ii) two temporal\nrelation types based on neighboring and distant frames. Through this\npartition-specific attention strategy, our SkateFormer can selectively focus on\nkey joints and frames crucial for action recognition in an action-adaptive\nmanner with efficient computation. Extensive experiments on various benchmark\ndatasets validate that our SkateFormer outperforms recent state-of-the-art\nmethods.\n"
    },
    {
        "title": "Don't Judge by the Look: A Motion Coherent Augmentation for Video\n  Recognition",
        "published_time": "2024-03-14T15:53:04Z",
        "abstract": "  Current training pipelines in object recognition neglect Hue Jittering when\ndoing data augmentation as it not only brings appearance changes that are\ndetrimental to classification, but also the implementation is inefficient in\npractice. In this study, we investigate the effect of hue variance in the\ncontext of video recognition and find this variance to be beneficial since\nstatic appearances are less important in videos that contain motion\ninformation. Based on this observation, we propose a data augmentation method\nfor video recognition, named Motion Coherent Augmentation (MCA), that\nintroduces appearance variation in videos and implicitly encourages the model\nto prioritize motion patterns, rather than static appearances. Concretely, we\npropose an operation SwapMix to efficiently modify the appearance of video\nsamples, and introduce Variation Alignment (VA) to resolve the distribution\nshift caused by SwapMix, enforcing the model to learn appearance invariant\nrepresentations. Comprehensive empirical evaluation across various\narchitectures and different datasets solidly validates the effectiveness and\ngeneralization ability of MCA, and the application of VA in other augmentation\nmethods. Code is available at https://github.com/BeSpontaneous/MCA-pytorch.\n"
    },
    {
        "title": "DiffSF: Diffusion Models for Scene Flow Estimation",
        "published_time": "2024-03-08T14:06:15Z",
        "abstract": "  Scene flow estimation is an essential ingredient for a variety of real-world\napplications, especially for autonomous agents, such as self-driving cars and\nrobots. While recent scene flow estimation approaches achieve a reasonable\naccuracy, their applicability to real-world systems additionally benefits from\na reliability measure. Aiming at improving accuracy while additionally\nproviding an estimate for uncertainty, we propose DiffSF that combines\ntransformer-based scene flow estimation with denoising diffusion models. In the\ndiffusion process, the ground truth scene flow vector field is gradually\nperturbed by adding Gaussian noise. In the reverse process, starting from\nrandomly sampled Gaussian noise, the scene flow vector field prediction is\nrecovered by conditioning on a source and a target point cloud. We show that\nthe diffusion process greatly increases the robustness of predictions compared\nto prior approaches resulting in state-of-the-art performance on standard scene\nflow estimation benchmarks. Moreover, by sampling multiple times with different\ninitial states, the denoising process predicts multiple hypotheses, which\nenables measuring the output uncertainty, allowing our approach to detect a\nmajority of the inaccurate predictions. The code is available at\nhttps://github.com/ZhangYushan3/DiffSF.\n"
    },
    {
        "title": "Faceptor: A Generalist Model for Face Perception",
        "published_time": "2024-03-14T15:42:31Z",
        "abstract": "  With the comprehensive research conducted on various face analysis tasks,\nthere is a growing interest among researchers to develop a unified approach to\nface perception. Existing methods mainly discuss unified representation and\ntraining, which lack task extensibility and application efficiency. To tackle\nthis issue, we focus on the unified model structure, exploring a face\ngeneralist model. As an intuitive design, Naive Faceptor enables tasks with the\nsame output shape and granularity to share the structural design of the\nstandardized output head, achieving improved task extensibility. Furthermore,\nFaceptor is proposed to adopt a well-designed single-encoder dual-decoder\narchitecture, allowing task-specific queries to represent new-coming semantics.\nThis design enhances the unification of model structure while improving\napplication efficiency in terms of storage overhead. Additionally, we introduce\nLayer-Attention into Faceptor, enabling the model to adaptively select features\nfrom optimal layers to perform the desired tasks. Through joint training on 13\nface perception datasets, Faceptor achieves exceptional performance in facial\nlandmark localization, face parsing, age estimation, expression recognition,\nbinary attribute classification, and face recognition, achieving or surpassing\nspecialized methods in most tasks. Our training framework can also be applied\nto auxiliary supervised learning, significantly improving performance in\ndata-sparse tasks such as age estimation and expression recognition. The code\nand models will be made publicly available at\nhttps://github.com/lxq1000/Faceptor.\n"
    },
    {
        "title": "AllSpark: Reborn Labeled Features from Unlabeled in Transformer for\n  Semi-Supervised Semantic Segmentation",
        "published_time": "2024-03-04T08:06:41Z",
        "abstract": "  Semi-supervised semantic segmentation (SSSS) has been proposed to alleviate\nthe burden of time-consuming pixel-level manual labeling, which leverages\nlimited labeled data along with larger amounts of unlabeled data. Current\nstate-of-the-art methods train the labeled data with ground truths and\nunlabeled data with pseudo labels. However, the two training flows are\nseparate, which allows labeled data to dominate the training process, resulting\nin low-quality pseudo labels and, consequently, sub-optimal results. To\nalleviate this issue, we present AllSpark, which reborns the labeled features\nfrom unlabeled ones with the channel-wise cross-attention mechanism. We further\nintroduce a Semantic Memory along with a Channel Semantic Grouping strategy to\nensure that unlabeled features adequately represent labeled features. The\nAllSpark shed new light on the architecture level designs of SSSS rather than\nframework level, which avoids increasingly complicated training pipeline\ndesigns. It can also be regarded as a flexible bottleneck module that can be\nseamlessly integrated into a general transformer-based segmentation model. The\nproposed AllSpark outperforms existing methods across all evaluation protocols\non Pascal, Cityscapes and COCO benchmarks without bells-and-whistles. Code and\nmodel weights are available at: https://github.com/xmed-lab/AllSpark.\n"
    },
    {
        "title": "Anomaly Detection by Adapting a pre-trained Vision Language Model",
        "published_time": "2024-03-14T15:35:07Z",
        "abstract": "  Recently, large vision and language models have shown their success when\nadapting them to many downstream tasks. In this paper, we present a unified\nframework named CLIP-ADA for Anomaly Detection by Adapting a pre-trained CLIP\nmodel. To this end, we make two important improvements: 1) To acquire unified\nanomaly detection across industrial images of multiple categories, we introduce\nthe learnable prompt and propose to associate it with abnormal patterns through\nself-supervised learning. 2) To fully exploit the representation power of CLIP,\nwe introduce an anomaly region refinement strategy to refine the localization\nquality. During testing, the anomalies are localized by directly calculating\nthe similarity between the representation of the learnable prompt and the\nimage. Comprehensive experiments demonstrate the superiority of our framework,\ne.g., we achieve the state-of-the-art 97.5/55.6 and 89.3/33.1 on MVTec-AD and\nVisA for anomaly detection and localization. In addition, the proposed method\nalso achieves encouraging performance with marginal training data, which is\nmore challenging.\n"
    },
    {
        "title": "SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with\n  Spike Streams",
        "published_time": "2024-03-14T15:29:09Z",
        "abstract": "  Reconstructing a sequence of sharp images from the blurry input is crucial\nfor enhancing our insights into the captured scene and poses a significant\nchallenge due to the limited temporal features embedded in the image. Spike\ncameras, sampling at rates up to 40,000 Hz, have proven effective in capturing\nmotion features and beneficial for solving this ill-posed problem. Nonetheless,\nexisting methods fall into the supervised learning paradigm, which suffers from\nnotable performance degradation when applied to real-world scenarios that\ndiverge from the synthetic training data domain. Moreover, the quality of\nreconstructed images is capped by the generated images based on motion analysis\ninterpolation, which inherently differs from the actual scene, affecting the\ngeneralization ability of these methods in real high-speed scenarios. To\naddress these challenges, we propose the first self-supervised framework for\nthe task of spike-guided motion deblurring. Our approach begins with the\nformulation of a spike-guided deblurring model that explores the theoretical\nrelationships among spike streams, blurry images, and their corresponding sharp\nsequences. We subsequently develop a self-supervised cascaded framework to\nalleviate the issues of spike noise and spatial-resolution mismatching\nencountered in the deblurring model. With knowledge distillation and\nre-blurring loss, we further design a lightweight deblur network to generate\nhigh-quality sequences with brightness and texture consistency with the\noriginal input. Quantitative and qualitative experiments conducted on our\nreal-world and synthetic datasets with spikes validate the superior\ngeneralization of the proposed framework. Our code, data and trained models\nwill be available at \\url{https://github.com/chenkang455/S-SDM}.\n"
    },
    {
        "title": "What Sketch Explainability Really Means for Downstream Tasks",
        "published_time": "2024-03-14T15:22:33Z",
        "abstract": "  In this paper, we explore the unique modality of sketch for explainability,\nemphasising the profound impact of human strokes compared to conventional\npixel-oriented studies. Beyond explanations of network behavior, we discern the\ngenuine implications of explainability across diverse downstream sketch-related\ntasks. We propose a lightweight and portable explainability solution -- a\nseamless plugin that integrates effortlessly with any pre-trained model,\neliminating the need for re-training. Demonstrating its adaptability, we\npresent four applications: highly studied retrieval and generation, and\ncompletely novel assisted drawing and sketch adversarial attacks. The\ncentrepiece to our solution is a stroke-level attribution map that takes\ndifferent forms when linked with downstream tasks. By addressing the inherent\nnon-differentiability of rasterisation, we enable explanations at both coarse\nstroke level (SLA) and partial stroke level (P-SLA), each with its advantages\nfor specific downstream tasks.\n"
    },
    {
        "title": "MambaTalk: Efficient Holistic Gesture Synthesis with Selective State\n  Space Models",
        "published_time": "2024-03-14T15:10:54Z",
        "abstract": "  Gesture synthesis is a vital realm of human-computer interaction, with\nwide-ranging applications across various fields like film, robotics, and\nvirtual reality. Recent advancements have utilized the diffusion model and\nattention mechanisms to improve gesture synthesis. However, due to the high\ncomputational complexity of these techniques, generating long and diverse\nsequences with low latency remains a challenge. We explore the potential of\nstate space models (SSMs) to address the challenge, implementing a two-stage\nmodeling strategy with discrete motion priors to enhance the quality of\ngestures. Leveraging the foundational Mamba block, we introduce MambaTalk,\nenhancing gesture diversity and rhythm through multimodal integration.\nExtensive experiments demonstrate that our method matches or exceeds the\nperformance of state-of-the-art models.\n"
    },
    {
        "title": "Eta Inversion: Designing an Optimal Eta Function for Diffusion-based\n  Real Image Editing",
        "published_time": "2024-03-14T15:07:36Z",
        "abstract": "  Diffusion models have achieved remarkable success in the domain of\ntext-guided image generation and, more recently, in text-guided image editing.\nA commonly adopted strategy for editing real images involves inverting the\ndiffusion process to obtain a noisy representation of the original image, which\nis then denoised to achieve the desired edits. However, current methods for\ndiffusion inversion often struggle to produce edits that are both faithful to\nthe specified text prompt and closely resemble the source image. To overcome\nthese limitations, we introduce a novel and adaptable diffusion inversion\ntechnique for real image editing, which is grounded in a theoretical analysis\nof the role of $\\eta$ in the DDIM sampling equation for enhanced editability.\nBy designing a universal diffusion inversion method with a time- and\nregion-dependent $\\eta$ function, we enable flexible control over the editing\nextent. Through a comprehensive series of quantitative and qualitative\nassessments, involving a comparison with a broad array of recent methods, we\ndemonstrate the superiority of our approach. Our method not only sets a new\nbenchmark in the field but also significantly outperforms existing strategies.\nOur code is available at https://github.com/furiosa-ai/eta-inversion\n"
    },
    {
        "title": "M&M: Multimodal-Multitask Model Integrating Audiovisual Cues in\n  Cognitive Load Assessment",
        "published_time": "2024-03-14T14:49:40Z",
        "abstract": "  This paper introduces the M&M model, a novel multimodal-multitask learning\nframework, applied to the AVCAffe dataset for cognitive load assessment (CLA).\nM&M uniquely integrates audiovisual cues through a dual-pathway architecture,\nfeaturing specialized streams for audio and video inputs. A key innovation lies\nin its cross-modality multihead attention mechanism, fusing the different\nmodalities for synchronized multitasking. Another notable feature is the\nmodel's three specialized branches, each tailored to a specific cognitive load\nlabel, enabling nuanced, task-specific analysis. While it shows modest\nperformance compared to the AVCAffe's single-task baseline, M\\&M demonstrates a\npromising framework for integrated multimodal processing. This work paves the\nway for future enhancements in multimodal-multitask learning systems,\nemphasizing the fusion of diverse data types for complex task handling.\n"
    },
    {
        "title": "3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation",
        "published_time": "2024-03-14T14:31:22Z",
        "abstract": "  Text-driven 3D scene generation techniques have made rapid progress in recent\nyears. Their success is mainly attributed to using existing generative models\nto iteratively perform image warping and inpainting to generate 3D scenes.\nHowever, these methods heavily rely on the outputs of existing models, leading\nto error accumulation in geometry and appearance that prevent the models from\nbeing used in various scenarios (e.g., outdoor and unreal scenarios). To\naddress this limitation, we generatively refine the newly generated local views\nby querying and aggregating global 3D information, and then progressively\ngenerate the 3D scene. Specifically, we employ a tri-plane features-based NeRF\nas a unified representation of the 3D scene to constrain global 3D consistency,\nand propose a generative refinement network to synthesize new contents with\nhigher quality by exploiting the natural image prior from 2D diffusion model as\nwell as the global 3D information of the current scene. Our extensive\nexperiments demonstrate that, in comparison to previous methods, our approach\nsupports wide variety of scene generation and arbitrary camera trajectories\nwith improved visual quality and 3D consistency.\n"
    },
    {
        "title": "Improving Real-Time Omnidirectional 3D Multi-Person Human Pose\n  Estimation with People Matching and Unsupervised 2D-3D Lifting",
        "published_time": "2024-03-14T14:30:31Z",
        "abstract": "  Current human pose estimation systems focus on retrieving an accurate 3D\nglobal estimate of a single person. Therefore, this paper presents one of the\nfirst 3D multi-person human pose estimation systems that is able to work in\nreal-time and is also able to handle basic forms of occlusion. First, we adjust\nan off-the-shelf 2D detector and an unsupervised 2D-3D lifting model for use\nwith a 360$^\\circ$ panoramic camera and mmWave radar sensors. We then introduce\nseveral contributions, including camera and radar calibrations, and the\nimproved matching of people within the image and radar space. The system\naddresses both the depth and scale ambiguity problems by employing a\nlightweight 2D-3D pose lifting algorithm that is able to work in real-time\nwhile exhibiting accurate performance in both indoor and outdoor environments\nwhich offers both an affordable and scalable solution. Notably, our system's\ntime complexity remains nearly constant irrespective of the number of detected\nindividuals, achieving a frame rate of approximately 7-8 fps on a laptop with a\ncommercial-grade GPU.\n"
    },
    {
        "title": "Open-Vocabulary Object Detection with Meta Prompt Representation and\n  Instance Contrastive Optimization",
        "published_time": "2024-03-14T14:25:10Z",
        "abstract": "  Classical object detectors are incapable of detecting novel class objects\nthat are not encountered before. Regarding this issue, Open-Vocabulary Object\nDetection (OVOD) is proposed, which aims to detect the objects in the candidate\nclass list. However, current OVOD models are suffering from overfitting on the\nbase classes, heavily relying on the large-scale extra data, and complex\ntraining process. To overcome these issues, we propose a novel framework with\nMeta prompt and Instance Contrastive learning (MIC) schemes. Firstly, we\nsimulate a novel-class-emerging scenario to help the prompt learner that learns\nclass and background prompts generalize to novel classes. Secondly, we design\nan instance-level contrastive strategy to promote intra-class compactness and\ninter-class separation, which benefits generalization of the detector to novel\nclass objects. Without using knowledge distillation, ensemble model or extra\ntraining data during detector training, our proposed MIC outperforms previous\nSOTA methods trained with these complex techniques on LVIS. Most importantly,\nMIC shows great generalization ability on novel classes, e.g., with $+4.3\\%$\nand $+1.9\\% \\ \\mathrm{AP}$ improvement compared with previous SOTA on COCO and\nObjects365, respectively.\n"
    },
    {
        "title": "Reconstruction and Simulation of Elastic Objects with Spring-Mass 3D\n  Gaussians",
        "published_time": "2024-03-14T14:25:10Z",
        "abstract": "  Reconstructing and simulating elastic objects from visual observations is\ncrucial for applications in computer vision and robotics. Existing methods,\nsuch as 3D Gaussians, provide modeling for 3D appearance and geometry but lack\nthe ability to simulate physical properties or optimize parameters for\nheterogeneous objects. We propose Spring-Gaus, a novel framework that\nintegrates 3D Gaussians with physics-based simulation for reconstructing and\nsimulating elastic objects from multi-view videos. Our method utilizes a 3D\nSpring-Mass model, enabling the optimization of physical parameters at the\nindividual point level while decoupling the learning of physics and appearance.\nThis approach achieves great sample efficiency, enhances generalization, and\nreduces sensitivity to the distribution of simulation particles. We evaluate\nSpring-Gaus on both synthetic and real-world datasets, demonstrating accurate\nreconstruction and simulation of elastic objects. This includes future\nprediction and simulation under varying initial states and environmental\nparameters. Project page: https://zlicheng.com/spring_gaus.\n"
    },
    {
        "title": "Efficient Transferability Assessment for Selection of Pre-trained\n  Detectors",
        "published_time": "2024-03-14T14:23:23Z",
        "abstract": "  Large-scale pre-training followed by downstream fine-tuning is an effective\nsolution for transferring deep-learning-based models. Since finetuning all\npossible pre-trained models is computational costly, we aim to predict the\ntransferability performance of these pre-trained models in a computational\nefficient manner. Different from previous work that seek out suitable models\nfor downstream classification and segmentation tasks, this paper studies the\nefficient transferability assessment of pre-trained object detectors. To this\nend, we build up a detector transferability benchmark which contains a large\nand diverse zoo of pre-trained detectors with various architectures, source\ndatasets and training schemes. Given this zoo, we adopt 7 target datasets from\n5 diverse domains as the downstream target tasks for evaluation. Further, we\npropose to assess classification and regression sub-tasks simultaneously in a\nunified framework. Additionally, we design a complementary metric for\nevaluating tasks with varying objects. Experimental results demonstrate that\nour method outperforms other state-of-the-art approaches in assessing\ntransferability under different target domains while efficiently reducing\nwall-clock time 32$\\times$ and requires a mere 5.2\\% memory footprint compared\nto brute-force fine-tuning of all pre-trained detectors.\n"
    },
    {
        "title": "Mitigating attribute amplification in counterfactual image generation",
        "published_time": "2024-03-14T14:14:47Z",
        "abstract": "  Causal generative modelling is gaining interest in medical imaging due to its\nability to answer interventional and counterfactual queries. Most work focuses\non generating counterfactual images that look plausible, using auxiliary\nclassifiers to enforce effectiveness of simulated interventions. We investigate\npitfalls in this approach, discovering the issue of attribute amplification,\nwhere unrelated attributes are spuriously affected during interventions,\nleading to biases across protected characteristics and disease status. We show\nthat attribute amplification is caused by the use of hard labels in the\ncounterfactual training process and propose soft counterfactual fine-tuning to\nmitigate this issue. Our method substantially reduces the amplification effect\nwhile maintaining effectiveness of generated images, demonstrated on a large\nchest X-ray dataset. Our work makes an important advancement towards more\nfaithful and unbiased causal modelling in medical imaging.\n"
    },
    {
        "title": "RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban\n  Scenes",
        "published_time": "2024-03-14T14:08:59Z",
        "abstract": "  The task of separating dynamic objects from static environments using NeRFs\nhas been widely studied in recent years. However, capturing large-scale scenes\nstill poses a challenge due to their complex geometric structures and\nunconstrained dynamics. Without the help of 3D motion cues, previous methods\noften require simplified setups with slow camera motion and only a few/single\ndynamic actors, leading to suboptimal solutions in most urban setups. To\novercome such limitations, we present RoDUS, a pipeline for decomposing static\nand dynamic elements in urban scenes, with thoughtfully separated NeRF models\nfor moving and non-moving components. Our approach utilizes a robust\nkernel-based initialization coupled with 4D semantic information to selectively\nguide the learning process. This strategy enables accurate capturing of the\ndynamics in the scene, resulting in reduced artifacts caused by NeRF on\nbackground reconstruction, all by using self-supervision. Notably, experimental\nevaluations on KITTI-360 and Pandaset datasets demonstrate the effectiveness of\nour method in decomposing challenging urban scenes into precise static and\ndynamic components.\n"
    },
    {
        "title": "Region-based U-net for accelerated training and enhanced precision in\n  deep brain segmentation",
        "published_time": "2024-03-14T14:04:29Z",
        "abstract": "  Segmentation of brain structures on MRI is the primary step for further\nquantitative analysis of brain diseases. Manual segmentation is still\nconsidered the gold standard in terms of accuracy; however, such data is\nextremely time-consuming to generate. This paper presents a deep learning-based\nsegmentation approach for 12 deep-brain structures, utilizing multiple\nregion-based U-Nets. The brain is divided into three focal regions of interest\nthat encompass the brainstem, the ventricular system, and the striatum. Next,\nthree region-based U-nets are run in parallel to parcellate these larger\nstructures into their respective four substructures. This approach not only\ngreatly reduces the training and processing times but also significantly\nenhances the segmentation accuracy, compared to segmenting the entire MRI image\nat once. Our approach achieves remarkable accuracy with an average Dice\nSimilarity Coefficient (DSC) of 0.901 and 95% Hausdorff Distance (HD95) of\n1.155 mm. The method was compared with state-of-the-art segmentation\napproaches, demonstrating a high level of accuracy and robustness of the\nproposed method.\n"
    },
    {
        "title": "Relaxing Accurate Initialization Constraint for 3D Gaussian Splatting",
        "published_time": "2024-03-14T14:04:21Z",
        "abstract": "  3D Gaussian splatting (3DGS) has recently demonstrated impressive\ncapabilities in real-time novel view synthesis and 3D reconstruction. However,\n3DGS heavily depends on the accurate initialization derived from\nStructure-from-Motion (SfM) methods. When trained with randomly initialized\npoint clouds, 3DGS fails to maintain its ability to produce high-quality\nimages, undergoing large performance drops of 4-5 dB in PSNR. Through extensive\nanalysis of SfM initialization in the frequency domain and analysis of a 1D\nregression task with multiple 1D Gaussians, we propose a novel optimization\nstrategy dubbed RAIN-GS (Relaxing Accurate Initialization Constraint for 3D\nGaussian Splatting), that successfully trains 3D Gaussians from random point\nclouds. We show the effectiveness of our strategy through quantitative and\nqualitative comparisons on multiple datasets, largely improving the performance\nin all settings. Our project page and code can be found at\nhttps://ku-cvlab.github.io/RAIN-GS.\n"
    },
    {
        "title": "OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in\n  Large-Scale Outdoor Environments",
        "published_time": "2024-03-14T14:03:29Z",
        "abstract": "  Environment maps endowed with sophisticated semantics are pivotal for\nfacilitating seamless interaction between robots and humans, enabling them to\neffectively carry out various tasks. Open-vocabulary maps, powered by\nVisual-Language models (VLMs), possess inherent advantages, including\nmultimodal retrieval and open-set classes. However, existing open-vocabulary\nmaps are constrained to closed indoor scenarios and VLM features, thereby\ndiminishing their usability and inference capabilities. Moreover, the absence\nof topological relationships further complicates the accurate querying of\nspecific instances. In this work, we propose OpenGraph, a representation of\nopen-vocabulary hierarchical graph structure designed for large-scale outdoor\nenvironments. OpenGraph initially extracts instances and their captions from\nvisual images using 2D foundation models, encoding the captions with features\nto enhance textual reasoning. Subsequently, 3D incremental panoramic mapping\nwith feature embedding is achieved by projecting images onto LiDAR point\nclouds. Finally, the environment is segmented based on lane graph connectivity\nto construct a hierarchical graph. Validation results from real public dataset\nSemanticKITTI demonstrate that, even without fine-tuning the models, OpenGraph\nexhibits the ability to generalize to novel semantic classes and achieve the\nhighest segmentation and query accuracy. The source code of OpenGraph is\npublicly available at https://github.com/BIT-DYN/OpenGraph.\n"
    },
    {
        "title": "XCoOp: Explainable Prompt Learning for Computer-Aided Diagnosis via\n  Concept-guided Context Optimization",
        "published_time": "2024-03-14T14:02:01Z",
        "abstract": "  Utilizing potent representations of the large vision-language models (VLMs)\nto accomplish various downstream tasks has attracted increasing attention.\nWithin this research field, soft prompt learning has become a representative\napproach for efficiently adapting VLMs such as CLIP, to tasks like image\nclassification. However, most existing prompt learning methods learn text\ntokens that are unexplainable, which cannot satisfy the stringent\ninterpretability requirements of Explainable Artificial Intelligence (XAI) in\nhigh-stakes scenarios like healthcare. To address this issue, we propose a\nnovel explainable prompt learning framework that leverages medical knowledge by\naligning the semantics of images, learnable prompts, and clinical\nconcept-driven prompts at multiple granularities. Moreover, our framework\naddresses the lack of valuable concept annotations by eliciting knowledge from\nlarge language models and offers both visual and textual explanations for the\nprompts. Extensive experiments and explainability analyses conducted on various\ndatasets, with and without concept labels, demonstrate that our method\nsimultaneously achieves superior diagnostic performance, flexibility, and\ninterpretability, shedding light on the effectiveness of foundation models in\nfacilitating XAI. The code will be made publically available.\n"
    },
    {
        "title": "Unsupervised Modality-Transferable Video Highlight Detection with\n  Representation Activation Sequence Learning",
        "published_time": "2024-03-14T13:52:03Z",
        "abstract": "  Identifying highlight moments of raw video materials is crucial for improving\nthe efficiency of editing videos that are pervasive on internet platforms.\nHowever, the extensive work of manually labeling footage has created obstacles\nto applying supervised methods to videos of unseen categories. The absence of\nan audio modality that contains valuable cues for highlight detection in many\nvideos also makes it difficult to use multimodal strategies. In this paper, we\npropose a novel model with cross-modal perception for unsupervised highlight\ndetection. The proposed model learns representations with visual-audio level\nsemantics from image-audio pair data via a self-reconstruction task. To achieve\nunsupervised highlight detection, we investigate the latent representations of\nthe network and propose the representation activation sequence learning (RASL)\nmodule with k-point contrastive learning to learn significant representation\nactivations. To connect the visual modality with the audio modality, we use the\nsymmetric contrastive learning (SCL) module to learn the paired visual and\naudio representations. Furthermore, an auxiliary task of masked feature vector\nsequence (FVS) reconstruction is simultaneously conducted during pretraining\nfor representation enhancement. During inference, the cross-modal pretrained\nmodel can generate representations with paired visual-audio semantics given\nonly the visual modality. The RASL module is used to output the highlight\nscores. The experimental results show that the proposed framework achieves\nsuperior performance compared to other state-of-the-art approaches.\n"
    },
    {
        "title": "ConDiSR: Contrastive Disentanglement and Style Regularization for Single\n  Domain Generalization",
        "published_time": "2024-03-14T13:50:44Z",
        "abstract": "  Medical data often exhibits distribution shifts, which cause test-time\nperformance degradation for deep learning models trained using standard\nsupervised learning pipelines. This challenge is addressed in the field of\nDomain Generalization (DG) with the sub-field of Single Domain Generalization\n(SDG) being specifically interesting due to the privacy- or logistics-related\nissues often associated with medical data. Existing disentanglement-based SDG\nmethods heavily rely on structural information embedded in segmentation masks,\nhowever classification labels do not provide such dense information. This work\nintroduces a novel SDG method aimed at medical image classification that\nleverages channel-wise contrastive disentanglement. It is further enhanced with\nreconstruction-based style regularization to ensure extraction of distinct\nstyle and structure feature representations. We evaluate our method on the\ncomplex task of multicenter histopathology image classification, comparing it\nagainst state-of-the-art (SOTA) SDG baselines. Results demonstrate that our\nmethod surpasses the SOTA by a margin of 1% in average accuracy while also\nshowing more stable performance. This study highlights the importance and\nchallenges of exploring SDG frameworks in the context of the classification\ntask. The code is publicly available at\nhttps://github.com/BioMedIA-MBZUAI/ConDiSR\n"
    },
    {
        "title": "GiT: Towards Generalist Vision Transformer through Universal Language\n  Interface",
        "published_time": "2024-03-14T13:47:41Z",
        "abstract": "  This paper proposes a simple, yet effective framework, called GiT,\nsimultaneously applicable for various vision tasks only with a vanilla ViT.\nMotivated by the universality of the Multi-layer Transformer architecture (e.g,\nGPT) widely used in large language models (LLMs), we seek to broaden its scope\nto serve as a powerful vision foundation model (VFM). However, unlike language\nmodeling, visual tasks typically require specific modules, such as bounding box\nheads for detection and pixel decoders for segmentation, greatly hindering the\napplication of powerful multi-layer transformers in the vision domain. To solve\nthis, we design a universal language interface that empowers the successful\nauto-regressive decoding to adeptly unify various visual tasks, from\nimage-level understanding (e.g., captioning), over sparse perception (e.g.,\ndetection), to dense prediction (e.g., segmentation). Based on the above\ndesigns, the entire model is composed solely of a ViT, without any specific\nadditions, offering a remarkable architectural simplification. GiT is a\nmulti-task visual model, jointly trained across five representative benchmarks\nwithout task-specific fine-tuning. Interestingly, our GiT builds a new\nbenchmark in generalist performance, and fosters mutual enhancement across\ntasks, leading to significant improvements compared to isolated training. This\nreflects a similar impact observed in LLMs. Further enriching training with 27\ndatasets, GiT achieves strong zero-shot results over various tasks. Due to its\nsimple design, this paradigm holds promise for narrowing the architectural gap\nbetween vision and language. Code and models will be available at\n\\url{https://github.com/Haiyang-W/GiT}.\n"
    },
    {
        "title": "Event-based Asynchronous HDR Imaging by Temporal Incident Light\n  Modulation",
        "published_time": "2024-03-14T13:45:09Z",
        "abstract": "  Dynamic Range (DR) is a pivotal characteristic of imaging systems. Current\nframe-based cameras struggle to achieve high dynamic range imaging due to the\nconflict between globally uniform exposure and spatially variant scene\nillumination. In this paper, we propose AsynHDR, a Pixel-Asynchronous HDR\nimaging system, based on key insights into the challenges in HDR imaging and\nthe unique event-generating mechanism of Dynamic Vision Sensors (DVS). Our\nproposed AsynHDR system integrates the DVS with a set of LCD panels. The LCD\npanels modulate the irradiance incident upon the DVS by altering their\ntransparency, thereby triggering the pixel-independent event streams. The HDR\nimage is subsequently decoded from the event streams through our\ntemporal-weighted algorithm. Experiments under standard test platform and\nseveral challenging scenes have verified the feasibility of the system in HDR\nimaging task.\n"
    },
    {
        "title": "Impact of Synthetic Images on Morphing Attack Detection Using a Siamese\n  Network",
        "published_time": "2024-03-14T13:31:56Z",
        "abstract": "  This paper evaluated the impact of synthetic images on Morphing Attack\nDetection (MAD) using a Siamese network with a semi-hard-loss function. Intra\nand cross-dataset evaluations were performed to measure synthetic image\ngeneralisation capabilities using a cross-dataset for evaluation. Three\ndifferent pre-trained networks were used as feature extractors from traditional\nMobileNetV2, MobileNetV3 and EfficientNetB0. Our results show that MAD trained\non EfficientNetB0 from FERET, FRGCv2, and FRLL can reach a lower error rate in\ncomparison with SOTA. Conversely, worse performances were reached when the\nsystem was trained only with synthetic images. A mixed approach (synthetic +\ndigital) database may help to improve MAD and reduce the error rate. This fact\nshows that we still need to keep going with our efforts to include synthetic\nimages in the training process.\n"
    },
    {
        "title": "Introducing Routing Functions to Vision-Language Parameter-Efficient\n  Fine-Tuning with Low-Rank Bottlenecks",
        "published_time": "2024-03-14T13:27:42Z",
        "abstract": "  Mainstream parameter-efficient fine-tuning (PEFT) methods, such as LoRA or\nAdapter, project a model's hidden states to a lower dimension, allowing\npre-trained models to adapt to new data through this low-rank bottleneck.\nHowever, PEFT tasks involving multiple modalities, like vision-language (VL)\ntasks, require not only adaptation to new data but also learning the\nrelationship between different modalities. Targeting at VL PEFT tasks, we\npropose a family of operations, called routing functions, to enhance VL\nalignment in the low-rank bottlenecks. The routing functions adopt linear\noperations and do not introduce new trainable parameters. In-depth analyses are\nconducted to study their behavior. In various VL PEFT settings, the routing\nfunctions significantly improve performance of the original PEFT methods,\nachieving over 20% improvement on VQAv2\n($\\text{RoBERTa}_{\\text{large}}$+ViT-L/16) and 30% on COCO Captioning\n(GPT2-medium+ViT-L/16). Also when fine-tuning a pre-trained multimodal model\nsuch as CLIP-BART, we observe smaller but consistent improvements across a\nrange of VL PEFT tasks.\n"
    },
    {
        "title": "DF4LCZ: A SAM-Empowered Data Fusion Framework for Scene-Level Local\n  Climate Zone Classification",
        "published_time": "2024-03-14T13:15:46Z",
        "abstract": "  Recent advancements in remote sensing (RS) technologies have shown their\npotential in accurately classifying local climate zones (LCZs). However,\ntraditional scene-level methods using convolutional neural networks (CNNs)\noften struggle to integrate prior knowledge of ground objects effectively.\nMoreover, commonly utilized data sources like Sentinel-2 encounter difficulties\nin capturing detailed ground object information. To tackle these challenges, we\npropose a data fusion method that integrates ground object priors extracted\nfrom high-resolution Google imagery with Sentinel-2 multispectral imagery. The\nproposed method introduces a novel Dual-stream Fusion framework for LCZ\nclassification (DF4LCZ), integrating instance-based location features from\nGoogle imagery with the scene-level spatial-spectral features extracted from\nSentinel-2 imagery. The framework incorporates a Graph Convolutional Network\n(GCN) module empowered by the Segment Anything Model (SAM) to enhance feature\nextraction from Google imagery. Simultaneously, the framework employs a 3D-CNN\narchitecture to learn the spectral-spatial features of Sentinel-2 imagery.\nExperiments are conducted on a multi-source remote sensing image dataset\nspecifically designed for LCZ classification, validating the effectiveness of\nthe proposed DF4LCZ. The related code and dataset are available at\nhttps://github.com/ctrlovefly/DF4LCZ.\n"
    },
    {
        "title": "Unleashing Network Potentials for Semantic Scene Completion",
        "published_time": "2024-03-12T11:48:49Z",
        "abstract": "  Semantic scene completion (SSC) aims to predict complete 3D voxel occupancy\nand semantics from a single-view RGB-D image, and recent SSC methods commonly\nadopt multi-modal inputs. However, our investigation reveals two limitations:\nineffective feature learning from single modalities and overfitting to limited\ndatasets. To address these issues, this paper proposes a novel SSC framework -\nAdversarial Modality Modulation Network (AMMNet) - with a fresh perspective of\noptimizing gradient updates. The proposed AMMNet introduces two core modules: a\ncross-modal modulation enabling the interdependence of gradient flows between\nmodalities, and a customized adversarial training scheme leveraging dynamic\ngradient competition. Specifically, the cross-modal modulation adaptively\nre-calibrates the features to better excite representation potentials from each\nsingle modality. The adversarial training employs a minimax game of evolving\ngradients, with customized guidance to strengthen the generator's perception of\nvisual fidelity from both geometric completeness and semantic correctness.\nExtensive experimental results demonstrate that AMMNet outperforms\nstate-of-the-art SSC methods by a large margin, providing a promising direction\nfor improving the effectiveness and generalization of SSC methods.\n"
    },
    {
        "title": "Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without\n  Real Data Exposure",
        "published_time": "2024-03-14T13:12:49Z",
        "abstract": "  With increasing concerns over data privacy and model copyrights, especially\nin the context of collaborations between AI service providers and data owners,\nan innovative SG-ZSL paradigm is proposed in this work. SG-ZSL is designed to\nfoster efficient collaboration without the need to exchange models or sensitive\ndata. It consists of a teacher model, a student model and a generator that\nlinks both model entities. The teacher model serves as a sentinel on behalf of\nthe data owner, replacing real data, to guide the student model at the AI\nservice provider's end during training. Considering the disparity of knowledge\nspace between the teacher and student, we introduce two variants of the teacher\nmodel: the omniscient and the quasi-omniscient teachers. Under these teachers'\nguidance, the student model seeks to match the teacher model's performance and\nexplores domains that the teacher has not covered. To trade off between privacy\nand performance, we further introduce two distinct security-level training\nprotocols: white-box and black-box, enhancing the paradigm's adaptability.\nDespite the inherent challenges of real data absence in the SG-ZSL paradigm, it\nconsistently outperforms in ZSL and GZSL tasks, notably in the white-box\nprotocol. Our comprehensive evaluation further attests to its robustness and\nefficiency across various setups, including stringent black-box training\nprotocol.\n"
    },
    {
        "title": "D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap\n  for Domain-Adaptive Object Detection",
        "published_time": "2024-03-14T13:05:43Z",
        "abstract": "  Domain adaptation for object detection typically entails transferring\nknowledge from one visible domain to another visible domain. However, there are\nlimited studies on adapting from the visible to the thermal domain, because the\ndomain gap between the visible and thermal domains is much larger than\nexpected, and traditional domain adaptation can not successfully facilitate\nlearning in this situation. To overcome this challenge, we propose a\nDistinctive Dual-Domain Teacher (D3T) framework that employs distinct training\nparadigms for each domain. Specifically, we segregate the source and target\ntraining sets for building dual-teachers and successively deploy exponential\nmoving average to the student model to individual teachers of each domain. The\nframework further incorporates a zigzag learning method between dual teachers,\nfacilitating a gradual transition from the visible to thermal domains during\ntraining. We validate the superiority of our method through newly designed\nexperimental protocols with well-known thermal datasets, i.e., FLIR and KAIST.\nSource code is available at https://github.com/EdwardDo69/D3T .\n"
    },
    {
        "title": "Mitigating Data Consistency Induced Discrepancy in Cascaded Diffusion\n  Models for Sparse-view CT Reconstruction",
        "published_time": "2024-03-14T12:58:28Z",
        "abstract": "  Sparse-view Computed Tomography (CT) image reconstruction is a promising\napproach to reduce radiation exposure, but it inevitably leads to image\ndegradation. Although diffusion model-based approaches are computationally\nexpensive and suffer from the training-sampling discrepancy, they provide a\npotential solution to the problem. This study introduces a novel Cascaded\nDiffusion with Discrepancy Mitigation (CDDM) framework, including the\nlow-quality image generation in latent space and the high-quality image\ngeneration in pixel space which contains data consistency and discrepancy\nmitigation in a one-step reconstruction process. The cascaded framework\nminimizes computational costs by moving some inference steps from pixel space\nto latent space. The discrepancy mitigation technique addresses the\ntraining-sampling gap induced by data consistency, ensuring the data\ndistribution is close to the original manifold. A specialized Alternating\nDirection Method of Multipliers (ADMM) is employed to process image gradients\nin separate directions, offering a more targeted approach to regularization.\nExperimental results across two datasets demonstrate CDDM's superior\nperformance in high-quality image generation with clearer boundaries compared\nto existing methods, highlighting the framework's computational efficiency.\n"
    },
    {
        "title": "AVIBench: Towards Evaluating the Robustness of Large Vision-Language\n  Model on Adversarial Visual-Instructions",
        "published_time": "2024-03-14T12:51:07Z",
        "abstract": "  Large Vision-Language Models (LVLMs) have shown significant progress in well\nresponding to visual-instructions from users. However, these instructions,\nencompassing images and text, are susceptible to both intentional and\ninadvertent attacks. Despite the critical importance of LVLMs' robustness\nagainst such threats, current research in this area remains limited. To bridge\nthis gap, we introduce AVIBench, a framework designed to analyze the robustness\nof LVLMs when facing various adversarial visual-instructions (AVIs), including\nfour types of image-based AVIs, ten types of text-based AVIs, and nine types of\ncontent bias AVIs (such as gender, violence, cultural, and racial biases, among\nothers). We generate 260K AVIs encompassing five categories of multimodal\ncapabilities (nine tasks) and content bias. We then conduct a comprehensive\nevaluation involving 14 open-source LVLMs to assess their performance. AVIBench\nalso serves as a convenient tool for practitioners to evaluate the robustness\nof LVLMs against AVIs. Our findings and extensive experimental results shed\nlight on the vulnerabilities of LVLMs, and highlight that inherent biases exist\neven in advanced closed-source LVLMs like GeminiProVision and GPT-4V. This\nunderscores the importance of enhancing the robustness, security, and fairness\nof LVLMs. The source code and benchmark will be made publicly available.\n"
    },
    {
        "title": "SketchINR: A First Look into Sketches as Implicit Neural Representations",
        "published_time": "2024-03-14T12:49:29Z",
        "abstract": "  We propose SketchINR, to advance the representation of vector sketches with\nimplicit neural models. A variable length vector sketch is compressed into a\nlatent space of fixed dimension that implicitly encodes the underlying shape as\na function of time and strokes. The learned function predicts the $xy$ point\ncoordinates in a sketch at each time and stroke. Despite its simplicity,\nSketchINR outperforms existing representations at multiple tasks: (i) Encoding\nan entire sketch dataset into a fixed size latent vector, SketchINR gives\n$60\\times$ and $10\\times$ data compression over raster and vector sketches,\nrespectively. (ii) SketchINR's auto-decoder provides a much higher-fidelity\nrepresentation than other learned vector sketch representations, and is\nuniquely able to scale to complex vector sketches such as FS-COCO. (iii)\nSketchINR supports parallelisation that can decode/render $\\sim$$100\\times$\nfaster than other learned vector representations such as SketchRNN. (iv)\nSketchINR, for the first time, emulates the human ability to reproduce a sketch\nwith varying abstraction in terms of number and complexity of strokes. As a\nfirst look at implicit sketches, SketchINR's compact high-fidelity\nrepresentation will support future work in modelling long and complex sketches.\n"
    },
    {
        "title": "LocalMamba: Visual State Space Model with Windowed Selective Scan",
        "published_time": "2024-03-14T12:32:40Z",
        "abstract": "  Recent advancements in state space models, notably Mamba, have demonstrated\nsignificant progress in modeling long sequences for tasks like language\nunderstanding. Yet, their application in vision tasks has not markedly\nsurpassed the performance of traditional Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). This paper posits that the key to enhancing\nVision Mamba (ViM) lies in optimizing scan directions for sequence modeling.\nTraditional ViM approaches, which flatten spatial tokens, overlook the\npreservation of local 2D dependencies, thereby elongating the distance between\nadjacent tokens. We introduce a novel local scanning strategy that divides\nimages into distinct windows, effectively capturing local dependencies while\nmaintaining a global perspective. Additionally, acknowledging the varying\npreferences for scan patterns across different network layers, we propose a\ndynamic method to independently search for the optimal scan choices for each\nlayer, substantially improving performance. Extensive experiments across both\nplain and hierarchical models underscore our approach's superiority in\neffectively capturing image representations. For example, our model\nsignificantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs.\nCode is available at: https://github.com/hunto/LocalMamba.\n"
    },
    {
        "title": "Consistent Prompting for Rehearsal-Free Continual Learning",
        "published_time": "2024-03-13T14:24:09Z",
        "abstract": "  Continual learning empowers models to adapt autonomously to the ever-changing\nenvironment or data streams without forgetting old knowledge. Prompt-based\napproaches are built on frozen pre-trained models to learn the task-specific\nprompts and classifiers efficiently. Existing prompt-based methods are\ninconsistent between training and testing, limiting their effectiveness. Two\ntypes of inconsistency are revealed. Test predictions are made from all\nclassifiers while training only focuses on the current task classifier without\nholistic alignment, leading to Classifier inconsistency. Prompt inconsistency\nindicates that the prompt selected during testing may not correspond to the one\nassociated with this task during training. In this paper, we propose a novel\nprompt-based method, Consistent Prompting (CPrompt), for more aligned training\nand testing. Specifically, all existing classifiers are exposed to prompt\ntraining, resulting in classifier consistency learning. In addition, prompt\nconsistency learning is proposed to enhance prediction robustness and boost\nprompt selection accuracy. Our Consistent Prompting surpasses its prompt-based\ncounterparts and achieves state-of-the-art performance on multiple continual\nlearning benchmarks. Detailed analysis shows that improvements come from more\nconsistent training and testing.\n"
    },
    {
        "title": "Video Editing via Factorized Diffusion Distillation",
        "published_time": "2024-03-14T12:22:54Z",
        "abstract": "  We introduce Emu Video Edit (EVE), a model that establishes a new\nstate-of-the art in video editing without relying on any supervised video\nediting data. To develop EVE we separately train an image editing adapter and a\nvideo generation adapter, and attach both to the same text-to-image model.\nThen, to align the adapters towards video editing we introduce a new\nunsupervised distillation procedure, Factorized Diffusion Distillation. This\nprocedure distills knowledge from one or more teachers simultaneously, without\nany supervised data. We utilize this procedure to teach EVE to edit videos by\njointly distilling knowledge to (i) precisely edit each individual frame from\nthe image editing adapter, and (ii) ensure temporal consistency among the\nedited frames using the video generation adapter. Finally, to demonstrate the\npotential of our approach in unlocking other capabilities, we align additional\ncombinations of adapters\n"
    },
    {
        "title": "Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling\n  and Visual-Language Co-Referring",
        "published_time": "2024-03-14T12:21:37Z",
        "abstract": "  Large Vision Language Models have achieved fine-grained object perception,\nbut the limitation of image resolution remains a significant obstacle to\nsurpass the performance of task-specific experts in complex and dense\nscenarios. Such limitation further restricts the model's potential to achieve\nnuanced visual and language referring in domains such as GUI Agents, Counting\nand \\etc. To address this issue, we introduce a unified high-resolution\ngeneralist model, Griffon v2, enabling flexible object referring with visual\nand textual prompts. To efficiently scaling up image resolution, we design a\nsimple and lightweight down-sampling projector to overcome the input tokens\nconstraint in Large Language Models. This design inherently preserves the\ncomplete contexts and fine details, and significantly improves multimodal\nperception ability especially for small objects. Building upon this, we further\nequip the model with visual-language co-referring capabilities through a\nplug-and-play visual tokenizer. It enables user-friendly interaction with\nflexible target images, free-form texts and even coordinates. Experiments\ndemonstrate that Griffon v2 can localize any objects of interest with visual\nand textual referring, achieve state-of-the-art performance on REC, phrase\ngrounding, and REG tasks, and outperform expert models in object detection and\nobject counting. Data, codes and models will be released at\nhttps://github.com/jefferyZhan/Griffon.\n"
    },
    {
        "title": "Perspective-Equivariant Imaging: an Unsupervised Framework for\n  Multispectral Pansharpening",
        "published_time": "2024-03-14T12:17:07Z",
        "abstract": "  Ill-posed image reconstruction problems appear in many scenarios such as\nremote sensing, where obtaining high quality images is crucial for\nenvironmental monitoring, disaster management and urban planning. Deep learning\nhas seen great success in overcoming the limitations of traditional methods.\nHowever, these inverse problems rarely come with ground truth data,\nhighlighting the importance of unsupervised learning from partial and noisy\nmeasurements alone. We propose perspective-equivariant imaging (EI), a\nframework that leverages perspective variability in optical camera-based\nimaging systems, such as satellites or handheld cameras, to recover information\nlost in ill-posed optical camera imaging problems. This extends previous EI\nwork to include a much richer non-linear class of group transforms and is shown\nto be an excellent prior for satellite and urban image data, where\nperspective-EI achieves state-of-the-art results in multispectral\npansharpening, outperforming other unsupervised methods in the literature. Code\nat https://andrewwango.github.io/perspective-equivariant-imaging\n"
    },
    {
        "title": "GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting\n  Editing",
        "published_time": "2024-03-13T17:35:28Z",
        "abstract": "  We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed\nby the 3D Gaussian Splatting (3DGS).\n  Our method first renders a collection of images by using the 3DGS and edits\nthem by using a pre-trained 2D diffusion model (ControlNet) based on the input\nprompt, which is then used to optimise the 3D model.\n  Our key contribution is multi-view consistent editing, which enables editing\nall images together instead of iteratively editing one image while updating the\n3D model as in previous works.\n  It leads to faster editing as well as higher visual quality.\n  This is achieved by the two terms:\n  (a) depth-conditioned editing that enforces geometric consistency across\nmulti-view images by leveraging naturally consistent depth maps.\n  (b) attention-based latent code alignment that unifies the appearance of\nedited images by conditioning their editing to several reference views through\nself and cross-view attention between images' latent representations.\n  Experiments demonstrate that our method achieves faster editing and better\nvisual results than previous state-of-the-art methods.\n"
    },
    {
        "title": "EfficientMFD: Towards More Efficient Multimodal Synchronous Fusion\n  Detection",
        "published_time": "2024-03-14T12:12:17Z",
        "abstract": "  Multimodal image fusion and object detection play a vital role in autonomous\ndriving. Current joint learning methods have made significant progress in the\nmultimodal fusion detection task combining the texture detail and objective\nsemantic information. However, the tedious training steps have limited its\napplications to wider real-world industrial deployment. To address this\nlimitation, we propose a novel end-to-end multimodal fusion detection\nalgorithm, named EfficientMFD, to simplify models that exhibit decent\nperformance with only one training step. Synchronous joint optimization is\nutilized in an end-to-end manner between two components, thus not being\naffected by the local optimal solution of the individual task. Besides, a\ncomprehensive optimization is established in the gradient matrix between the\nshared parameters for both tasks. It can converge to an optimal point with\nfusion detection weights. We extensively test it on several public datasets,\ndemonstrating superior performance on not only visually appealing fusion but\nalso favorable detection performance (e.g., 6.6% mAP50:95) over other\nstate-of-the-art approaches.\n"
    },
    {
        "title": "SD-Net: Symmetric-Aware Keypoint Prediction and Domain Adaptation for 6D\n  Pose Estimation In Bin-picking Scenarios",
        "published_time": "2024-03-14T12:08:44Z",
        "abstract": "  Despite the success in 6D pose estimation in bin-picking scenarios, existing\nmethods still struggle to produce accurate prediction results for symmetry\nobjects and real world scenarios. The primary bottlenecks include 1) the\nambiguity keypoints caused by object symmetries; 2) the domain gap between real\nand synthetic data. To circumvent these problem, we propose a new 6D pose\nestimation network with symmetric-aware keypoint prediction and self-training\ndomain adaptation (SD-Net). SD-Net builds on pointwise keypoint regression and\ndeep hough voting to perform reliable detection keypoint under clutter and\nocclusion. Specifically, at the keypoint prediction stage, we designe a robust\n3D keypoints selection strategy considering the symmetry class of objects and\nequivalent keypoints, which facilitate locating 3D keypoints even in highly\noccluded scenes. Additionally, we build an effective filtering algorithm on\npredicted keypoint to dynamically eliminate multiple ambiguity and outlier\nkeypoint candidates. At the domain adaptation stage, we propose the\nself-training framework using a student-teacher training scheme. To carefully\ndistinguish reliable predictions, we harnesses a tailored heuristics for 3D\ngeometry pseudo labelling based on semi-chamfer distance. On public Sil'eane\ndataset, SD-Net achieves state-of-the-art results, obtaining an average\nprecision of 96%. Testing learning and generalization abilities on public\nParametric datasets, SD-Net is 8% higher than the state-of-the-art method. The\ncode is available at https://github.com/dingthuang/SD-Net.\n"
    },
    {
        "title": "Semi- and Weakly-Supervised Learning for Mammogram Mass Segmentation\n  with Limited Annotations",
        "published_time": "2024-03-14T12:05:25Z",
        "abstract": "  Accurate identification of breast masses is crucial in diagnosing breast\ncancer; however, it can be challenging due to their small size and being\ncamouflaged in surrounding normal glands. Worse still, it is also expensive in\nclinical practice to obtain adequate pixel-wise annotations for training deep\nneural networks. To overcome these two difficulties with one stone, we propose\na semi- and weakly-supervised learning framework for mass segmentation that\nutilizes limited strongly-labeled samples and sufficient weakly-labeled samples\nto achieve satisfactory performance. The framework consists of an auxiliary\nbranch to exclude lesion-irrelevant background areas, a segmentation branch for\nfinal prediction, and a spatial prompting module to integrate the complementary\ninformation of the two branches. We further disentangle encoded obscure\nfeatures into lesion-related and others to boost performance. Experiments on\nCBIS-DDSM and INbreast datasets demonstrate the effectiveness of our method.\n"
    },
    {
        "title": "Knowledge Distillation in YOLOX-ViT for Side-Scan Sonar Object Detection",
        "published_time": "2024-03-14T12:03:28Z",
        "abstract": "  In this paper we present YOLOX-ViT, a novel object detection model, and\ninvestigate the efficacy of knowledge distillation for model size reduction\nwithout sacrificing performance. Focused on underwater robotics, our research\naddresses key questions about the viability of smaller models and the impact of\nthe visual transformer layer in YOLOX. Furthermore, we introduce a new\nside-scan sonar image dataset, and use it to evaluate our object detector's\nperformance. Results show that knowledge distillation effectively reduces false\npositives in wall detection. Additionally, the introduced visual transformer\nlayer significantly improves object detection accuracy in the underwater\nenvironment. The source code of the knowledge distillation in the YOLOX-ViT is\nat https://github.com/remaro-network/KD-YOLOX-ViT.\n"
    },
    {
        "title": "Annotation Free Semantic Segmentation with Vision Foundation Models",
        "published_time": "2024-03-14T11:57:58Z",
        "abstract": "  Semantic Segmentation is one of the most challenging vision tasks, usually\nrequiring large amounts of training data with expensive pixel-level\nannotations. With the success of foundation models and especially\nvision-language models, recent works attempt to achieve zero-shot semantic\nsegmentation while requiring either large scale training or additional\nimage/pixel-level annotations. In this work, we build a lightweight module on\ntop of a self-supervised pretrained vision encoder to align patch features with\na pre-trained text encoder. Importantly, we generate free annotations for any\nsemantic segmentation dataset using existing foundation models and train our\nalignment module cost free. We use CLIP to detect objects and SAM to generate\nhigh quality object masks. Our approach can bring language-based semantics to\nany pre-trained vision encoder with minimal training. Our module is\nlightweight, uses foundation models as a sole source of supervision and shows\nimpressive generalization capability from little training data with no\nannotation.\n"
    },
    {
        "title": "Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical\n  Perspective",
        "published_time": "2024-03-14T11:51:01Z",
        "abstract": "  Medical anomaly detection aims to identify abnormal findings using only\nnormal training data, playing a crucial role in health screening and\nrecognizing rare diseases. Reconstruction-based methods, particularly those\nutilizing autoencoders (AEs), are dominant in this field. They work under the\nassumption that AEs trained on only normal data cannot reconstruct unseen\nabnormal regions well, thereby enabling the anomaly detection based on\nreconstruction errors. However, this assumption does not always hold due to the\nmismatch between the reconstruction training objective and the anomaly\ndetection task objective, rendering these methods theoretically unsound. This\nstudy focuses on providing a theoretical foundation for AE-based reconstruction\nmethods in anomaly detection. By leveraging information theory, we elucidate\nthe principles of these methods and reveal that the key to improving AE in\nanomaly detection lies in minimizing the information entropy of latent vectors.\nExperiments on four datasets with two image modalities validate the\neffectiveness of our theory. To the best of our knowledge, this is the first\neffort to theoretically clarify the principles and design philosophy of AE for\nanomaly detection. Code will be available upon acceptance.\n"
    },
    {
        "title": "StainFuser: Controlling Diffusion for Faster Neural Style Transfer in\n  Multi-Gigapixel Histology Images",
        "published_time": "2024-03-14T11:49:43Z",
        "abstract": "  Stain normalization algorithms aim to transform the color and intensity\ncharacteristics of a source multi-gigapixel histology image to match those of a\ntarget image, mitigating inconsistencies in the appearance of stains used to\nhighlight cellular components in the images. We propose a new approach,\nStainFuser, which treats this problem as a style transfer task using a novel\nConditional Latent Diffusion architecture, eliminating the need for handcrafted\ncolor components. With this method, we curate SPI-2M the largest stain\nnormalization dataset to date of over 2 million histology images with neural\nstyle transfer for high-quality transformations. Trained on this data,\nStainFuser outperforms current state-of-the-art GAN and handcrafted methods in\nterms of the quality of normalized images. Additionally, compared to existing\napproaches, it improves the performance of nuclei instance segmentation and\nclassification models when used as a test time augmentation method on the\nchallenging CoNIC dataset. Finally, we apply StainFuser on multi-gigapixel\nWhole Slide Images (WSIs) and demonstrate improved performance in terms of\ncomputational efficiency, image quality and consistency across tiles over\ncurrent methods.\n"
    },
    {
        "title": "Select and Distill: Selective Dual-Teacher Knowledge Transfer for\n  Continual Learning on Vision-Language Models",
        "published_time": "2024-03-14T11:36:36Z",
        "abstract": "  Large-scale vision-language models (VLMs) have shown a strong zero-shot\ngeneralization capability on unseen-domain data. However, when adapting\npre-trained VLMs to a sequence of downstream tasks, they are prone to\nforgetting previously learned knowledge and degrade their zero-shot\nclassification capability. To tackle this problem, we propose a unique\nSelective Dual-Teacher Knowledge Transfer framework that leverages the most\nrecent fine-tuned and the original pre-trained VLMs as dual teachers to\npreserve the previously learned knowledge and zero-shot capabilities,\nrespectively. With only access to an unlabeled reference dataset, our proposed\nframework performs a selective knowledge distillation mechanism by measuring\nthe feature discrepancy from the dual teacher VLMs. Consequently, our selective\ndual-teacher knowledge distillation would mitigate catastrophic forgetting of\npreviously learned knowledge while preserving the zero-shot capabilities from\npre-trained VLMs. Through extensive experiments on benchmark datasets, we show\nthat our proposed framework is favorable against state-of-the-art continual\nlearning approaches for preventing catastrophic forgetting and zero-shot\ndegradation.\n"
    },
    {
        "title": "Anatomical Structure-Guided Medical Vision-Language Pre-training",
        "published_time": "2024-03-14T11:29:47Z",
        "abstract": "  Learning medical visual representations through vision-language pre-training\nhas reached remarkable progress. Despite the promising performance, it still\nfaces challenges, i.e., local alignment lacks interpretability and clinical\nrelevance, and the insufficient internal and external representation learning\nof image-report pairs. To address these issues, we propose an Anatomical\nStructure-Guided (ASG) framework. Specifically, we parse raw reports into\ntriplets <anatomical region, finding, existence>, and fully utilize each\nelement as supervision to enhance representation learning. For anatomical\nregion, we design an automatic anatomical region-sentence alignment paradigm in\ncollaboration with radiologists, considering them as the minimum semantic units\nto explore fine-grained local alignment. For finding and existence, we regard\nthem as image tags, applying an image-tag recognition decoder to associate\nimage features with their respective tags within each sample and constructing\nsoft labels for contrastive learning to improve the semantic association of\ndifferent image-report pairs. We evaluate the proposed ASG framework on two\ndownstream tasks, including five public benchmarks. Experimental results\ndemonstrate that our method outperforms the state-of-the-art methods.\n"
    },
    {
        "title": "SELECTOR: Heterogeneous graph network with convolutional masked\n  autoencoder for multimodal robust prediction of cancer survival",
        "published_time": "2024-03-14T11:23:39Z",
        "abstract": "  Accurately predicting the survival rate of cancer patients is crucial for\naiding clinicians in planning appropriate treatment, reducing cancer-related\nmedical expenses, and significantly enhancing patients' quality of life.\nMultimodal prediction of cancer patient survival offers a more comprehensive\nand precise approach. However, existing methods still grapple with challenges\nrelated to missing multimodal data and information interaction within\nmodalities. This paper introduces SELECTOR, a heterogeneous graph-aware network\nbased on convolutional mask encoders for robust multimodal prediction of cancer\npatient survival. SELECTOR comprises feature edge reconstruction, convolutional\nmask encoder, feature cross-fusion, and multimodal survival prediction modules.\nInitially, we construct a multimodal heterogeneous graph and employ the\nmeta-path method for feature edge reconstruction, ensuring comprehensive\nincorporation of feature information from graph edges and effective embedding\nof nodes. To mitigate the impact of missing features within the modality on\nprediction accuracy, we devised a convolutional masked autoencoder (CMAE) to\nprocess the heterogeneous graph post-feature reconstruction. Subsequently, the\nfeature cross-fusion module facilitates communication between modalities,\nensuring that output features encompass all features of the modality and\nrelevant information from other modalities. Extensive experiments and analysis\non six cancer datasets from TCGA demonstrate that our method significantly\noutperforms state-of-the-art methods in both modality-missing and\nintra-modality information-confirmed cases. Our codes are made available at\nhttps://github.com/panliangrui/Selector.\n"
    },
    {
        "title": "Adversarial Training with OCR Modality Perturbation for Scene-Text\n  Visual Question Answering",
        "published_time": "2024-03-14T11:22:06Z",
        "abstract": "  Scene-Text Visual Question Answering (ST-VQA) aims to understand scene text\nin images and answer questions related to the text content. Most existing\nmethods heavily rely on the accuracy of Optical Character Recognition (OCR)\nsystems, and aggressive fine-tuning based on limited spatial location\ninformation and erroneous OCR text information often leads to inevitable\noverfitting. In this paper, we propose a multimodal adversarial training\narchitecture with spatial awareness capabilities. Specifically, we introduce an\nAdversarial OCR Enhancement (AOE) module, which leverages adversarial training\nin the embedding space of OCR modality to enhance fault-tolerant representation\nof OCR texts, thereby reducing noise caused by OCR errors. Simultaneously, We\nadd a Spatial-Aware Self-Attention (SASA) mechanism to help the model better\ncapture the spatial relationships among OCR tokens. Various experiments\ndemonstrate that our method achieves significant performance improvements on\nboth the ST-VQA and TextVQA datasets and provides a novel paradigm for\nmultimodal adversarial training.\n"
    },
    {
        "title": "Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal\n  Correlation",
        "published_time": "2024-01-16T12:48:52Z",
        "abstract": "  Recent test-time adaptation methods heavily rely on nuanced adjustments of\nbatch normalization (BN) parameters. However, one critical assumption often\ngoes overlooked: that of independently and identically distributed (i.i.d.)\ntest batches with respect to unknown labels. This oversight leads to skewed BN\nstatistics and undermines the reliability of the model under non-i.i.d.\nscenarios. To tackle this challenge, this paper presents a novel method termed\n'Un-Mixing Test-Time Normalization Statistics' (UnMix-TNS). Our method\nre-calibrates the statistics for each instance within a test batch by mixing it\nwith multiple distinct statistics components, thus inherently simulating the\ni.i.d. scenario. The core of this method hinges on a distinctive online\nunmixing procedure that continuously updates these statistics components by\nincorporating the most similar instances from new test batches. Remarkably\ngeneric in its design, UnMix-TNS seamlessly integrates with a wide range of\nleading test-time adaptation methods and pre-trained architectures equipped\nwith BN layers. Empirical evaluations corroborate the robustness of UnMix-TNS\nunder varied scenarios-ranging from single to continual and mixed domain\nshifts, particularly excelling with temporally correlated test data and\ncorrupted non-i.i.d. real-world streams. This adaptability is maintained even\nwith very small batch sizes or single instances. Our results highlight\nUnMix-TNS's capacity to markedly enhance stability and performance across\nvarious benchmarks. Our code is publicly available at\nhttps://github.com/devavratTomar/unmixtns.\n"
    },
    {
        "title": "CLIP-EBC: CLIP Can Count Accurately through Enhanced Blockwise\n  Classification",
        "published_time": "2024-03-14T11:08:33Z",
        "abstract": "  The CLIP (Contrastive Language-Image Pretraining) model has exhibited\noutstanding performance in recognition problems, such as zero-shot image\nclassification and object detection. However, its ability to count remains\nunderstudied due to the inherent challenges of transforming counting--a\nregression task--into a recognition task. In this paper, we investigate CLIP's\npotential in counting, focusing specifically on estimating crowd sizes.\nExisting classification-based crowd-counting methods have encountered issues,\nincluding inappropriate discretization strategies, which impede the application\nof CLIP and result in suboptimal performance. To address these challenges, we\npropose the Enhanced Blockwise Classification (EBC) framework. In contrast to\nprevious methods, EBC relies on integer-valued bins that facilitate the\nlearning of robust decision boundaries. Within our model-agnostic EBC\nframework, we introduce CLIP-EBC, the first fully CLIP-based crowd-counting\nmodel capable of generating density maps. Comprehensive evaluations across\ndiverse crowd-counting datasets demonstrate the state-of-the-art performance of\nour methods. Particularly, EBC can improve existing models by up to 76.9%.\nMoreover, our CLIP-EBC model surpasses current crowd-counting methods,\nachieving mean absolute errors of 55.0 and 6.3 on ShanghaiTech part A and part\nB datasets, respectively. The code will be made publicly available.\n"
    },
    {
        "title": "EventRPG: Event Data Augmentation with Relevance Propagation Guidance",
        "published_time": "2024-03-14T10:52:45Z",
        "abstract": "  Event camera, a novel bio-inspired vision sensor, has drawn a lot of\nattention for its low latency, low power consumption, and high dynamic range.\nCurrently, overfitting remains a critical problem in event-based classification\ntasks for Spiking Neural Network (SNN) due to its relatively weak spatial\nrepresentation capability. Data augmentation is a simple but efficient method\nto alleviate overfitting and improve the generalization ability of neural\nnetworks, and saliency-based augmentation methods are proven to be effective in\nthe image processing field. However, there is no approach available for\nextracting saliency maps from SNNs. Therefore, for the first time, we present\nSpiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking\nLayer-wise Relevance Propagation rule (SLRP) in order for SNN to generate\nstable and accurate CAMs and saliency maps. Based on this, we propose EventRPG,\nwhich leverages relevance propagation on the spiking neural network for more\nefficient augmentation. Our proposed method has been evaluated on several SNN\nstructures, achieving state-of-the-art performance in object recognition tasks\nincluding N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as\nwell as action recognition task SL-Animals with an accuracy of 91.59%. Our code\nis available at https://github.com/myuansun/EventRPG.\n"
    },
    {
        "title": "Advanced Tumor Segmentation in Medical Imaging: An Ensemble Approach for\n  BraTS 2023 Adult Glioma and Pediatric Tumor Tasks",
        "published_time": "2024-03-14T10:37:41Z",
        "abstract": "  Automated segmentation proves to be a valuable tool in precisely detecting\ntumors within medical images. The accurate identification and segmentation of\ntumor types hold paramount importance in diagnosing, monitoring, and treating\nhighly fatal brain tumors. The BraTS challenge serves as a platform for\nresearchers to tackle this issue by participating in open challenges focused on\ntumor segmentation. This study outlines our methodology for segmenting tumors\nin the context of two distinct tasks from the BraTS 2023 challenge: Adult\nGlioma and Pediatric Tumors. Our approach leverages two encoder-decoder-based\nCNN models, namely SegResNet and MedNeXt, for segmenting three distinct\nsubregions of tumors. We further introduce a set of robust postprocessing to\nimprove the segmentation, especially for the newly introduced BraTS 2023\nmetrics. The specifics of our approach and comprehensive performance analyses\nare expounded upon in this work. Our proposed approach achieves third place in\nthe BraTS 2023 Adult Glioma Segmentation Challenges with an average of 0.8313\nand 36.38 Dice and HD95 scores on the test set, respectively.\n"
    },
    {
        "title": "WSI-SAM: Multi-resolution Segment Anything Model (SAM) for\n  histopathology whole-slide images",
        "published_time": "2024-03-14T10:30:43Z",
        "abstract": "  The Segment Anything Model (SAM) marks a significant advancement in\nsegmentation models, offering powerful zero-shot capabilities and dynamic\nprompting. However, existing medical SAMs are not suitable for the multi-scale\nnature of whole-slide images (WSIs), restricting their effectiveness. To\nresolve this drawback, we present WSI-SAM, enhancing SAM with precise object\nsegmentation capabilities for histopathology images using multi-resolution\npatches, while preserving its original prompt-driven design, efficiency, and\nzero-shot adaptability. To fully exploit pretrained knowledge while minimizing\ntraining overhead, we keep SAM frozen, only introducing minimal additional\nparameters and computation. In particular, we introduce High-Resolution (HR)\ntoken, Low-Resolution (LR) token and dual mask decoder. This decoder integrates\nthe original SAM mask decoder with a lightweight fusion module that integrates\nfeatures at multiple scales. Instead of predicting a mask independently, we\nintegrate HR and LR token at intermediate layer to jointly learn features of\nthe same object across multiple resolutions. Experiments show that our WSI-SAM\noutperforms state-of-the-art SAM and its variants. In particular, our model\noutperforms SAM by 4.1 and 2.5 percent points on a ductal carcinoma in situ\n(DCIS) segmentation tasks and breast cancer metastasis segmentation task\n(CAMELYON16 dataset). The code will be available at\nhttps://github.com/HongLiuuuuu/WSI-SAM.\n"
    },
    {
        "title": "XReal: Realistic Anatomy and Pathology-Aware X-ray Generation via\n  Controllable Diffusion Model",
        "published_time": "2024-03-14T10:03:58Z",
        "abstract": "  Large-scale generative models have demonstrated impressive capacity in\nproducing visually compelling images, with increasing applications in medical\nimaging. However, they continue to grapple with the challenge of image\nhallucination and the generation of anatomically inaccurate outputs. These\nlimitations are mainly due to the sole reliance on textual inputs and lack of\nspatial control over the generated images, hindering the potential usefulness\nof such models in real-life settings. We present XReal, a novel controllable\ndiffusion model for generating realistic chest X-ray images through precise\nanatomy and pathology location control. Our lightweight method can seamlessly\nintegrate spatial control in a pre-trained text-to-image diffusion model\nwithout fine-tuning, retaining its existing knowledge while enhancing its\ngeneration capabilities. XReal outperforms state-of-the-art x-ray diffusion\nmodels in quantitative and qualitative metrics while showing 13% and 10%\nanatomy and pathology realism gain, respectively, based on the expert\nradiologist evaluation. Our model holds promise for advancing generative models\nin medical imaging, offering greater precision and adaptability while inviting\nfurther exploration in this evolving field. A large synthetically generated\ndata with annotations and code is publicly available at\nhttps://github.com/BioMedIA-MBZUAI/XReal.\n"
    },
    {
        "title": "Hyper-3DG: Text-to-3D Gaussian Generation via Hypergraph",
        "published_time": "2024-03-14T09:59:55Z",
        "abstract": "  Text-to-3D generation represents an exciting field that has seen rapid\nadvancements, facilitating the transformation of textual descriptions into\ndetailed 3D models. However, current progress often neglects the intricate\nhigh-order correlation of geometry and texture within 3D objects, leading to\nchallenges such as over-smoothness, over-saturation and the Janus problem. In\nthis work, we propose a method named ``3D Gaussian Generation via Hypergraph\n(Hyper-3DG)'', designed to capture the sophisticated high-order correlations\npresent within 3D objects. Our framework is anchored by a well-established\nmainflow and an essential module, named ``Geometry and Texture Hypergraph\nRefiner (HGRefiner)''. This module not only refines the representation of 3D\nGaussians but also accelerates the update process of these 3D Gaussians by\nconducting the Patch-3DGS Hypergraph Learning on both explicit attributes and\nlatent visual features. Our framework allows for the production of finely\ngenerated 3D objects within a cohesive optimization, effectively circumventing\ndegradation. Extensive experimentation has shown that our proposed method\nsignificantly enhances the quality of 3D generation while incurring no\nadditional computational overhead for the underlying framework. (Project code:\nhttps://github.com/yjhboy/Hyper3DG)\n"
    },
    {
        "title": "D-YOLO a robust framework for object detection in adverse weather\n  conditions",
        "published_time": "2024-03-14T09:57:15Z",
        "abstract": "  Adverse weather conditions including haze, snow and rain lead to decline in\nimage qualities, which often causes a decline in performance for deep-learning\nbased detection networks. Most existing approaches attempts to rectify hazy\nimages before performing object detection, which increases the complexity of\nthe network and may result in the loss in latent information. To better\nintegrate image restoration and object detection tasks, we designed a\ndouble-route network with an attention feature fusion module, taking both hazy\nand dehazed features into consideration. We also proposed a subnetwork to\nprovide haze-free features to the detection network. Specifically, our D-YOLO\nimproves the performance of the detection network by minimizing the distance\nbetween the clear feature extraction subnetwork and detection network.\nExperiments on RTTS and FoggyCityscapes datasets show that D-YOLO demonstrates\nbetter performance compared to the state-of-the-art methods. It is a robust\ndetection framework for bridging the gap between low-level dehazing and\nhigh-level detection.\n"
    },
    {
        "title": "Improving Distant 3D Object Detection Using 2D Box Supervision",
        "published_time": "2024-03-14T09:54:31Z",
        "abstract": "  Improving the detection of distant 3d objects is an important yet challenging\ntask. For camera-based 3D perception, the annotation of 3d bounding relies\nheavily on LiDAR for accurate depth information. As such, the distance of\nannotation is often limited due to the sparsity of LiDAR points on distant\nobjects, which hampers the capability of existing detectors for long-range\nscenarios. We address this challenge by considering only 2D box supervision for\ndistant objects since they are easy to annotate. We propose LR3D, a framework\nthat learns to recover the missing depth of distant objects. LR3D adopts an\nimplicit projection head to learn the generation of mapping between 2D boxes\nand depth using the 3D supervision on close objects. This mapping allows the\ndepth estimation of distant objects conditioned on their 2D boxes, making\nlong-range 3D detection with 2D supervision feasible. Experiments show that\nwithout distant 3D annotations, LR3D allows camera-based methods to detect\ndistant objects (over 200m) with comparable accuracy to full 3D supervision.\nOur framework is general, and could widely benefit 3D detection methods to a\nlarge extent.\n"
    },
    {
        "title": "PoIFusion: Multi-Modal 3D Object Detection via Fusion at Points of\n  Interest",
        "published_time": "2024-03-14T09:28:12Z",
        "abstract": "  In this work, we present PoIFusion, a simple yet effective multi-modal 3D\nobject detection framework to fuse the information of RGB images and LiDAR\npoint clouds at the point of interest (abbreviated as PoI). Technically, our\nPoIFusion follows the paradigm of query-based object detection, formulating\nobject queries as dynamic 3D boxes. The PoIs are adaptively generated from each\nquery box on the fly, serving as the keypoints to represent a 3D object and\nplay the role of basic units in multi-modal fusion. Specifically, we project\nPoIs into the view of each modality to sample the corresponding feature and\nintegrate the multi-modal features at each PoI through a dynamic fusion block.\nFurthermore, the features of PoIs derived from the same query box are\naggregated together to update the query feature. Our approach prevents\ninformation loss caused by view transformation and eliminates the\ncomputation-intensive global attention, making the multi-modal 3D object\ndetector more applicable. We conducted extensive experiments on the nuScenes\ndataset to evaluate our approach. Remarkably, our PoIFusion achieves 74.9\\% NDS\nand 73.4\\% mAP, setting a state-of-the-art record on the multi-modal 3D object\ndetection benchmark. Codes will be made available via\n\\url{https://djiajunustc.github.io/projects/poifusion}.\n"
    },
    {
        "title": "Customizing Segmentation Foundation Model via Prompt Learning for\n  Instance Segmentation",
        "published_time": "2024-03-14T09:13:51Z",
        "abstract": "  Recently, foundation models trained on massive datasets to adapt to a wide\nrange of domains have attracted considerable attention and are actively being\nexplored within the computer vision community. Among these, the Segment\nAnything Model (SAM) stands out for its remarkable progress in generalizability\nand flexibility for image segmentation tasks, achieved through prompt-based\nobject mask generation. However, despite its strength, SAM faces two key\nlimitations when applied to customized instance segmentation that segments\nspecific objects or those in unique environments not typically present in the\ntraining data: 1) the ambiguity inherent in input prompts and 2) the necessity\nfor extensive additional training to achieve optimal segmentation. To address\nthese challenges, we propose a novel method, customized instance segmentation\nvia prompt learning tailored to SAM. Our method involves a prompt learning\nmodule (PLM), which adjusts input prompts into the embedding space to better\nalign with user intentions, thereby enabling more efficient training.\nFurthermore, we introduce a point matching module (PMM) to enhance the feature\nrepresentation for finer segmentation by ensuring detailed alignment with\nground truth boundaries. Experimental results on various customized instance\nsegmentation scenarios demonstrate the effectiveness of the proposed method.\n"
    },
    {
        "title": "Noise Dimension of GAN: An Image Compression Perspective",
        "published_time": "2024-03-14T09:09:06Z",
        "abstract": "  Generative adversial network (GAN) is a type of generative model that maps a\nhigh-dimensional noise to samples in target distribution. However, the\ndimension of noise required in GAN is not well understood. Previous approaches\nview GAN as a mapping from a continuous distribution to another continous\ndistribution. In this paper, we propose to view GAN as a discrete sampler\ninstead. From this perspective, we build a connection between the minimum noise\nrequired and the bits to losslessly compress the images. Furthermore, to\nunderstand the behaviour of GAN when noise dimension is limited, we propose\ndivergence-entropy trade-off. This trade-off depicts the best divergence we can\nachieve when noise is limited. And as rate distortion trade-off, it can be\nnumerically solved when source distribution is known. Finally, we verifies our\ntheory with experiments on image generation.\n"
    },
    {
        "title": "SAM-Lightening: A Lightweight Segment Anything Model with Dilated Flash\n  Attention to Achieve 30 times Acceleration",
        "published_time": "2024-03-14T09:07:34Z",
        "abstract": "  Segment Anything Model (SAM) has garnered significant attention in\nsegmentation tasks due to their zero-shot generalization ability. However, a\nbroader application of SAMs to real-world practice has been restricted by their\nlow inference speed and high computational memory demands, which mainly stem\nfrom the attention mechanism. Existing work concentrated on optimizing the\nencoder, yet has not adequately addressed the inefficiency of the attention\nmechanism itself, even when distilled to a smaller model, which thus leaves\nspace for further improvement. In response, we introduce SAM-Lightening, a\nvariant of SAM, that features a re-engineered attention mechanism, termed\nDilated Flash Attention. It not only facilitates higher parallelism, enhancing\nprocessing efficiency but also retains compatibility with the existing\nFlashAttention. Correspondingly, we propose a progressive distillation to\nenable an efficient knowledge transfer from the vanilla SAM without costly\ntraining from scratch. Experiments on COCO and LVIS reveal that SAM-Lightening\nsignificantly outperforms the state-of-the-art methods in both run-time\nefficiency and segmentation accuracy. Specifically, it can achieve an inference\nspeed of 7 milliseconds (ms) per image, for images of size 1024*1024 pixels,\nwhich is 30.1 times faster than the vanilla SAM and 2.1 times than the\nstate-of-the-art. Moreover, it takes only 244MB memory, which is 3.5\\% of the\nvanilla SAM. The code and weights are available at\nhttps://anonymous.4open.science/r/SAM-LIGHTENING-BC25/.\n"
    },
    {
        "title": "Intention-driven Ego-to-Exo Video Generation",
        "published_time": "2024-03-14T09:07:31Z",
        "abstract": "  Ego-to-exo video generation refers to generating the corresponding exocentric\nvideo according to the egocentric video, providing valuable applications in\nAR/VR and embodied AI. Benefiting from advancements in diffusion model\ntechniques, notable progress has been achieved in video generation. However,\nexisting methods build upon the spatiotemporal consistency assumptions between\nadjacent frames, which cannot be satisfied in the ego-to-exo scenarios due to\ndrastic changes in views. To this end, this paper proposes an Intention-Driven\nEgo-to-exo video generation framework (IDE) that leverages action intention\nconsisting of human movement and action description as view-independent\nrepresentation to guide video generation, preserving the consistency of content\nand motion. Specifically, the egocentric head trajectory is first estimated\nthrough multi-view stereo matching. Then, cross-view feature perception module\nis introduced to establish correspondences between exo- and ego- views, guiding\nthe trajectory transformation module to infer human full-body movement from the\nhead trajectory. Meanwhile, we present an action description unit that maps the\naction semantics into the feature space consistent with the exocentric image.\nFinally, the inferred human movement and high-level action descriptions jointly\nguide the generation of exocentric motion and interaction content (i.e.,\ncorresponding optical flow and occlusion maps) in the backward process of the\ndiffusion model, ultimately warping them into the corresponding exocentric\nvideo. We conduct extensive experiments on the relevant dataset with diverse\nexo-ego video pairs, and our IDE outperforms state-of-the-art models in both\nsubjective and objective assessments, demonstrating its efficacy in ego-to-exo\nvideo generation.\n"
    },
    {
        "title": "Are Vision Language Models Texture or Shape Biased and Can We Steer\n  Them?",
        "published_time": "2024-03-14T09:07:14Z",
        "abstract": "  Vision language models (VLMs) have drastically changed the computer vision\nmodel landscape in only a few years, opening an exciting array of new\napplications from zero-shot image classification, over to image captioning, and\nvisual question answering. Unlike pure vision models, they offer an intuitive\nway to access visual content through language prompting. The wide applicability\nof such models encourages us to ask whether they also align with human vision -\nspecifically, how far they adopt human-induced visual biases through multimodal\nfusion, or whether they simply inherit biases from pure vision models. One\nimportant visual bias is the texture vs. shape bias, or the dominance of local\nover global information. In this paper, we study this bias in a wide range of\npopular VLMs. Interestingly, we find that VLMs are often more shape-biased than\ntheir vision encoders, indicating that visual biases are modulated to some\nextent through text in multimodal models. If text does indeed influence visual\nbiases, this suggests that we may be able to steer visual biases not just\nthrough visual input but also through language: a hypothesis that we confirm\nthrough extensive experiments. For instance, we are able to steer shape bias\nfrom as low as 49% to as high as 72% through prompting alone. For now, the\nstrong human bias towards shape (96%) remains out of reach for all tested VLMs.\n"
    },
    {
        "title": "PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient\n  Task Adaptation",
        "published_time": "2024-03-14T09:06:49Z",
        "abstract": "  Recently, the scale of transformers has grown rapidly, which introduces\nconsiderable challenges in terms of training overhead and inference efficiency\nin the scope of task adaptation. Existing works, namely Parameter-Efficient\nFine-Tuning (PEFT) and model compression, have separately investigated the\nchallenges. However, PEFT cannot guarantee the inference efficiency of the\noriginal backbone, especially for large-scale models. Model compression\nrequires significant training costs for structure searching and re-training.\nConsequently, a simple combination of them cannot guarantee accomplishing both\ntraining efficiency and inference efficiency with minimal costs. In this paper,\nwe propose a novel Parallel Yielding Re-Activation (PYRA) method for such a\nchallenge of training-inference efficient task adaptation. PYRA first utilizes\nparallel yielding adaptive weights to comprehensively perceive the data\ndistribution in downstream tasks. A re-activation strategy for token modulation\nis then applied for tokens to be merged, leading to calibrated token features.\nExtensive experiments demonstrate that PYRA outperforms all competing methods\nunder both low compression rate and high compression rate, demonstrating its\neffectiveness and superiority in maintaining both training efficiency and\ninference efficiency for large-scale foundation models. Our code will be\nreleased to the public.\n"
    },
    {
        "title": "Intention-aware Denoising Diffusion Model for Trajectory Prediction",
        "published_time": "2024-03-14T09:05:25Z",
        "abstract": "  Trajectory prediction is an essential component in autonomous driving,\nparticularly for collision avoidance systems. Considering the inherent\nuncertainty of the task, numerous studies have utilized generative models to\nproduce multiple plausible future trajectories for each agent. However, most of\nthem suffer from restricted representation ability or unstable training issues.\nTo overcome these limitations, we propose utilizing the diffusion model to\ngenerate the distribution of future trajectories. Two cruxes are to be settled\nto realize such an idea. First, the diversity of intention is intertwined with\nthe uncertain surroundings, making the true distribution hard to parameterize.\nSecond, the diffusion process is time-consuming during the inference phase,\nrendering it unrealistic to implement in a real-time driving system. We propose\nan Intention-aware denoising Diffusion Model (IDM), which tackles the above two\nproblems. We decouple the original uncertainty into intention uncertainty and\naction uncertainty and model them with two dependent diffusion processes. To\ndecrease the inference time, we reduce the variable dimensions in the\nintention-aware diffusion process and restrict the initial distribution of the\naction-aware diffusion process, which leads to fewer diffusion steps. To\nvalidate our approach, we conduct experiments on the Stanford Drone Dataset\n(SDD) and ETH/UCY dataset. Our methods achieve state-of-the-art results, with\nan FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY\ndataset. Compared with the original diffusion model, IDM reduces inference time\nby two-thirds. Interestingly, our experiments further reveal that introducing\nintention information is beneficial in modeling the diffusion process of fewer\nsteps.\n"
    },
    {
        "title": "Generalized Relevance Learning Grassmann Quantization",
        "published_time": "2024-03-14T08:53:01Z",
        "abstract": "  Due to advancements in digital cameras, it is easy to gather multiple images\n(or videos) from an object under different conditions. Therefore, image-set\nclassification has attracted more attention, and different solutions were\nproposed to model them. A popular way to model image sets is subspaces, which\nform a manifold called the Grassmann manifold. In this contribution, we extend\nthe application of Generalized Relevance Learning Vector Quantization to deal\nwith Grassmann manifold. The proposed model returns a set of prototype\nsubspaces and a relevance vector. While prototypes model typical behaviours\nwithin classes, the relevance factors specify the most discriminative principal\nvectors (or images) for the classification task. They both provide insights\ninto the model's decisions by highlighting influential images and pixels for\npredictions. Moreover, due to learning prototypes, the model complexity of the\nnew method during inference is independent of dataset size, unlike previous\nworks. We applied it to several recognition tasks including handwritten digit\nrecognition, face recognition, activity recognition, and object recognition.\nExperiments demonstrate that it outperforms previous works with lower\ncomplexity and can successfully model the variation, such as handwritten style\nor lighting conditions. Moreover, the presence of relevances makes the model\nrobust to the selection of subspaces' dimensionality.\n"
    },
    {
        "title": "Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse\n  Mixture-of-Experts",
        "published_time": "2024-03-14T08:43:43Z",
        "abstract": "  Diffusion models have achieved remarkable success across a range of\ngenerative tasks. Recent efforts to enhance diffusion model architectures have\nreimagined them as a form of multi-task learning, where each task corresponds\nto a denoising task at a specific noise level. While these efforts have focused\non parameter isolation and task routing, they fall short of capturing detailed\ninter-task relationships and risk losing semantic information, respectively. In\nresponse, we introduce Switch Diffusion Transformer (Switch-DiT), which\nestablishes inter-task relationships between conflicting tasks without\ncompromising semantic information. To achieve this, we employ a sparse\nmixture-of-experts within each transformer block to utilize semantic\ninformation and facilitate handling conflicts in tasks through parameter\nisolation. Additionally, we propose a diffusion prior loss, encouraging similar\ntasks to share their denoising paths while isolating conflicting ones. Through\nthese, each transformer block contains a shared expert across all tasks, where\nthe common and task-specific denoising paths enable the diffusion model to\nconstruct its beneficial way of synergizing denoising tasks. Extensive\nexperiments validate the effectiveness of our approach in improving both image\nquality and convergence rate, and further analysis demonstrates that Switch-DiT\nconstructs tailored denoising paths across various generation scenarios.\n"
    },
    {
        "title": "SHAN: Object-Level Privacy Detection via Inference on Scene\n  Heterogeneous Graph",
        "published_time": "2024-03-14T08:32:14Z",
        "abstract": "  With the rise of social platforms, protecting privacy has become an important\nissue. Privacy object detection aims to accurately locate private objects in\nimages. It is the foundation of safeguarding individuals' privacy rights and\nensuring responsible data handling practices in the digital age. Since privacy\nof object is not shift-invariant, the essence of the privacy object detection\ntask is inferring object privacy based on scene information. However, privacy\nobject detection has long been studied as a subproblem of common object\ndetection tasks. Therefore, existing methods suffer from serious deficiencies\nin accuracy, generalization, and interpretability. Moreover, creating\nlarge-scale privacy datasets is difficult due to legal constraints and existing\nprivacy datasets lack label granularity. The granularity of existing privacy\ndetection methods remains limited to the image level. To address the above two\nissues, we introduce two benchmark datasets for object-level privacy detection\nand propose SHAN, Scene Heterogeneous graph Attention Network, a model\nconstructs a scene heterogeneous graph from an image and utilizes\nself-attention mechanisms for scene inference to obtain object privacy. Through\nexperiments, we demonstrated that SHAN performs excellently in privacy object\ndetection tasks, with all metrics surpassing those of the baseline model.\n"
    },
    {
        "title": "Towards the Uncharted: Density-Descending Feature Perturbation for\n  Semi-supervised Semantic Segmentation",
        "published_time": "2024-03-11T06:59:05Z",
        "abstract": "  Semi-supervised semantic segmentation allows model to mine effective\nsupervision from unlabeled data to complement label-guided training. Recent\nresearch has primarily focused on consistency regularization techniques,\nexploring perturbation-invariant training at both the image and feature levels.\nIn this work, we proposed a novel feature-level consistency learning framework\nnamed Density-Descending Feature Perturbation (DDFP). Inspired by the\nlow-density separation assumption in semi-supervised learning, our key insight\nis that feature density can shed a light on the most promising direction for\nthe segmentation classifier to explore, which is the regions with lower\ndensity. We propose to shift features with confident predictions towards\nlower-density regions by perturbation injection. The perturbed features are\nthen supervised by the predictions on the original features, thereby compelling\nthe classifier to explore less dense regions to effectively regularize the\ndecision boundary. Central to our method is the estimation of feature density.\nTo this end, we introduce a lightweight density estimator based on normalizing\nflow, allowing for efficient capture of the feature density distribution in an\nonline manner. By extracting gradients from the density estimator, we can\ndetermine the direction towards less dense regions for each feature. The\nproposed DDFP outperforms other designs on feature-level perturbations and\nshows state of the art performances on both Pascal VOC and Cityscapes dataset\nunder various partition protocols. The project is available at\nhttps://github.com/Gavinwxy/DDFP.\n"
    },
    {
        "title": "Continual Segmentation with Disentangled Objectness Learning and Class\n  Recognition",
        "published_time": "2024-03-06T05:33:50Z",
        "abstract": "  Most continual segmentation methods tackle the problem as a per-pixel\nclassification task. However, such a paradigm is very challenging, and we find\nquery-based segmenters with built-in objectness have inherent advantages\ncompared with per-pixel ones, as objectness has strong transfer ability and\nforgetting resistance. Based on these findings, we propose CoMasTRe by\ndisentangling continual segmentation into two stages: forgetting-resistant\ncontinual objectness learning and well-researched continual classification.\nCoMasTRe uses a two-stage segmenter learning class-agnostic mask proposals at\nthe first stage and leaving recognition to the second stage. During continual\nlearning, a simple but effective distillation is adopted to strengthen\nobjectness. To further mitigate the forgetting of old classes, we design a\nmulti-label class distillation strategy suited for segmentation. We assess the\neffectiveness of CoMasTRe on PASCAL VOC and ADE20K. Extensive experiments show\nthat our method outperforms per-pixel and query-based methods on both datasets.\nCode will be available at https://github.com/jordangong/CoMasTRe.\n"
    },
    {
        "title": "Template-Free Single-View 3D Human Digitalization with Diffusion-Guided\n  LRM",
        "published_time": "2024-01-22T18:08:22Z",
        "abstract": "  Reconstructing 3D humans from a single image has been extensively\ninvestigated. However, existing approaches often fall short on capturing fine\ngeometry and appearance details, hallucinating occluded parts with plausible\ndetails, and achieving generalization across unseen and in-the-wild datasets.\nWe present Human-LRM, a diffusion-guided feed-forward model that predicts the\nimplicit field of a human from a single image. Leveraging the power of the\nstate-of-the-art reconstruction model (i.e., LRM) and generative model (i.e\nStable Diffusion), our method is able to capture human without any template\nprior, e.g., SMPL, and effectively enhance occluded parts with rich and\nrealistic details. Our approach first uses a single-view LRM model with an\nenhanced geometry decoder to get the triplane NeRF representation. The novel\nview renderings from the triplane NeRF provide strong geometry and color prior,\nfrom which we generate photo-realistic details for the occluded parts using a\ndiffusion model. The generated multiple views then enable reconstruction with\nhigh-quality geometry and appearance, leading to superior overall performance\ncomparing to all existing human reconstruction methods.\n"
    },
    {
        "title": "VM-UNET-V2 Rethinking Vision Mamba UNet for Medical Image Segmentation",
        "published_time": "2024-03-14T08:12:39Z",
        "abstract": "  In the field of medical image segmentation, models based on both CNN and\nTransformer have been thoroughly investigated. However, CNNs have limited\nmodeling capabilities for long-range dependencies, making it challenging to\nexploit the semantic information within images fully. On the other hand, the\nquadratic computational complexity poses a challenge for Transformers.\nRecently, State Space Models (SSMs), such as Mamba, have been recognized as a\npromising method. They not only demonstrate superior performance in modeling\nlong-range interactions, but also preserve a linear computational complexity.\nInspired by the Mamba architecture, We proposed Vison Mamba-UNetV2, the Visual\nState Space (VSS) Block is introduced to capture extensive contextual\ninformation, the Semantics and Detail Infusion (SDI) is introduced to augment\nthe infusion of low-level and high-level features. We conduct comprehensive\nexperiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB\nand ETIS-LaribPolypDB public datasets. The results indicate that VM-UNetV2\nexhibits competitive performance in medical image segmentation tasks. Our code\nis available at https://github.com/nobodyplayer1/VM-UNetV2.\n"
    },
    {
        "title": "Discriminative Probing and Tuning for Text-to-Image Generation",
        "published_time": "2024-03-07T08:37:33Z",
        "abstract": "  Despite advancements in text-to-image generation (T2I), prior methods often\nface text-image misalignment problems such as relation confusion in generated\nimages. Existing solutions involve cross-attention manipulation for better\ncompositional understanding or integrating large language models for improved\nlayout planning. However, the inherent alignment capabilities of T2I models are\nstill inadequate. By reviewing the link between generative and discriminative\nmodeling, we posit that T2I models' discriminative abilities may reflect their\ntext-image alignment proficiency during generation. In this light, we advocate\nbolstering the discriminative abilities of T2I models to achieve more precise\ntext-to-image alignment for generation. We present a discriminative adapter\nbuilt on T2I models to probe their discriminative abilities on two\nrepresentative tasks and leverage discriminative fine-tuning to improve their\ntext-image alignment. As a bonus of the discriminative adapter, a\nself-correction mechanism can leverage discriminative gradients to better align\ngenerated images to text prompts during inference. Comprehensive evaluations\nacross three benchmark datasets, including both in-distribution and\nout-of-distribution scenarios, demonstrate our method's superior generation\nperformance. Meanwhile, it achieves state-of-the-art discriminative performance\non the two discriminative tasks compared to other generative models.\n"
    },
    {
        "title": "Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D\n  Prior",
        "published_time": "2024-03-14T07:39:59Z",
        "abstract": "  Recent works on text-to-3d generation show that using only 2D diffusion\nsupervision for 3D generation tends to produce results with inconsistent\nappearances (e.g., faces on the back view) and inaccurate shapes (e.g., animals\nwith extra legs). Existing methods mainly address this issue by retraining\ndiffusion models with images rendered from 3D data to ensure multi-view\nconsistency while struggling to balance 2D generation quality with 3D\nconsistency. In this paper, we present a new framework Sculpt3D that equips the\ncurrent pipeline with explicit injection of 3D priors from retrieved reference\nobjects without re-training the 2D diffusion model. Specifically, we\ndemonstrate that high-quality and diverse 3D geometry can be guaranteed by\nkeypoints supervision through a sparse ray sampling approach. Moreover, to\nensure accurate appearances of different views, we further modulate the output\nof the 2D diffusion model to the correct patterns of the template views without\naltering the generated object's style. These two decoupled designs effectively\nharness 3D information from reference objects to generate 3D objects while\npreserving the generation quality of the 2D diffusion model. Extensive\nexperiments show our method can largely improve the multi-view consistency\nwhile retaining fidelity and diversity. Our project page is available at:\nhttps://stellarcheng.github.io/Sculpt3D/.\n"
    },
    {
        "title": "Metadata-Driven Federated Learning of Connectional Brain Templates in\n  Non-IID Multi-Domain Scenarios",
        "published_time": "2024-03-14T07:38:22Z",
        "abstract": "  A connectional brain template (CBT) is a holistic representation of a\npopulation of multi-view brain connectivity graphs, encoding shared patterns\nand normalizing typical variations across individuals. The federation of CBT\nlearning allows for an inclusive estimation of the representative center of\nmulti-domain brain connectivity datasets in a fully data-preserving manner.\nHowever, existing methods overlook the non-independent and identically\ndistributed (non-IDD) issue stemming from multidomain brain connectivity\nheterogeneity, in which data domains are drawn from different hospitals and\nimaging modalities. To overcome this limitation, we unprecedentedly propose a\nmetadata-driven federated learning framework, called MetaFedCBT, for\ncross-domain CBT learning. Given the data drawn from a specific domain (i.e.,\nhospital), our model aims to learn metadata in a fully supervised manner by\nintroducing a local client-based regressor network. The generated meta-data is\nforced to meet the statistical attributes (e.g., mean) of other domains, while\npreserving their privacy. Our supervised meta-data generation approach boosts\nthe unsupervised learning of a more centered, representative, and holistic CBT\nof a particular brain state across diverse domains. As the federated learning\nprogresses over multiple rounds, the learned metadata and associated generated\nconnectivities are continuously updated to better approximate the target domain\ninformation. MetaFedCBT overcomes the non-IID issue of existing methods by\ngenerating informative brain connectivities for privacy-preserving holistic CBT\nlearning with guidance using metadata. Extensive experiments on multi-view\nmorphological brain networks of normal and patient subjects demonstrate that\nour MetaFedCBT is a superior federated CBT learning model and significantly\nadvances the state-of-the-art performance.\n"
    },
    {
        "title": "Biophysics Informed Pathological Regularisation for Brain Tumour\n  Segmentation",
        "published_time": "2024-03-14T07:21:46Z",
        "abstract": "  Recent advancements in deep learning have significantly improved brain tumour\nsegmentation techniques; however, the results still lack confidence and\nrobustness as they solely consider image data without biophysical priors or\npathological information. Integrating biophysics-informed regularisation is one\neffective way to change this situation, as it provides an prior regularisation\nfor automated end-to-end learning. In this paper, we propose a novel approach\nthat designs brain tumour growth Partial Differential Equation (PDE) models as\na regularisation with deep learning, operational with any network model. Our\nmethod introduces tumour growth PDE models directly into the segmentation\nprocess, improving accuracy and robustness, especially in data-scarce\nscenarios. This system estimates tumour cell density using a periodic\nactivation function. By effectively integrating this estimation with\nbiophysical models, we achieve a better capture of tumour characteristics. This\napproach not only aligns the segmentation closer to actual biological behaviour\nbut also strengthens the model's performance under limited data conditions. We\ndemonstrate the effectiveness of our framework through extensive experiments on\nthe BraTS 2023 dataset, showcasing significant improvements in both precision\nand reliability of tumour segmentation.\n"
    },
    {
        "title": "GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting",
        "published_time": "2024-03-13T14:02:54Z",
        "abstract": "  Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 1000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding.\n"
    },
    {
        "title": "Rethinking Referring Object Removal",
        "published_time": "2024-03-14T06:26:34Z",
        "abstract": "  Referring object removal refers to removing the specific object in an image\nreferred by natural language expressions and filling the missing region with\nreasonable semantics. To address this task, we construct the ComCOCO, a\nsynthetic dataset consisting of 136,495 referring expressions for 34,615\nobjects in 23,951 image pairs. Each pair contains an image with referring\nexpressions and the ground truth after elimination. We further propose an\nend-to-end syntax-aware hybrid mapping network with an encoding-decoding\nstructure. Linguistic features are hierarchically extracted at the syntactic\nlevel and fused in the downsampling process of visual features with multi-head\nattention. The feature-aligned pyramid network is leveraged to generate\nsegmentation masks and replace internal pixels with region affinity learned\nfrom external semantics in high-level feature maps. Extensive experiments\ndemonstrate that our model outperforms diffusion models and two-stage methods\nwhich process the segmentation and inpainting task separately by a significant\nmargin.\n"
    },
    {
        "title": "Single Domain Generalization for Crowd Counting",
        "published_time": "2024-03-14T06:16:21Z",
        "abstract": "  Current image-based crowd counting widely employs density map regression due\nto its promising results. However, the method often suffers from severe\nperformance degradation when tested on data from unseen scenarios. To address\nthis so-called \"domain shift\" problem, we investigate single domain\ngeneralization (SDG) for crowd counting. The existing SDG approaches are mainly\nfor classification and segmentation, and can hardly be extended to our case due\nto its regression nature and label ambiguity (i.e., ambiguous pixel-level\nground truths). We propose MPCount, a novel SDG approach effective even for\nnarrow source distribution. Reconstructing diverse features for density map\nregression with a single memory bank, MPCount retains only domain-invariant\nrepresentations using a content error mask and attention consistency loss. It\nfurther introduces patch-wise classification as an auxiliary task to boost the\nrobustness of density prediction to achieve highly accurate labels. Through\nextensive experiments on different datasets, MPCount is shown to significantly\nimprove counting accuracy compared to the state of the art under diverse\nscenarios unobserved in the training data of narrow source distribution. Code\nis available at https://github.com/Shimmer93/MPCount.\n"
    },
    {
        "title": "Randomized Principal Component Analysis for Hyperspectral Image\n  Classification",
        "published_time": "2024-03-14T05:40:23Z",
        "abstract": "  The high-dimensional feature space of the hyperspectral imagery poses major\nchallenges to the processing and analysis of the hyperspectral data sets. In\nsuch a case, dimensionality reduction is necessary to decrease the\ncomputational complexity. The random projections open up new ways of\ndimensionality reduction, especially for large data sets. In this paper, the\nprincipal component analysis (PCA) and randomized principal component analysis\n(R-PCA) for the classification of hyperspectral images using support vector\nmachines (SVM) and light gradient boosting machines (LightGBM) have been\ninvestigated. In this experimental research, the number of features was reduced\nto 20 and 30 for classification of two hyperspectral datasets (Indian Pines and\nPavia University). The experimental results demonstrated that PCA outperformed\nR-PCA for SVM for both datasets, but received close accuracy values for\nLightGBM. The highest classification accuracies were obtained as 0.9925 and\n0.9639 by LightGBM with original features for the Pavia University and Indian\nPines, respectively.\n"
    },
    {
        "title": "Emo-Avatar: Efficient Monocular Video Style Avatar through Texture\n  Rendering",
        "published_time": "2024-02-01T18:14:42Z",
        "abstract": "  Artistic video portrait generation is a significant and sought-after task in\nthe fields of computer graphics and vision. While various methods have been\ndeveloped that integrate NeRFs or StyleGANs with instructional editing models\nfor creating and editing drivable portraits, these approaches face several\nchallenges. They often rely heavily on large datasets, require extensive\ncustomization processes, and frequently result in reduced image quality. To\naddress the above problems, we propose the Efficient Monotonic Video Style\nAvatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's\ncapacity for producing dynamic, drivable portrait videos. We proposed a\ntwo-stage deferred neural rendering pipeline. In the first stage, we utilize\nfew-shot PTI initialization to initialize the StyleGAN generator through\nseveral extreme poses sampled from the video to capture the consistent\nrepresentation of aligned faces from the target portrait. In the second stage,\nwe propose a Laplacian pyramid for high-frequency texture sampling from UV maps\ndeformed by dynamic flow of expression for motion-aware texture prior\nintegration to provide torso features to enhance StyleGAN's ability to generate\ncomplete and upper body for portrait video rendering. Emo-Avatar reduces style\ncustomization time from hours to merely 5 minutes compared with existing\nmethods. In addition, Emo-Avatar requires only a single reference image for\nediting and employs region-aware contrastive learning with semantic invariant\nCLIP guidance, ensuring consistent high-resolution output and identity\npreservation. Through both quantitative and qualitative assessments, Emo-Avatar\ndemonstrates superior performance over existing methods in terms of training\nefficiency, rendering quality and editability in self- and cross-reenactment.\n"
    },
    {
        "title": "CardioCaps: Attention-based Capsule Network for Class-Imbalanced\n  Echocardiogram Classification",
        "published_time": "2024-03-14T05:01:31Z",
        "abstract": "  Capsule Neural Networks (CapsNets) is a novel architecture that utilizes\nvector-wise representations formed by multiple neurons. Specifically, the\nDynamic Routing CapsNets (DR-CapsNets) employ an affine matrix and dynamic\nrouting mechanism to train capsules and acquire translation-equivariance\nproperties, enhancing its robustness compared to traditional Convolutional\nNeural Networks (CNNs). Echocardiograms, which capture moving images of the\nheart, present unique challenges for traditional image classification methods.\nIn this paper, we explore the potential of DR-CapsNets and propose CardioCaps,\na novel attention-based DR-CapsNet architecture for class-imbalanced\nechocardiogram classification. CardioCaps comprises two key components: a\nweighted margin loss incorporating a regression auxiliary loss and an attention\nmechanism. First, the weighted margin loss prioritizes positive cases,\nsupplemented by an auxiliary loss function based on the Ejection Fraction (EF)\nregression task, a crucial measure of cardiac function. This approach enhances\nthe model's resilience in the face of class imbalance. Second, recognizing the\nquadratic complexity of dynamic routing leading to training inefficiencies, we\nadopt the attention mechanism as a more computationally efficient alternative.\nOur results demonstrate that CardioCaps surpasses traditional machine learning\nbaseline methods, including Logistic Regression, Random Forest, and XGBoost\nwith sampling methods and a class weight matrix. Furthermore, CardioCaps\noutperforms other deep learning baseline methods such as CNNs, ResNets, U-Nets,\nand ViTs, as well as advanced CapsNets methods such as EM-CapsNets and\nEfficient-CapsNets. Notably, our model demonstrates robustness to class\nimbalance, achieving high precision even in datasets with a substantial\nproportion of negative cases.\n"
    },
    {
        "title": "S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering",
        "published_time": "2024-03-14T05:00:29Z",
        "abstract": "  Anchor-based large-scale multi-view clustering has attracted considerable\nattention for its effectiveness in handling massive datasets. However, current\nmethods mainly seek the consensus embedding feature for clustering by exploring\nglobal correlations between anchor graphs or projection matrices.In this paper,\nwe propose a simple yet efficient scalable multi-view tensor clustering\n(S^2MVTC) approach, where our focus is on learning correlations of embedding\nfeatures within and across views. Specifically, we first construct the\nembedding feature tensor by stacking the embedding features of different views\ninto a tensor and rotating it. Additionally, we build a novel tensor\nlow-frequency approximation (TLFA) operator, which incorporates graph\nsimilarity into embedding feature learning, efficiently achieving smooth\nrepresentation of embedding features within different views. Furthermore,\nconsensus constraints are applied to embedding features to ensure inter-view\nsemantic consistency. Experimental results on six large-scale multi-view\ndatasets demonstrate that S^2MVTC significantly outperforms state-of-the-art\nalgorithms in terms of clustering performance and CPU execution time,\nespecially when handling massive data. The code of S^2MVTC is publicly\navailable at https://github.com/longzhen520/S2MVTC.\n"
    },
    {
        "title": "LDReg: Local Dimensionality Regularized Self-Supervised Learning",
        "published_time": "2024-01-19T03:50:19Z",
        "abstract": "  Representations learned via self-supervised learning (SSL) can be susceptible\nto dimensional collapse, where the learned representation subspace is of\nextremely low dimensionality and thus fails to represent the full data\ndistribution and modalities. Dimensional collapse also known as the\n\"underfilling\" phenomenon is one of the major causes of degraded performance on\ndownstream tasks. Previous work has investigated the dimensional collapse\nproblem of SSL at a global level. In this paper, we demonstrate that\nrepresentations can span over high dimensional space globally, but collapse\nlocally. To address this, we propose a method called $\\textit{local\ndimensionality regularization (LDReg)}$. Our formulation is based on the\nderivation of the Fisher-Rao metric to compare and optimize local distance\ndistributions at an asymptotically small radius for each data point. By\nincreasing the local intrinsic dimensionality, we demonstrate through a range\nof experiments that LDReg improves the representation quality of SSL. The\nresults also show that LDReg can regularize dimensionality at both local and\nglobal levels.\n"
    },
    {
        "title": "Soften to Defend: Towards Adversarial Robustness via Self-Guided Label\n  Refinement",
        "published_time": "2024-03-14T04:48:31Z",
        "abstract": "  Adversarial training (AT) is currently one of the most effective ways to\nobtain the robustness of deep neural networks against adversarial attacks.\nHowever, most AT methods suffer from robust overfitting, i.e., a significant\ngeneralization gap in adversarial robustness between the training and testing\ncurves. In this paper, we first identify a connection between robust\noverfitting and the excessive memorization of noisy labels in AT from a view of\ngradient norm. As such label noise is mainly caused by a distribution mismatch\nand improper label assignments, we are motivated to propose a label refinement\napproach for AT. Specifically, our Self-Guided Label Refinement first\nself-refines a more accurate and informative label distribution from\nover-confident hard labels, and then it calibrates the training by dynamically\nincorporating knowledge from self-distilled models into the current model and\nthus requiring no external teachers. Empirical results demonstrate that our\nmethod can simultaneously boost the standard accuracy and robust performance\nacross multiple benchmark datasets, attack types, and architectures. In\naddition, we also provide a set of analyses from the perspectives of\ninformation theory to dive into our method and suggest the importance of soft\nlabels for robust generalization.\n"
    },
    {
        "title": "Virtual birefringence imaging and histological staining of amyloid\n  deposits in label-free tissue using autofluorescence microscopy and deep\n  learning",
        "published_time": "2024-03-14T04:48:06Z",
        "abstract": "  Systemic amyloidosis is a group of diseases characterized by the deposition\nof misfolded proteins in various organs and tissues, leading to progressive\norgan dysfunction and failure. Congo red stain is the gold standard chemical\nstain for the visualization of amyloid deposits in tissue sections, as it forms\ncomplexes with the misfolded proteins and shows a birefringence pattern under\npolarized light microscopy. However, Congo red staining is tedious and costly\nto perform, and prone to false diagnoses due to variations in the amount of\namyloid, staining quality and expert interpretation through manual examination\nof tissue under a polarization microscope. Here, we report the first\ndemonstration of virtual birefringence imaging and virtual Congo red staining\nof label-free human tissue to show that a single trained neural network can\nrapidly transform autofluorescence images of label-free tissue sections into\nbrightfield and polarized light microscopy equivalent images, matching the\nhistochemically stained versions of the same samples. We demonstrate the\nefficacy of our method with blind testing and pathologist evaluations on\ncardiac tissue where the virtually stained images agreed well with the\nhistochemically stained ground truth images. Our virtually stained polarization\nand brightfield images highlight amyloid birefringence patterns in a\nconsistent, reproducible manner while mitigating diagnostic challenges due to\nvariations in the quality of chemical staining and manual imaging processes as\npart of the clinical workflow.\n"
    },
    {
        "title": "Deep unfolding Network for Hyperspectral Image Super-Resolution with\n  Automatic Exposure Correction",
        "published_time": "2024-03-14T04:41:30Z",
        "abstract": "  In recent years, the fusion of high spatial resolution multispectral image\n(HR-MSI) and low spatial resolution hyperspectral image (LR-HSI) has been\nrecognized as an effective method for HSI super-resolution (HSI-SR). However,\nboth HSI and MSI may be acquired under extreme conditions such as night or\npoorly illuminating scenarios, which may cause different exposure levels,\nthereby seriously downgrading the yielded HSISR. In contrast to most existing\nmethods based on respective low-light enhancements (LLIE) of MSI and HSI\nfollowed by their fusion, a deep Unfolding HSI Super-Resolution with Automatic\nExposure Correction (UHSR-AEC) is proposed, that can effectively generate a\nhigh-quality fused HSI-SR (in texture and features) even under very imbalanced\nexposures, thanks to the correlation between LLIE and HSI-SR taken into\naccount. Extensive experiments are provided to demonstrate the state-of-the-art\noverall performance of the proposed UHSR-AEC, including comparison with some\nbenchmark peer methods.\n"
    },
    {
        "title": "PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for\n  Recognizing Low-Quality Images",
        "published_time": "2024-02-20T05:06:20Z",
        "abstract": "  A standard practice in developing image recognition models is to train a\nmodel on a specific image resolution and then deploy it. However, in real-world\ninference, models often encounter images different from the training sets in\nresolution and/or subject to natural variations such as weather changes, noise\ntypes and compression artifacts. While traditional solutions involve training\nmultiple models for different resolutions or input variations, these methods\nare computationally expensive and thus do not scale in practice. To this end,\nwe propose a novel neural network model, parallel-structured and all-component\nFourier neural operator (PAC-FNO), that addresses the problem. Unlike\nconventional feed-forward neural networks, PAC-FNO operates in the frequency\ndomain, allowing it to handle images of varying resolutions within a single\nmodel. We also propose a two-stage algorithm for training PAC-FNO with a\nminimal modification to the original, downstream model. Moreover, the proposed\nPAC-FNO is ready to work with existing image recognition models. Extensively\nevaluating methods with seven image recognition benchmarks, we show that the\nproposed PAC-FNO improves the performance of existing baseline models on images\nwith various resolutions by up to 77.1% and various types of natural variations\nin the images at inference.\n"
    },
    {
        "title": "Desigen: A Pipeline for Controllable Design Template Generation",
        "published_time": "2024-03-14T04:32:28Z",
        "abstract": "  Templates serve as a good starting point to implement a design (e.g., banner,\nslide) but it takes great effort from designers to manually create. In this\npaper, we present Desigen, an automatic template creation pipeline which\ngenerates background images as well as harmonious layout elements over the\nbackground. Different from natural images, a background image should preserve\nenough non-salient space for the overlaying layout elements. To equip existing\nadvanced diffusion-based models with stronger spatial control, we propose two\nsimple but effective techniques to constrain the saliency distribution and\nreduce the attention weight in desired regions during the background generation\nprocess. Then conditioned on the background, we synthesize the layout with a\nTransformer-based autoregressive generator. To achieve a more harmonious\ncomposition, we propose an iterative inference strategy to adjust the\nsynthesized background and layout in multiple rounds. We constructed a design\ndataset with more than 40k advertisement banners to verify our approach.\nExtensive experiments demonstrate that the proposed pipeline generates\nhigh-quality templates comparable to human designers. More than a single-page\ndesign, we further show an application of presentation generation that outputs\na set of theme-consistent slides. The data and code are available at\nhttps://whaohan.github.io/desigen.\n"
    },
    {
        "title": "Efficient Bitrate Ladder Construction using Transfer Learning and\n  Spatio-Temporal Features",
        "published_time": "2024-01-06T11:37:20Z",
        "abstract": "  Providing high-quality video with efficient bitrate is a main challenge in\nvideo industry. The traditional one-size-fits-all scheme for bitrate ladders is\ninefficient and reaching the best content-aware decision computationally\nimpractical due to extensive encodings required. To mitigate this, we propose a\nbitrate and complexity efficient bitrate ladder prediction method using\ntransfer learning and spatio-temporal features. We propose: (1) using feature\nmaps from well-known pre-trained DNNs to predict rate-quality behavior with\nlimited training data; and (2) improving highest quality rung efficiency by\npredicting minimum bitrate for top quality and using it for the top rung. The\nmethod tested on 102 video scenes demonstrates 94.1% reduction in complexity\nversus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning\nwas thoroughly studied through four networks and ablation studies.\n"
    },
    {
        "title": "PreSight: Enhancing Autonomous Vehicle Perception with City-Scale NeRF\n  Priors",
        "published_time": "2024-03-14T03:52:33Z",
        "abstract": "  Autonomous vehicles rely extensively on perception systems to navigate and\ninterpret their surroundings. Despite significant advancements in these systems\nrecently, challenges persist under conditions like occlusion, extreme lighting,\nor in unfamiliar urban areas. Unlike these systems, humans do not solely depend\non immediate observations to perceive the environment. In navigating new\ncities, humans gradually develop a preliminary mental map to supplement\nreal-time perception during subsequent visits. Inspired by this human approach,\nwe introduce a novel framework, Pre-Sight, that leverages past traversals to\nconstruct static prior memories, enhancing online perception in later\nnavigations. Our method involves optimizing a city-scale neural radiance field\nwith data from previous journeys to generate neural priors. These priors, rich\nin semantic and geometric details, are derived without manual annotations and\ncan seamlessly augment various state-of-the-art perception models, improving\ntheir efficacy with minimal additional computational cost. Experimental results\non the nuScenes dataset demonstrate the framework's high compatibility with\ndiverse online perception models. Specifically, it shows remarkable\nimprovements in HD-map construction and occupancy prediction tasks,\nhighlighting its potential as a new perception framework for autonomous driving\nsystems. Our code will be released at\nhttps://github.com/yuantianyuan01/PreSight.\n"
    },
    {
        "title": "UniCode: Learning a Unified Codebook for Multimodal Large Language\n  Models",
        "published_time": "2024-03-14T03:29:58Z",
        "abstract": "  In this paper, we propose \\textbf{UniCode}, a novel approach within the\ndomain of multimodal large language models (MLLMs) that learns a unified\ncodebook to efficiently tokenize visual, text, and potentially other types of\nsignals. This innovation addresses a critical limitation in existing MLLMs:\ntheir reliance on a text-only codebook, which restricts MLLM's ability to\ngenerate images and texts in a multimodal context. Towards this end, we propose\na language-driven iterative training paradigm, coupled with an in-context\npre-training task we term ``image decompression'', enabling our model to\ninterpret compressed visual data and generate high-quality images.The unified\ncodebook empowers our model to extend visual instruction tuning to\nnon-linguistic generation tasks. Moreover, UniCode is adaptable to diverse\nstacked quantization approaches in order to compress visual signals into a more\ncompact token representation. Despite using significantly fewer parameters and\nless data during training, Unicode demonstrates promising capabilities in\nvisual reconstruction and generation. It also achieves performances comparable\nto leading MLLMs across a spectrum of VQA benchmarks.\n"
    },
    {
        "title": "Dyadic Interaction Modeling for Social Behavior Generation",
        "published_time": "2024-03-14T03:21:33Z",
        "abstract": "  Human-human communication is like a delicate dance where listeners and\nspeakers concurrently interact to maintain conversational dynamics. Hence, an\neffective model for generating listener nonverbal behaviors requires\nunderstanding the dyadic context and interaction. In this paper, we present an\neffective framework for creating 3D facial motions in dyadic interactions.\nExisting work consider a listener as a reactive agent with reflexive behaviors\nto the speaker's voice and facial motions. The heart of our framework is Dyadic\nInteraction Modeling (DIM), a pre-training approach that jointly models\nspeakers' and listeners' motions through masking and contrastive learning to\nlearn representations that capture the dyadic context. To enable the generation\nof non-deterministic behaviors, we encode both listener and speaker motions\ninto discrete latent representations, through VQ-VAE. The pre-trained model is\nfurther fine-tuned for motion generation. Extensive experiments demonstrate the\nsuperiority of our framework in generating listener motions, establishing a new\nstate-of-the-art according to the quantitative measures capturing the diversity\nand realism of generated motions. Qualitative results demonstrate the superior\ncapabilities of the proposed approach in generating diverse and realistic\nexpressions, eye blinks and head gestures.\n"
    },
    {
        "title": "Hyperparameters in Continual Learning: a Reality Check",
        "published_time": "2024-03-14T03:13:01Z",
        "abstract": "  Various algorithms for continual learning (CL) have been designed with the\ngoal of effectively alleviating the trade-off between stability and plasticity\nduring the CL process. To achieve this goal, tuning appropriate hyperparameters\nfor each algorithm is essential. As an evaluation protocol, it has been common\npractice to train a CL algorithm using diverse hyperparameter values on a CL\nscenario constructed with a benchmark dataset. Subsequently, the best\nperformance attained with the optimal hyperparameter value serves as the\ncriterion for evaluating the CL algorithm. In this paper, we contend that this\nevaluation protocol is not only impractical but also incapable of effectively\nassessing the CL capability of a CL algorithm. Returning to the fundamental\nprinciples of model evaluation in machine learning, we propose an evaluation\nprotocol that involves Hyperparameter Tuning and Evaluation phases. Those\nphases consist of different datasets but share the same CL scenario. In the\nHyperparameter Tuning phase, each algorithm is iteratively trained with\ndifferent hyperparameter values to find the optimal hyperparameter values.\nSubsequently, in the Evaluation phase, the optimal hyperparameter values is\ndirectly applied for training each algorithm, and their performance in the\nEvaluation phase serves as the criterion for evaluating them. Through\nexperiments on CIFAR-100 and ImageNet-100 based on the proposed protocol in\nclass-incremental learning, we not only observed that the existing evaluation\nmethod fail to properly assess the CL capability of each algorithm but also\nobserve that some recently proposed state-of-the-art algorithms, which reported\nsuperior performance, actually exhibit inferior performance compared to the\nprevious algorithm.\n"
    },
    {
        "title": "When Semantic Segmentation Meets Frequency Aliasing",
        "published_time": "2024-03-14T03:12:02Z",
        "abstract": "  Despite recent advancements in semantic segmentation, where and what pixels\nare hard to segment remains largely unexplored. Existing research only\nseparates an image into easy and hard regions and empirically observes the\nlatter are associated with object boundaries. In this paper, we conduct a\ncomprehensive analysis of hard pixel errors, categorizing them into three\ntypes: false responses, merging mistakes, and displacements. Our findings\nreveal a quantitative association between hard pixels and aliasing, which is\ndistortion caused by the overlapping of frequency components in the Fourier\ndomain during downsampling. To identify the frequencies responsible for\naliasing, we propose using the equivalent sampling rate to calculate the\nNyquist frequency, which marks the threshold for aliasing. Then, we introduce\nthe aliasing score as a metric to quantify the extent of aliasing. While\npositively correlated with the proposed aliasing score, three types of hard\npixels exhibit different patterns. Here, we propose two novel de-aliasing\nfilter (DAF) and frequency mixing (FreqMix) modules to alleviate aliasing\ndegradation by accurately removing or adjusting frequencies higher than the\nNyquist frequency. The DAF precisely removes the frequencies responsible for\naliasing before downsampling, while the FreqMix dynamically selects\nhigh-frequency components within the encoder block. Experimental results\ndemonstrate consistent improvements in semantic segmentation and low-light\ninstance segmentation tasks. The code is available at:\n\\url{https://github.com/Linwei-Chen/Seg-Aliasing}.\n"
    },
    {
        "title": "Distribution and Depth-Aware Transformers for 3D Human Mesh Recovery",
        "published_time": "2024-03-14T03:07:58Z",
        "abstract": "  Precise Human Mesh Recovery (HMR) with in-the-wild data is a formidable\nchallenge and is often hindered by depth ambiguities and reduced precision.\nExisting works resort to either pose priors or multi-modal data such as\nmulti-view or point cloud information, though their methods often overlook the\nvaluable scene-depth information inherently present in a single image.\nMoreover, achieving robust HMR for out-of-distribution (OOD) data is\nexceedingly challenging due to inherent variations in pose, shape and depth.\nConsequently, understanding the underlying distribution becomes a vital\nsubproblem in modeling human forms. Motivated by the need for unambiguous and\nrobust human modeling, we introduce Distribution and depth-aware human mesh\nrecovery (D2A-HMR), an end-to-end transformer architecture meticulously\ndesigned to minimize the disparity between distributions and incorporate\nscene-depth leveraging prior depth information. Our approach demonstrates\nsuperior performance in handling OOD data in certain scenarios while\nconsistently achieving competitive results against state-of-the-art HMR methods\non controlled datasets.\n"
    },
    {
        "title": "TBI Image/Text (TBI-IT): Comprehensive Text and Image Datasets for\n  Traumatic Brain Injury Research",
        "published_time": "2024-03-14T03:07:49Z",
        "abstract": "  In this paper, we introduce a new dataset in the medical field of Traumatic\nBrain Injury (TBI), called TBI-IT, which includes both electronic medical\nrecords (EMRs) and head CT images. This dataset is designed to enhance the\naccuracy of artificial intelligence in the diagnosis and treatment of TBI. This\ndataset, built upon the foundation of standard text and image data,\nincorporates specific annotations within the EMRs, extracting key content from\nthe text information, and categorizes the annotation content of imaging data\ninto five types: brain midline, hematoma, left cerebral ventricle, right\ncerebral ventricle and fracture. TBI-IT aims to be a foundational dataset for\nfeature learning in image segmentation tasks and named entity recognition.\n"
    },
    {
        "title": "Leveraging Foundation Model Automatic Data Augmentation Strategies and\n  Skeletal Points for Hands Action Recognition in Industrial Assembly Lines",
        "published_time": "2024-03-14T02:55:06Z",
        "abstract": "  On modern industrial assembly lines, many intelligent algorithms have been\ndeveloped to replace or supervise workers. However, we found that there were\nbottlenecks in both training datasets and real-time performance when deploying\nalgorithms on actual assembly line. Therefore, we developed a promising\nstrategy for expanding industrial datasets, which utilized large models with\nstrong generalization abilities to achieve efficient, high-quality, and\nlarge-scale dataset expansion, solving the problem of insufficient and\nlow-quality industrial datasets. We also applied this strategy to video action\nrecognition. We proposed a method of converting hand action recognition\nproblems into hand skeletal trajectory classification problems, which solved\nthe real-time performance problem of industrial algorithms. In the \"hand\nmovements during wire insertion\" scenarios on the actual assembly line, the\naccuracy of hand action recognition reached 98.8\\%. We conducted detailed\nexperimental analysis to demonstrate the effectiveness and superiority of the\nmethod, and deployed the entire process on Midea's actual assembly line.\n"
    },
    {
        "title": "StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based\n  Semantic Control",
        "published_time": "2024-03-14T02:51:01Z",
        "abstract": "  The enormous success of diffusion models in text-to-image synthesis has made\nthem promising candidates for the next generation of end-user applications for\nimage generation and editing. Previous works have focused on improving the\nusability of diffusion models by reducing the inference time or increasing user\ninteractivity by allowing new, fine-grained controls such as region-based text\nprompts. However, we empirically find that integrating both branches of works\nis nontrivial, limiting the potential of diffusion models. To solve this\nincompatibility, we present StreamMultiDiffusion, the first real-time\nregion-based text-to-image generation framework. By stabilizing fast inference\ntechniques and restructuring the model into a newly proposed multi-prompt\nstream batch architecture, we achieve $\\times 10$ faster panorama generation\nthan existing solutions, and the generation speed of 1.57 FPS in region-based\ntext-to-image synthesis on a single RTX 2080 Ti GPU. Our solution opens up a\nnew paradigm for interactive image generation named semantic palette, where\nhigh-quality images are generated in real-time from given multiple hand-drawn\nregions, encoding prescribed semantic meanings (e.g., eagle, girl). Our code\nand demo application are available at\nhttps://github.com/ironjr/StreamMultiDiffusion.\n"
    },
    {
        "title": "CLOAF: CoLlisiOn-Aware Human Flow",
        "published_time": "2024-03-14T02:38:09Z",
        "abstract": "  Even the best current algorithms for estimating body 3D shape and pose yield\nresults that include body self-intersections. In this paper, we present CLOAF,\nwhich exploits the diffeomorphic nature of Ordinary Differential Equations to\neliminate such self-intersections while still imposing body shape constraints.\nWe show that, unlike earlier approaches to addressing this issue, ours\ncompletely eliminates the self-intersections without compromising the accuracy\nof the reconstructions. Being differentiable, CLOAF can be used to fine-tune\npose and shape estimation baselines to improve their overall performance and\neliminate self-intersections in their predictions. Furthermore, we demonstrate\nhow our CLOAF strategy can be applied to practically any motion field induced\nby the user. CLOAF also makes it possible to edit motion to interact with the\nenvironment without worrying about potential collision or loss of body-shape\nprior.\n"
    },
    {
        "title": "Taming Cross-Domain Representation Variance in Federated Prototype\n  Learning with Heterogeneous Data Domains",
        "published_time": "2024-03-14T02:36:16Z",
        "abstract": "  Federated learning (FL) allows collaborative machine learning training\nwithout sharing private data. While most FL methods assume identical data\ndomains across clients, real-world scenarios often involve heterogeneous data\ndomains. Federated Prototype Learning (FedPL) addresses this issue, using mean\nfeature vectors as prototypes to enhance model generalization. However,\nexisting FedPL methods create the same number of prototypes for each client,\nleading to cross-domain performance gaps and disparities for clients with\nvaried data distributions. To mitigate cross-domain feature representation\nvariance, we introduce FedPLVM, which establishes variance-aware dual-level\nprototypes clustering and employs a novel $\\alpha$-sparsity prototype loss. The\ndual-level prototypes clustering strategy creates local clustered prototypes\nbased on private data features, then performs global prototypes clustering to\nreduce communication complexity and preserve local data privacy. The\n$\\alpha$-sparsity prototype loss aligns samples from underrepresented domains,\nenhancing intra-class similarity and reducing inter-class similarity.\nEvaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our\nmethod's superiority over existing approaches.\n"
    },
    {
        "title": "The First to Know: How Token Distributions Reveal Hidden Knowledge in\n  Large Vision-Language Models?",
        "published_time": "2024-03-14T02:25:35Z",
        "abstract": "  Large vision-language models (LVLMs), designed to interpret and respond to\nhuman instructions, occasionally generate hallucinated or harmful content due\nto inappropriate instructions. This study uses linear probing to shed light on\nthe hidden knowledge at the output layer of LVLMs. We demonstrate that the\nlogit distributions of the first tokens contain sufficient information to\ndetermine whether to respond to the instructions, including recognizing\nunanswerable visual questions, defending against multi-modal jailbreaking\nattack, and identifying deceptive questions. Such hidden knowledge is gradually\nlost in logits of subsequent tokens during response generation. Then, we\nillustrate a simple decoding strategy at the generation of the first token,\neffectively improving the generated content. In experiments, we find a few\ninteresting insights: First, the CLIP model already contains a strong signal\nfor solving these tasks, indicating potential bias in the existing datasets.\nSecond, we observe performance improvement by utilizing the first logit\ndistributions on three additional tasks, including indicting uncertainty in\nmath solving, mitigating hallucination, and image classification. Last, with\nthe same training data, simply finetuning LVLMs improve models' performance but\nis still inferior to linear probing on these tasks.\n"
    },
    {
        "title": "Gradient-Aware Logit Adjustment Loss for Long-tailed Classifier",
        "published_time": "2024-03-14T02:21:01Z",
        "abstract": "  In the real-world setting, data often follows a long-tailed distribution,\nwhere head classes contain significantly more training samples than tail\nclasses. Consequently, models trained on such data tend to be biased toward\nhead classes. The medium of this bias is imbalanced gradients, which include\nnot only the ratio of scale between positive and negative gradients but also\nimbalanced gradients from different negative classes. Therefore, we propose the\nGradient-Aware Logit Adjustment (GALA) loss, which adjusts the logits based on\naccumulated gradients to balance the optimization process. Additionally, We\nfind that most of the solutions to long-tailed problems are still biased\ntowards head classes in the end, and we propose a simple and post hoc\nprediction re-balancing strategy to further mitigate the basis toward head\nclass. Extensive experiments are conducted on multiple popular long-tailed\nrecognition benchmark datasets to evaluate the effectiveness of these two\ndesigns. Our approach achieves top-1 accuracy of 48.5\\%, 41.4\\%, and 73.3\\% on\nCIFAR100-LT, Places-LT, and iNaturalist, outperforming the state-of-the-art\nmethod GCL by a significant margin of 3.62\\%, 0.76\\% and 1.2\\%, respectively.\nCode is available at https://github.com/lt-project-repository/lt-project.\n"
    },
    {
        "title": "rFaceNet: An End-to-End Network for Enhanced Physiological Signal\n  Extraction through Identity-Specific Facial Contours",
        "published_time": "2024-03-14T02:11:16Z",
        "abstract": "  Remote photoplethysmography (rPPG) technique extracts blood volume pulse\n(BVP) signals from subtle pixel changes in video frames. This study introduces\nrFaceNet, an advanced rPPG method that enhances the extraction of facial BVP\nsignals with a focus on facial contours. rFaceNet integrates identity-specific\nfacial contour information and eliminates redundant data. It efficiently\nextracts facial contours from temporally normalized frame inputs through a\nTemporal Compressor Unit (TCU) and steers the model focus to relevant facial\nregions by using the Cross-Task Feature Combiner (CTFC). Through elaborate\ntraining, the quality and interpretability of facial physiological signals\nextracted by rFaceNet are greatly improved compared to previous methods.\nMoreover, our novel approach demonstrates superior performance than SOTA\nmethods in various heart rate estimation benchmarks.\n"
    },
    {
        "title": "Unlocking the conversion of Web Screenshots into HTML Code with the\n  WebSight Dataset",
        "published_time": "2024-03-14T01:40:40Z",
        "abstract": "  Using vision-language models (VLMs) in web development presents a promising\nstrategy to increase efficiency and unblock no-code solutions: by providing a\nscreenshot or a sketch of a UI, a VLM could generate the code to reproduce it,\nfor instance in a language like HTML. Despite the advancements in VLMs for\nvarious tasks, the specific challenge of converting a screenshot into a\ncorresponding HTML has been minimally explored. We posit that this is mainly\ndue to the absence of a suitable, high-quality dataset. This work introduces\nWebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and\ntheir corresponding screenshots. We fine-tune a foundational VLM on our dataset\nand show proficiency in converting webpage screenshots to functional HTML code.\nTo accelerate the research in this area, we open-source WebSight.\n"
    },
    {
        "title": "VisionGPT: Vision-Language Understanding Agent Using Generalized\n  Multimodal Framework",
        "published_time": "2024-03-14T01:39:40Z",
        "abstract": "  With the emergence of large language models (LLMs) and vision foundation\nmodels, how to combine the intelligence and capacity of these open-sourced or\nAPI-available models to achieve open-world visual perception remains an open\nquestion. In this paper, we introduce VisionGPT to consolidate and automate the\nintegration of state-of-the-art foundation models, thereby facilitating\nvision-language understanding and the development of vision-oriented AI.\nVisionGPT builds upon a generalized multimodal framework that distinguishes\nitself through three key features: (1) utilizing LLMs (e.g., LLaMA-2) as the\npivot to break down users' requests into detailed action proposals to call\nsuitable foundation models; (2) integrating multi-source outputs from\nfoundation models automatically and generating comprehensive responses for\nusers; (3) adaptable to a wide range of applications such as text-conditioned\nimage understanding/generation/editing and visual question answering. This\npaper outlines the architecture and capabilities of VisionGPT, demonstrating\nits potential to revolutionize the field of computer vision through enhanced\nefficiency, versatility, and generalization, and performance. Our code and\nmodels will be made publicly available. Keywords: VisionGPT, Open-world visual\nperception, Vision-language understanding, Large language model, and Foundation\nmodel\n"
    },
    {
        "title": "VDNA-PR: Using General Dataset Representations for Robust Sequential\n  Visual Place Recognition",
        "published_time": "2024-03-14T01:30:28Z",
        "abstract": "  This paper adapts a general dataset representation technique to produce\nrobust Visual Place Recognition (VPR) descriptors, crucial to enable real-world\nmobile robot localisation. Two parallel lines of work on VPR have shown, on one\nside, that general-purpose off-the-shelf feature representations can provide\nrobustness to domain shifts, and, on the other, that fused information from\nsequences of images improves performance. In our recent work on measuring\ndomain gaps between image datasets, we proposed a Visual Distribution of Neuron\nActivations (VDNA) representation to represent datasets of images. This\nrepresentation can naturally handle image sequences and provides a general and\ngranular feature representation derived from a general-purpose model. Moreover,\nour representation is based on tracking neuron activation values over the list\nof images to represent and is not limited to a particular neural network layer,\ntherefore having access to high- and low-level concepts. This work shows how\nVDNAs can be used for VPR by learning a very lightweight and simple encoder to\ngenerate task-specific descriptors. Our experiments show that our\nrepresentation can allow for better robustness than current solutions to\nserious domain shifts away from the training data distribution, such as to\nindoor environments and aerial imagery.\n"
    },
    {
        "title": "CART: Caltech Aerial RGB-Thermal Dataset in the Wild",
        "published_time": "2024-03-13T23:31:04Z",
        "abstract": "  We present the first publicly available RGB-thermal dataset designed for\naerial robotics operating in natural environments. Our dataset captures a\nvariety of terrains across the continental United States, including rivers,\nlakes, coastlines, deserts, and forests, and consists of synchronized RGB,\nlong-wave thermal, global positioning, and inertial data. Furthermore, we\nprovide semantic segmentation annotations for 10 classes commonly encountered\nin natural settings in order to facilitate the development of perception\nalgorithms robust to adverse weather and nighttime conditions. Using this\ndataset, we propose new and challenging benchmarks for thermal and RGB-thermal\nsemantic segmentation, RGB-to-thermal image translation, and visual-inertial\nodometry. We present extensive results using state-of-the-art methods and\nhighlight the challenges posed by temporal and geographical domain shifts in\nour data. Dataset and accompanying code will be provided at\nhttps://github.com/aerorobotics/caltech-aerial-rgbt-dataset\n"
    },
    {
        "title": "NTIRE 2023 Image Shadow Removal Challenge Technical Report: Team IIM_TTI",
        "published_time": "2024-03-13T23:27:31Z",
        "abstract": "  In this paper, we analyze and discuss ShadowFormer in preparation for the\nNTIRE2023 Shadow Removal Challenge [1], implementing five key improvements:\nimage alignment, the introduction of a perceptual quality loss function, the\nsemi-automatic annotation for shadow detection, joint learning of shadow\ndetection and removal, and the introduction of new data augmentation techniques\nfor shadow removal. Our method achieved scores of 0.196 (3rd out of 19) in\nLPIPS and 7.44 (3rd out of 19) in the Mean Opinion Score (MOS).\n"
    },
    {
        "title": "7T MRI Synthesization from 3T Acquisitions",
        "published_time": "2024-03-13T22:06:44Z",
        "abstract": "  Supervised deep learning techniques can be used to generate synthetic 7T MRIs\nfrom 3T MRI inputs. This image enhancement process leverages the advantages of\nultra-high-field MRI to improve the signal-to-noise and contrast-to-noise\nratios of 3T acquisitions. In this paper, we introduce multiple novel 7T\nsynthesization algorithms based on custom-designed variants of the V-Net\nconvolutional neural network. We demonstrate that the V-Net based model has\nsuperior performance in enhancing both single-site and multi-site MRI datasets\ncompared to the existing benchmark model. When trained on 3T-7T MRI pairs from\n8 subjects with mild Traumatic Brain Injury (TBI), our model achieves\nstate-of-the-art 7T synthesization performance. Compared to previous works,\nsynthetic 7T images generated from our pipeline also display superior\nenhancement of pathological tissue. Additionally, we implement and test a data\naugmentation scheme for training models that are robust to variations in the\ninput distribution. This allows synthetic 7T models to accommodate\nintra-scanner and inter-scanner variability in multisite datasets. On a\nharmonized dataset consisting of 18 3T-7T MRI pairs from two institutions,\nincluding both healthy subjects and those with mild TBI, our model maintains\nits performance and can generalize to 3T MRI inputs with lower resolution. Our\nfindings demonstrate the promise of V-Net based models for MRI enhancement and\noffer a preliminary probe into improving the generalizability of synthetic 7T\nmodels with data augmentation.\n"
    },
    {
        "title": "Representing Anatomical Trees by Denoising Diffusion of Implicit Neural\n  Fields",
        "published_time": "2024-03-13T21:43:24Z",
        "abstract": "  Anatomical trees play a central role in clinical diagnosis and treatment\nplanning. However, accurately representing anatomical trees is challenging due\nto their varying and complex topology and geometry. Traditional methods for\nrepresenting tree structures, captured using medical imaging, while invaluable\nfor visualizing vascular and bronchial networks, exhibit drawbacks in terms of\nlimited resolution, flexibility, and efficiency. Recently, implicit neural\nrepresentations (INRs) have emerged as a powerful tool for representing shapes\naccurately and efficiently. We propose a novel approach for representing\nanatomical trees using INR, while also capturing the distribution of a set of\ntrees via denoising diffusion in the space of INRs. We accurately capture the\nintricate geometries and topologies of anatomical trees at any desired\nresolution. Through extensive qualitative and quantitative evaluation, we\ndemonstrate high-fidelity tree reconstruction with arbitrary resolution yet\ncompact storage, and versatility across anatomical sites and tree complexities.\n"
    },
    {
        "title": "PathM3: A Multimodal Multi-Task Multiple Instance Learning Framework for\n  Whole Slide Image Classification and Captioning",
        "published_time": "2024-03-13T21:19:12Z",
        "abstract": "  In the field of computational histopathology, both whole slide images (WSIs)\nand diagnostic captions provide valuable insights for making diagnostic\ndecisions. However, aligning WSIs with diagnostic captions presents a\nsignificant challenge. This difficulty arises from two main factors: 1)\nGigapixel WSIs are unsuitable for direct input into deep learning models, and\nthe redundancy and correlation among the patches demand more attention; and 2)\nAuthentic WSI diagnostic captions are extremely limited, making it difficult to\ntrain an effective model. To overcome these obstacles, we present PathM3, a\nmultimodal, multi-task, multiple instance learning (MIL) framework for WSI\nclassification and captioning. PathM3 adapts a query-based transformer to\neffectively align WSIs with diagnostic captions. Given that histopathology\nvisual patterns are redundantly distributed across WSIs, we aggregate each\npatch feature with MIL method that considers the correlations among instances.\nFurthermore, our PathM3 overcomes data scarcity in WSI-level captions by\nleveraging limited WSI diagnostic caption data in the manner of multi-task\njoint learning. Extensive experiments with improved classification accuracy and\ncaption generation demonstrate the effectiveness of our method on both WSI\nclassification and captioning task.\n"
    },
    {
        "title": "Using Deep Learning for Morphological Classification in Pigs with a\n  Focus on Sanitary Monitoring",
        "published_time": "2024-03-13T21:05:34Z",
        "abstract": "  The aim of this paper is to evaluate the use of D-CNN (Deep Convolutional\nNeural Networks) algorithms to classify pig body conditions in normal or not\nnormal conditions, with a focus on characteristics that are observed in\nsanitary monitoring, and were used six different algorithms to do this task.\nThe study focused on five pig characteristics, being these caudophagy, ear\nhematoma, scratches on the body, redness, and natural stains (brown or black).\nThe results of the study showed that D-CNN was effective in classifying\ndeviations in pig body morphologies related to skin characteristics. The\nevaluation was conducted by analyzing the performance metrics Precision,\nRecall, and F-score, as well as the statistical analyses ANOVA and the\nScott-Knott test. The contribution of this article is characterized by the\nproposal of using D-CNN networks for morphological classification in pigs, with\na focus on characteristics identified in sanitary monitoring. Among the best\nresults, the average Precision metric of 80.6\\% to classify caudophagy was\nachieved for the InceptionResNetV2 network, indicating the potential use of\nthis technology for the proposed task. Additionally, a new image database was\ncreated, containing various pig's distinct body characteristics, which can\nserve as data for future research.\n"
    },
    {
        "title": "Robust COVID-19 Detection in CT Images with CLIP",
        "published_time": "2024-03-13T20:26:50Z",
        "abstract": "  In the realm of medical imaging, particularly for COVID-19 detection, deep\nlearning models face substantial challenges such as the necessity for extensive\ncomputational resources, the paucity of well-annotated datasets, and a\nsignificant amount of unlabeled data. In this work, we introduce the first\nlightweight detector designed to overcome these obstacles, leveraging a frozen\nCLIP image encoder and a trainable multilayer perception (MLP). Enhanced with\nConditional Value at Risk (CVaR) for robustness and a loss landscape flattening\nstrategy for improved generalization, our model is tailored for high efficacy\nin COVID-19 detection. Furthermore, we integrate a teacher-student framework to\ncapitalize on the vast amounts of unlabeled data, enabling our model to achieve\nsuperior performance despite the inherent data limitations. Experimental\nresults on the COV19-CT-DB dataset demonstrate the effectiveness of our\napproach, surpassing baseline by up to 10.6% in `macro' F1 score in supervised\nlearning. The code is available at\nhttps://github.com/Purdue-M2/COVID-19_Detection_M2_PURDUE.\n"
    },
    {
        "title": "FogGuard: guarding YOLO against fog using perceptual loss",
        "published_time": "2024-03-13T20:13:25Z",
        "abstract": "  In this paper, we present a novel fog-aware object detection network called\nFogGuard, designed to address the challenges posed by foggy weather conditions.\nAutonomous driving systems heavily rely on accurate object detection\nalgorithms, but adverse weather conditions can significantly impact the\nreliability of deep neural networks (DNNs).\n  Existing approaches fall into two main categories, 1) image enhancement such\nas IA-YOLO 2) domain adaptation based approaches. Image enhancement based\ntechniques attempt to generate fog-free image. However, retrieving a fogless\nimage from a foggy image is a much harder problem than detecting objects in a\nfoggy image. Domain-adaptation based approaches, on the other hand, do not make\nuse of labelled datasets in the target domain. Both categories of approaches\nare attempting to solve a harder version of the problem. Our approach builds\nover fine-tuning on the\n  Our framework is specifically designed to compensate for foggy conditions\npresent in the scene, ensuring robust performance even. We adopt YOLOv3 as the\nbaseline object detection algorithm and introduce a novel Teacher-Student\nPerceptual loss, to high accuracy object detection in foggy images.\n  Through extensive evaluations on common datasets such as PASCAL VOC and RTTS,\nwe demonstrate the improvement in performance achieved by our network. We\ndemonstrate that FogGuard achieves 69.43\\% mAP, as compared to 57.78\\% for\nYOLOv3 on the RTTS dataset.\n  Furthermore, we show that while our training method increases time\ncomplexity, it does not introduce any additional overhead during inference\ncompared to the regular YOLO network.\n"
    },
    {
        "title": "Unveiling the Truth: Exploring Human Gaze Patterns in Fake Images",
        "published_time": "2024-03-13T19:56:30Z",
        "abstract": "  Creating high-quality and realistic images is now possible thanks to the\nimpressive advancements in image generation. A description in natural language\nof your desired output is all you need to obtain breathtaking results. However,\nas the use of generative models grows, so do concerns about the propagation of\nmalicious content and misinformation. Consequently, the research community is\nactively working on the development of novel fake detection techniques,\nprimarily focusing on low-level features and possible fingerprints left by\ngenerative models during the image generation process. In a different vein, in\nour work, we leverage human semantic knowledge to investigate the possibility\nof being included in frameworks of fake image detection. To achieve this, we\ncollect a novel dataset of partially manipulated images using diffusion models\nand conduct an eye-tracking experiment to record the eye movements of different\nobservers while viewing real and fake stimuli. A preliminary statistical\nanalysis is conducted to explore the distinctive patterns in how humans\nperceive genuine and altered images. Statistical findings reveal that, when\nperceiving counterfeit samples, humans tend to focus on more confined regions\nof the image, in contrast to the more dispersed observational pattern observed\nwhen viewing genuine images. Our dataset is publicly available at:\nhttps://github.com/aimagelab/unveiling-the-truth.\n"
    },
    {
        "title": "CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with\n  Ground Truth Flow",
        "published_time": "2024-03-13T19:21:03Z",
        "abstract": "  Autonomous driving stands as a pivotal domain in computer vision, shaping the\nfuture of transportation. Within this paradigm, the backbone of the system\nplays a crucial role in interpreting the complex environment. However, a\nnotable challenge has been the loss of clear supervision when it comes to\nBird's Eye View elements. To address this limitation, we introduce\nCLIP-BEVFormer, a novel approach that leverages the power of contrastive\nlearning techniques to enhance the multi-view image-derived BEV backbones with\nground truth information flow. We conduct extensive experiments on the\nchallenging nuScenes dataset and showcase significant and consistent\nimprovements over the SOTA. Specifically, CLIP-BEVFormer achieves an impressive\n8.5\\% and 9.2\\% enhancement in terms of NDS and mAP, respectively, over the\nprevious best BEV model on the 3D object detection task.\n"
    },
    {
        "title": "Cross-Modal Learning of Housing Quality in Amsterdam",
        "published_time": "2024-03-13T19:11:58Z",
        "abstract": "  In our research we test data and models for the recognition of housing\nquality in the city of Amsterdam from ground-level and aerial imagery. For\nground-level images we compare Google StreetView (GSV) to Flickr images. Our\nresults show that GSV predicts the most accurate building quality scores,\napproximately 30% better than using only aerial images. However, we find that\nthrough careful filtering and by using the right pre-trained model, Flickr\nimage features combined with aerial image features are able to halve the\nperformance gap to GSV features from 30% to 15%. Our results indicate that\nthere are viable alternatives to GSV for liveability factor prediction, which\nis encouraging as GSV images are more difficult to acquire and not always\navailable.\n"
    },
    {
        "title": "Envision3D: One Image to 3D with Anchor Views Interpolation",
        "published_time": "2024-03-13T18:46:33Z",
        "abstract": "  We present Envision3D, a novel method for efficiently generating high-quality\n3D content from a single image. Recent methods that extract 3D content from\nmulti-view images generated by diffusion models show great potential. However,\nit is still challenging for diffusion models to generate dense multi-view\nconsistent images, which is crucial for the quality of 3D content extraction.\nTo address this issue, we propose a novel cascade diffusion framework, which\ndecomposes the challenging dense views generation task into two tractable\nstages, namely anchor views generation and anchor views interpolation. In the\nfirst stage, we train the image diffusion model to generate global consistent\nanchor views conditioning on image-normal pairs. Subsequently, leveraging our\nvideo diffusion model fine-tuned on consecutive multi-view images, we conduct\ninterpolation on the previous anchor views to generate extra dense views. This\nframework yields dense, multi-view consistent images, providing comprehensive\n3D information. To further enhance the overall generation quality, we introduce\na coarse-to-fine sampling strategy for the reconstruction algorithm to robustly\nextract textured meshes from the generated dense images. Extensive experiments\ndemonstrate that our method is capable of generating high-quality 3D content in\nterms of texture and geometry, surpassing previous image-to-3D baseline\nmethods.\n"
    },
    {
        "title": "Federated Data Model",
        "published_time": "2024-03-13T18:16:54Z",
        "abstract": "  In artificial intelligence (AI), especially deep learning, data diversity and\nvolume play a pivotal role in model development. However, training a robust\ndeep learning model often faces challenges due to data privacy, regulations,\nand the difficulty of sharing data between different locations, especially for\nmedical applications. To address this, we developed a method called the\nFederated Data Model (FDM). This method uses diffusion models to learn the\ncharacteristics of data at one site and then creates synthetic data that can be\nused at another site without sharing the actual data. We tested this approach\nwith a medical image segmentation task, focusing on cardiac magnetic resonance\nimages from different hospitals. Our results show that models trained with this\nmethod perform well both on the data they were originally trained on and on\ndata from other sites. This approach offers a promising way to train accurate\nand privacy-respecting AI models across different locations.\n"
    },
    {
        "title": "SLCF-Net: Sequential LiDAR-Camera Fusion for Semantic Scene Completion\n  using a 3D Recurrent U-Net",
        "published_time": "2024-03-13T18:12:53Z",
        "abstract": "  We introduce SLCF-Net, a novel approach for the Semantic Scene Completion\n(SSC) task that sequentially fuses LiDAR and camera data. It jointly estimates\nmissing geometry and semantics in a scene from sequences of RGB images and\nsparse LiDAR measurements. The images are semantically segmented by a\npre-trained 2D U-Net and a dense depth prior is estimated from a\ndepth-conditioned pipeline fueled by Depth Anything. To associate the 2D image\nfeatures with the 3D scene volume, we introduce Gaussian-decay Depth-prior\nProjection (GDP). This module projects the 2D features into the 3D volume along\nthe line of sight with a Gaussian-decay function, centered around the depth\nprior. Volumetric semantics is computed by a 3D U-Net. We propagate the hidden\n3D U-Net state using the sensor motion and design a novel loss to ensure\ntemporal consistency. We evaluate our approach on the SemanticKITTI dataset and\ncompare it with leading SSC approaches. The SLCF-Net excels in all SSC metrics\nand shows great temporal consistency.\n"
    },
    {
        "title": "ARtVista: Gateway To Empower Anyone Into Artist",
        "published_time": "2024-03-13T18:00:57Z",
        "abstract": "  Drawing is an art that enables people to express their imagination and\nemotions. However, individuals usually face challenges in drawing, especially\nwhen translating conceptual ideas into visually coherent representations and\nbridging the gap between mental visualization and practical execution. In\nresponse, we propose ARtVista - a novel system integrating AR and generative AI\ntechnologies. ARtVista not only recommends reference images aligned with users'\nabstract ideas and generates sketches for users to draw but also goes beyond,\ncrafting vibrant paintings in various painting styles. ARtVista also offers\nusers an alternative approach to create striking paintings by simulating the\npaint-by-number concept on reference images, empowering users to create\nvisually stunning artwork devoid of the necessity for advanced drawing skills.\nWe perform a pilot study and reveal positive feedback on its usability,\nemphasizing its effectiveness in visualizing user ideas and aiding the painting\nprocess to achieve stunning pictures without requiring advanced drawing skills.\nThe source code will be available at https://github.com/htrvu/ARtVista.\n"
    },
    {
        "title": "DialogGen: Multi-modal Interactive Dialogue System for Multi-turn\n  Text-to-Image Generation",
        "published_time": "2024-03-13T18:00:01Z",
        "abstract": "  Text-to-image (T2I) generation models have significantly advanced in recent\nyears. However, effective interaction with these models is challenging for\naverage users due to the need for specialized prompt engineering knowledge and\nthe inability to perform multi-turn image generation, hindering a dynamic and\niterative creation process. Recent attempts have tried to equip Multi-modal\nLarge Language Models (MLLMs) with T2I models to bring the user's natural\nlanguage instructions into reality. Hence, the output modality of MLLMs is\nextended, and the multi-turn generation quality of T2I models is enhanced\nthanks to the strong multi-modal comprehension ability of MLLMs. However, many\nof these works face challenges in identifying correct output modalities and\ngenerating coherent images accordingly as the number of output modalities\nincreases and the conversations go deeper. Therefore, we propose DialogGen, an\neffective pipeline to align off-the-shelf MLLMs and T2I models to build a\nMulti-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image\ngeneration. It is composed of drawing prompt alignment, careful training data\ncuration, and error correction. Moreover, as the field of MIDS flourishes,\ncomprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms\nof output modality correctness and multi-modal output coherence. To address\nthis issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a\ncomprehensive bilingual benchmark designed to assess the ability of MLLMs to\ngenerate accurate and coherent multi-modal content that supports image editing.\nIt contains two evaluation metrics to measure the model's ability to switch\nmodalities and the coherence of the output images. Our extensive experiments on\nDialogBen and user study demonstrate the effectiveness of DialogGen compared\nwith other State-of-the-Art models.\n"
    },
    {
        "title": "PAPERCLIP: Associating Astronomical Observations and Natural Language\n  with Multi-Modal Models",
        "published_time": "2024-03-13T18:00:00Z",
        "abstract": "  We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation\nfor Contrastive Language-Image Pre-training), a method which associates\nastronomical observations imaged by telescopes with natural language using a\nneural network model. The model is fine-tuned from a pre-trained Contrastive\nLanguage-Image Pre-training (CLIP) model using successful observing proposal\nabstracts and corresponding downstream observations, with the abstracts\noptionally summarized via guided generation using large language models (LLMs).\nUsing observations from the Hubble Space Telescope (HST) as an example, we show\nthat the fine-tuned model embodies a meaningful joint representation between\nobservations and natural language through tests targeting image retrieval\n(i.e., finding the most relevant observations using natural language queries)\nand description retrieval (i.e., querying for astrophysical object classes and\nuse cases most relevant to a given observation). Our study demonstrates the\npotential for using generalist foundation models rather than task-specific\nmodels for interacting with astronomical data by leveraging text as an\ninterface.\n"
    },
    {
        "title": "FastMAC: Stochastic Spectral Sampling of Correspondence Graph",
        "published_time": "2024-03-13T17:59:56Z",
        "abstract": "  3D correspondence, i.e., a pair of 3D points, is a fundamental concept in\ncomputer vision. A set of 3D correspondences, when equipped with compatibility\nedges, forms a correspondence graph. This graph is a critical component in\nseveral state-of-the-art 3D point cloud registration approaches, e.g., the one\nbased on maximal cliques (MAC). However, its properties have not been well\nunderstood. So we present the first study that introduces graph signal\nprocessing into the domain of correspondence graph. We exploit the generalized\ndegree signal on correspondence graph and pursue sampling strategies that\npreserve high-frequency components of this signal. To address time-consuming\nsingular value decomposition in deterministic sampling, we resort to a\nstochastic approximate sampling strategy. As such, the core of our method is\nthe stochastic spectral sampling of correspondence graph. As an application, we\nbuild a complete 3D registration algorithm termed as FastMAC, that reaches\nreal-time speed while leading to little to none performance drop. Through\nextensive experiments, we validate that FastMAC works for both indoor and\noutdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while\nmaintaining high registration success rate on KITTI. Codes are publicly\navailable at https://github.com/Forrest-110/FastMAC.\n"
    },
    {
        "title": "3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface",
        "published_time": "2024-03-13T17:59:50Z",
        "abstract": "  This paper introduces 3DFIRES, a novel system for scene-level 3D\nreconstruction from posed images. Designed to work with as few as one view,\n3DFIRES reconstructs the complete geometry of unseen scenes, including hidden\nsurfaces. With multiple view inputs, our method produces full reconstruction\nwithin all camera frustums. A key feature of our approach is the fusion of\nmulti-view information at the feature level, enabling the production of\ncoherent and comprehensive 3D reconstruction. We train our system on\nnon-watertight scans from large-scale real scene dataset. We show it matches\nthe efficacy of single-view reconstruction methods with only one input and\nsurpasses existing techniques in both quantitative and qualitative measures for\nsparse-view 3D reconstruction.\n"
    },
    {
        "title": "MonoOcc: Digging into Monocular Semantic Occupancy Prediction",
        "published_time": "2024-03-13T17:59:04Z",
        "abstract": "  Monocular Semantic Occupancy Prediction aims to infer the complete 3D\ngeometry and semantic information of scenes from only 2D images. It has\ngarnered significant attention, particularly due to its potential to enhance\nthe 3D perception of autonomous vehicles. However, existing methods rely on a\ncomplex cascaded framework with relatively limited information to restore 3D\nscenes, including a dependency on supervision solely on the whole network's\noutput, single-frame input, and the utilization of a small backbone. These\nchallenges, in turn, hinder the optimization of the framework and yield\ninferior prediction results, particularly concerning smaller and long-tailed\nobjects. To address these issues, we propose MonoOcc. In particular, we (i)\nimprove the monocular occupancy prediction framework by proposing an auxiliary\nsemantic loss as supervision to the shallow layers of the framework and an\nimage-conditioned cross-attention module to refine voxel features with visual\nclues, and (ii) employ a distillation module that transfers temporal\ninformation and richer knowledge from a larger image backbone to the monocular\nsemantic occupancy prediction framework with low cost of hardware. With these\nadvantages, our method yields state-of-the-art performance on the camera-based\nSemanticKITTI Scene Completion benchmark. Codes and models can be accessed at\nhttps://github.com/ucaszyp/MonoOcc\n"
    },
    {
        "title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis",
        "published_time": "2024-03-13T17:59:02Z",
        "abstract": "  We propose VLOGGER, a method for audio-driven human video generation from a\nsingle input image of a person, which builds on the success of recent\ngenerative diffusion models. Our method consists of 1) a stochastic\nhuman-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture\nthat augments text-to-image models with both spatial and temporal controls.\nThis supports the generation of high quality video of variable length, easily\ncontrollable through high-level representations of human faces and bodies. In\ncontrast to previous work, our method does not require training for each\nperson, does not rely on face detection and cropping, generates the complete\nimage (not just the face or the lips), and considers a broad spectrum of\nscenarios (e.g. visible torso or diverse subject identities) that are critical\nto correctly synthesize humans who communicate. We also curate MENTOR, a new\nand diverse dataset with 3d pose and expression annotations, one order of\nmagnitude larger than previous ones (800,000 identities) and with dynamic\ngestures, on which we train and ablate our main technical contributions.\n  VLOGGER outperforms state-of-the-art methods in three public benchmarks,\nconsidering image quality, identity preservation and temporal consistency while\nalso generating upper-body gestures. We analyze the performance of VLOGGER with\nrespect to multiple diversity metrics, showing that our architectural choices\nand the use of MENTOR benefit training a fair and unbiased model at scale.\nFinally we show applications in video editing and personalization.\n"
    },
    {
        "title": "Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative\n  Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches",
        "published_time": "2024-03-13T17:58:34Z",
        "abstract": "  Knee osteoarthritis is a degenerative joint disease that induces chronic pain\nand disability. Bone morphological analysis is a promising tool to understand\nthe mechanical aspect of this disorder. This study proposes a 2D bone\nmorphological analysis using manually segmented bones to explore morphological\nfeatures related to distinct pain conditions. Furthermore, six semantic\nsegmentation algorithms are assessed for extracting femur and tibia bones from\nX-ray images. Our analysis reveals that the morphology of the femur undergoes\nsignificant changes in instances where pain worsens. Conversely, improvements\nin pain may not manifest pronounced alterations in bone shape. The\nfew-shot-learning-based algorithm, UniverSeg, demonstrated superior\nsegmentation results with Dice scores of 99.69% for femur and 99.60% for tibia.\nRegarding pain condition classification, the zero-shot-learning-based\nalgorithm, CP-SAM, achieved the highest accuracy at 66% among all models.\nUniverSeg is recommended for automatic knee bone segmentation, while SAM models\nshow potential with prompt encoder modifications for optimized outcomes. These\nfindings highlight the effectiveness of few-shot learning for semantic\nsegmentation and the potential of zero-shot learning in enhancing\nclassification models for knee osteoarthritis diagnosis.\n"
    },
    {
        "title": "MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving\n  Representation Learning",
        "published_time": "2024-03-13T17:58:00Z",
        "abstract": "  Learning robust and scalable visual representations from massive multi-view\nvideo data remains a challenge in computer vision and autonomous driving.\nExisting pre-training methods either rely on expensive supervised learning with\n3D annotations, limiting the scalability, or focus on single-frame or monocular\ninputs, neglecting the temporal information. We propose MIM4D, a novel\npre-training paradigm based on dual masked image modeling (MIM). MIM4D\nleverages both spatial and temporal relations by training on masked multi-view\nvideo inputs. It constructs pseudo-3D features using continuous scene flow and\nprojects them onto 2D plane for supervision. To address the lack of dense 3D\nsupervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable\nrendering to learn geometric representations. We demonstrate that MIM4D\nachieves state-of-the-art performance on the nuScenes dataset for visual\nrepresentation learning in autonomous driving. It significantly improves\nexisting methods on multiple downstream tasks, including BEV segmentation (8.7%\nIoU), 3D object detection (3.5% mAP), and HD map construction (1.4% mAP). Our\nwork offers a new choice for learning representation at scale in autonomous\ndriving. Code and models are released at https://github.com/hustvl/MIM4D\n"
    },
    {
        "title": "Spatiotemporal Diffusion Model with Paired Sampling for Accelerated\n  Cardiac Cine MRI",
        "published_time": "2024-03-13T17:56:12Z",
        "abstract": "  Current deep learning reconstruction for accelerated cardiac cine MRI suffers\nfrom spatial and temporal blurring. We aim to improve image sharpness and\nmotion delineation for cine MRI under high undersampling rates. A\nspatiotemporal diffusion enhancement model conditional on an existing deep\nlearning reconstruction along with a novel paired sampling strategy was\ndeveloped. The diffusion model provided sharper tissue boundaries and clearer\nmotion than the original reconstruction in experts evaluation on clinical data.\nThe innovative paired sampling strategy substantially reduced artificial noises\nin the generative results.\n"
    },
    {
        "title": "DAM: Dynamic Adapter Merging for Continual Video QA Learning",
        "published_time": "2024-03-13T17:53:47Z",
        "abstract": "  We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM\n"
    },
    {
        "title": "Clinically Feasible Diffusion Reconstruction for Highly-Accelerated\n  Cardiac Cine MRI",
        "published_time": "2024-03-13T17:51:01Z",
        "abstract": "  The currently limited quality of accelerated cardiac cine reconstruction may\npotentially be improved by the emerging diffusion models, but the clinically\nunacceptable long processing time poses a challenge. We aim to develop a\nclinically feasible diffusion-model-based reconstruction pipeline to improve\nthe image quality of cine MRI. A multi-in multi-out diffusion enhancement model\ntogether with fast inference strategies were developed to be used in\nconjunction with a reconstruction model. The diffusion reconstruction reduced\nspatial and temporal blurring in prospectively undersampled clinical data, as\nvalidated by experts inspection. The 1.5s per video processing time enabled the\napproach to be applied in clinical scenarios.\n"
    },
    {
        "title": "Real-time 3D semantic occupancy prediction for autonomous vehicles using\n  memory-efficient sparse convolution",
        "published_time": "2024-03-13T17:50:59Z",
        "abstract": "  In autonomous vehicles, understanding the surrounding 3D environment of the\nego vehicle in real-time is essential. A compact way to represent scenes while\nencoding geometric distances and semantic object information is via 3D semantic\noccupancy maps. State of the art 3D mapping methods leverage transformers with\ncross-attention mechanisms to elevate 2D vision-centric camera features into\nthe 3D domain. However, these methods encounter significant challenges in\nreal-time applications due to their high computational demands during\ninference. This limitation is particularly problematic in autonomous vehicles,\nwhere GPU resources must be shared with other tasks such as localization and\nplanning. In this paper, we introduce an approach that extracts features from\nfront-view 2D camera images and LiDAR scans, then employs a sparse convolution\nnetwork (Minkowski Engine), for 3D semantic occupancy prediction. Given that\noutdoor scenes in autonomous driving scenarios are inherently sparse, the\nutilization of sparse convolution is particularly apt. By jointly solving the\nproblems of 3D scene completion of sparse scenes and 3D semantic segmentation,\nwe provide a more efficient learning framework suitable for real-time\napplications in autonomous vehicles. We also demonstrate competitive accuracy\non the nuScenes dataset.\n"
    },
    {
        "title": "iCONTRA: Toward Thematic Collection Design Via Interactive Concept\n  Transfer",
        "published_time": "2024-03-13T17:48:39Z",
        "abstract": "  Creating thematic collections in industries demands innovative designs and\ncohesive concepts. Designers may face challenges in maintaining thematic\nconsistency when drawing inspiration from existing objects, landscapes, or\nartifacts. While AI-powered graphic design tools offer help, they often fail to\ngenerate cohesive sets based on specific thematic concepts. In response, we\nintroduce iCONTRA, an interactive CONcept TRAnsfer system. With a user-friendly\ninterface, iCONTRA enables both experienced designers and novices to\neffortlessly explore creative design concepts and efficiently generate thematic\ncollections. We also propose a zero-shot image editing algorithm, eliminating\nthe need for fine-tuning models, which gradually integrates information from\ninitial objects, ensuring consistency in the generation process without\ninfluencing the background. A pilot study suggests iCONTRA's potential to\nreduce designers' efforts. Experimental results demonstrate its effectiveness\nin producing consistent and high-quality object concept transfers. iCONTRA\nstands as a promising tool for innovation and creative exploration in thematic\ncollection design. The source code will be available at:\nhttps://github.com/vdkhoi20/iCONTRA.\n"
    },
    {
        "title": "Strengthening Multimodal Large Language Model with Bootstrapped\n  Preference Optimization",
        "published_time": "2024-03-13T17:29:45Z",
        "abstract": "  Multimodal Large Language Models (MLLMs) excel in generating responses based\non visual inputs. However, they often suffer from a bias towards generating\nresponses similar to their pretraining corpus, overshadowing the importance of\nvisual information. We treat this bias as a \"preference\" for pretraining\nstatistics, which hinders the model's grounding in visual input. To mitigate\nthis issue, we propose Bootstrapped Preference Optimization (BPO), which\nconducts preference learning with datasets containing negative responses\nbootstrapped from the model itself. Specifically, we propose the following two\nstrategies: 1) using distorted image inputs to the MLLM for eliciting responses\nthat contain signified pretraining bias; 2) leveraging text-based LLM to\nexplicitly inject erroneous but common elements into the original response.\nThose undesirable responses are paired with original annotated responses from\nthe datasets to construct the preference dataset, which is subsequently\nutilized to perform preference learning. Our approach effectively suppresses\npretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive\nexperimentation demonstrates significant performance improvements across\nmultiple benchmarks, advancing the state-of-the-art in multimodal\nconversational systems.\n"
    },
    {
        "title": "Cross-Domain Few-Shot Segmentation via Iterative Support-Query\n  Correspondence Mining",
        "published_time": "2024-01-16T14:45:41Z",
        "abstract": "  Cross-Domain Few-Shot Segmentation (CD-FSS) poses the challenge of segmenting\nnovel categories from a distinct domain using only limited exemplars. In this\npaper, we undertake a comprehensive study of CD-FSS and uncover two crucial\ninsights: (i) the necessity of a fine-tuning stage to effectively transfer the\nlearned meta-knowledge across domains, and (ii) the overfitting risk during the\nna\\\"ive fine-tuning due to the scarcity of novel category examples. With these\ninsights, we propose a novel cross-domain fine-tuning strategy that addresses\nthe challenging CD-FSS tasks. We first design Bi-directional Few-shot\nPrediction (BFP), which establishes support-query correspondence in a\nbi-directional manner, crafting augmented supervision to reduce the overfitting\nrisk. Then we further extend BFP into Iterative Few-shot Adaptor (IFA), which\nis a recursive framework to capture the support-query correspondence\niteratively, targeting maximal exploitation of supervisory signals from the\nsparse novel category samples. Extensive empirical evaluations show that our\nmethod significantly outperforms the state-of-the-arts (+7.8\\%), which verifies\nthat IFA tackles the cross-domain challenges and mitigates the overfitting\nsimultaneously. The code is available at: https://github.com/niejiahao1998/IFA.\n"
    },
    {
        "title": "Ambient Diffusion Posterior Sampling: Solving Inverse Problems with\n  Diffusion Models trained on Corrupted Data",
        "published_time": "2024-03-13T17:28:20Z",
        "abstract": "  We provide a framework for solving inverse problems with diffusion models\nlearned from linearly corrupted data. Our method, Ambient Diffusion Posterior\nSampling (A-DPS), leverages a generative model pre-trained on one type of\ncorruption (e.g. image inpainting) to perform posterior sampling conditioned on\nmeasurements from a potentially different forward process (e.g. image\nblurring). We test the efficacy of our approach on standard natural image\ndatasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes\noutperform models trained on clean data for several image restoration tasks in\nboth speed and performance. We further extend the Ambient Diffusion framework\nto train MRI models with access only to Fourier subsampled multi-coil MRI\nmeasurements at various acceleration factors (R=2, 4, 6, 8). We again observe\nthat models trained on highly subsampled data are better priors for solving\ninverse problems in the high acceleration regime than models trained on fully\nsampled data. We open-source our code and the trained Ambient Diffusion MRI\nmodels: https://github.com/utcsilab/ambient-diffusion-mri .\n"
    },
    {
        "title": "Historical Astronomical Diagrams Decomposition in Geometric Primitives",
        "published_time": "2024-03-13T17:20:25Z",
        "abstract": "  Automatically extracting the geometric content from the hundreds of thousands\nof diagrams drawn in historical manuscripts would enable historians to study\nthe diffusion of astronomical knowledge on a global scale. However,\nstate-of-the-art vectorization methods, often designed to tackle modern data,\nare not adapted to the complexity and diversity of historical astronomical\ndiagrams. Our contribution is thus twofold. First, we introduce a unique\ndataset of 303 astronomical diagrams from diverse traditions, ranging from the\nXIIth to the XVIIIth century, annotated with more than 3000 line segments,\ncircles and arcs. Second, we develop a model that builds on DINO-DETR to enable\nthe prediction of multiple geometric primitives. We show that it can be trained\nsolely on synthetic data and accurately predict primitives on our challenging\ndataset. Our approach widely improves over the LETR baseline, which is\nrestricted to lines, by introducing a meaningful parametrization for multiple\nprimitives, jointly training for detection and parameter refinement, using\ndeformable attention and training on rich synthetic data. Our dataset and code\nare available on our webpage.\n"
    },
    {
        "title": "Diffusion-based Iterative Counterfactual Explanations for Fetal\n  Ultrasound Image Quality Assessment",
        "published_time": "2024-03-13T17:04:56Z",
        "abstract": "  Obstetric ultrasound image quality is crucial for accurate diagnosis and\nmonitoring of fetal health. However, producing high-quality standard planes is\ndifficult, influenced by the sonographer's expertise and factors like the\nmaternal BMI or the fetus dynamics. In this work, we propose using\ndiffusion-based counterfactual explainable AI to generate realistic\nhigh-quality standard planes from low-quality non-standard ones. Through\nquantitative and qualitative evaluation, we demonstrate the effectiveness of\nour method in producing plausible counterfactuals of increased quality. This\nshows future promise both for enhancing training of clinicians by providing\nvisual feedback, as well as for improving image quality and, consequently,\ndownstream diagnosis and monitoring.\n"
    },
    {
        "title": "Deep Learning for In-Orbit Cloud Segmentation and Classification in\n  Hyperspectral Satellite Data",
        "published_time": "2024-03-13T16:58:37Z",
        "abstract": "  This article explores the latest Convolutional Neural Networks (CNNs) for\ncloud detection aboard hyperspectral satellites. The performance of the latest\n1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and\n2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed.\nEvaluation criteria include precision and computational efficiency for in-orbit\ndeployment. Experiments utilize NASA's EO-1 Hyperion data, with varying\nspectral channel numbers after Principal Component Analysis. Results indicate\nthat 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs,\nwhile maintaining compactness with larger spectral channel sets, albeit with\nincreased inference times. However, the performance of 1D CNN degrades with\nsignificant channel reduction. In this context, the 2D-Justo-UNet-Simple offers\nthe best balance for in-orbit deployment, considering precision, memory, and\ntime costs. While nnU-net is suitable for on-ground processing, deployment of\nlightweight 1D-Justo-LiuNet is recommended for high-precision applications.\nAlternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced\ncosts between timing and precision in orbit.\n"
    },
    {
        "title": "FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with\n  Focused Masked Autoencoders",
        "published_time": "2024-03-13T16:57:04Z",
        "abstract": "  In recent years, automated Gallbladder Cancer (GBC) detection has gained the\nattention of researchers. Current state-of-the-art (SOTA) methodologies relying\non ultrasound sonography (US) images exhibit limited generalization,\nemphasizing the need for transformative approaches. We observe that individual\nUS frames may lack sufficient information to capture disease manifestation.\nThis study advocates for a paradigm shift towards video-based GBC detection,\nleveraging the inherent advantages of spatiotemporal representations. Employing\nthe Masked Autoencoder (MAE) for representation learning, we address\nshortcomings in conventional image-based methods. We propose a novel design\ncalled FocusMAE to systematically bias the selection of masking tokens from\nhigh-information regions, fostering a more refined representation of\nmalignancy. Additionally, we contribute the most extensive US video dataset for\nGBC detection. We also note that, this is the first study on US video-based GBC\ndetection. We validate the proposed methods on the curated dataset, and report\na new state-of-the-art (SOTA) accuracy of 96.4% for the GBC detection problem,\nagainst an accuracy of 84% by current Image-based SOTA - GBCNet, and RadFormer,\nand 94.7% by Video-based SOTA - AdaMAE. We further demonstrate the generality\nof the proposed FocusMAE on a public CT-based Covid detection dataset,\nreporting an improvement in accuracy by 3.3% over current baselines. The source\ncode and pretrained models are available at:\nhttps://github.com/sbasu276/FocusMAE.\n"
    },
    {
        "title": "Exploiting Structural Consistency of Chest Anatomy for Unsupervised\n  Anomaly Detection in Radiography Images",
        "published_time": "2024-03-13T16:44:49Z",
        "abstract": "  Radiography imaging protocols focus on particular body regions, therefore\nproducing images of great similarity and yielding recurrent anatomical\nstructures across patients. Exploiting this structured information could\npotentially ease the detection of anomalies from radiography images. To this\nend, we propose a Simple Space-Aware Memory Matrix for In-painting and\nDetecting anomalies from radiography images (abbreviated as SimSID). We\nformulate anomaly detection as an image reconstruction task, consisting of a\nspace-aware memory matrix and an in-painting block in the feature space. During\nthe training, SimSID can taxonomize the ingrained anatomical structures into\nrecurrent visual patterns, and in the inference, it can identify anomalies\n(unseen/modified visual patterns) from the test image. Our SimSID surpasses the\nstate of the arts in unsupervised anomaly detection by +8.0%, +5.0%, and +9.9%\nAUC scores on ZhangLab, COVIDx, and CheXpert benchmark datasets, respectively.\nCode: https://github.com/MrGiovanni/SimSID\n"
    },
    {
        "title": "OneVOS: Unifying Video Object Segmentation with All-in-One Transformer\n  Framework",
        "published_time": "2024-03-13T16:38:26Z",
        "abstract": "  Contemporary Video Object Segmentation (VOS) approaches typically consist\nstages of feature extraction, matching, memory management, and multiple objects\naggregation. Recent advanced models either employ a discrete modeling for these\ncomponents in a sequential manner, or optimize a combined pipeline through\nsubstructure aggregation. However, these existing explicit staged approaches\nprevent the VOS framework from being optimized as a unified whole, leading to\nthe limited capacity and suboptimal performance in tackling complex videos. In\nthis paper, we propose OneVOS, a novel framework that unifies the core\ncomponents of VOS with All-in-One Transformer. Specifically, to unify all\naforementioned modules into a vision transformer, we model all the features of\nframes, masks and memory for multiple objects as transformer tokens, and\nintegrally accomplish feature extraction, matching and memory management of\nmultiple objects through the flexible attention mechanism. Furthermore, a\nUnidirectional Hybrid Attention is proposed through a double decoupling of the\noriginal attention operation, to rectify semantic errors and ambiguities of\nstored tokens in OneVOS framework. Finally, to alleviate the storage burden and\nexpedite inference, we propose the Dynamic Token Selector, which unveils the\nworking mechanism of OneVOS and naturally leads to a more efficient version of\nOneVOS. Extensive experiments demonstrate the superiority of OneVOS, achieving\nstate-of-the-art performance across 7 datasets, particularly excelling in\ncomplex LVOS and MOSE datasets with 70.1% and 66.4% $J \\& F$ scores, surpassing\nprevious state-of-the-art methods by 4.2% and 7.0%, respectively. And our code\nwill be available for reproducibility and further research.\n"
    },
    {
        "title": "HAIFIT: Human-Centered AI for Fashion Image Translation",
        "published_time": "2024-03-13T16:06:07Z",
        "abstract": "  In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications.\n"
    },
    {
        "title": "Data Augmentation in Human-Centric Vision",
        "published_time": "2024-03-13T16:05:18Z",
        "abstract": "  This survey presents a comprehensive analysis of data augmentation techniques\nin human-centric vision tasks, a first of its kind in the field. It delves into\na wide range of research areas including person ReID, human parsing, human pose\nestimation, and pedestrian detection, addressing the significant challenges\nposed by overfitting and limited training data in these domains. Our work\ncategorizes data augmentation methods into two main types: data generation and\ndata perturbation. Data generation covers techniques like graphic engine-based\ngeneration, generative model-based generation, and data recombination, while\ndata perturbation is divided into image-level and human-level perturbations.\nEach method is tailored to the unique requirements of human-centric tasks, with\nsome applicable across multiple areas. Our contributions include an extensive\nliterature review, providing deep insights into the influence of these\naugmentation techniques in human-centric vision and highlighting the nuances of\neach method. We also discuss open issues and future directions, such as the\nintegration of advanced generative models like Latent Diffusion Models, for\ncreating more realistic and diverse training data. This survey not only\nencapsulates the current state of data augmentation in human-centric vision but\nalso charts a course for future research, aiming to develop more robust,\naccurate, and efficient human-centric vision systems.\n"
    },
    {
        "title": "A Causal Inspired Early-Branching Structure for Domain Generalization",
        "published_time": "2024-03-13T16:04:29Z",
        "abstract": "  Learning domain-invariant semantic representations is crucial for achieving\ndomain generalization (DG), where a model is required to perform well on unseen\ntarget domains. One critical challenge is that standard training often results\nin entangled semantic and domain-specific features. Previous works suggest\nformulating the problem from a causal perspective and solving the entanglement\nproblem by enforcing marginal independence between the causal (\\ie semantic)\nand non-causal (\\ie domain-specific) features. Despite its simplicity, the\nbasic marginal independent-based idea alone may be insufficient to identify the\ncausal feature. By d-separation, we observe that the causal feature can be\nfurther characterized by being independent of the domain conditioned on the\nobject, and we propose the following two strategies as complements for the\nbasic framework.\n  First, the observation implicitly implies that for the same object, the\ncausal feature should not be associated with the non-causal feature, revealing\nthat the common practice of obtaining the two features with a shared base\nfeature extractor and two lightweight prediction heads might be inappropriate.\nTo meet the constraint, we propose a simple early-branching structure, where\nthe causal and non-causal feature obtaining branches share the first few blocks\nwhile diverging thereafter, for better structure design; Second, the\nobservation implies that the causal feature remains invariant across different\ndomains for the same object. To this end, we suggest that augmentation should\nbe incorporated into the framework to better characterize the causal feature,\nand we further suggest an effective random domain sampling scheme to fulfill\nthe task. Theoretical and experimental results show that the two strategies are\nbeneficial for the basic marginal independent-based framework. Code is\navailable at \\url{https://github.com/liangchen527/CausEB}.\n"
    },
    {
        "title": "Refractive COLMAP: Refractive Structure-from-Motion Revisited",
        "published_time": "2024-03-13T15:52:20Z",
        "abstract": "  In this paper, we present a complete refractive Structure-from-Motion (RSfM)\nframework for underwater 3D reconstruction using refractive camera setups (for\nboth, flat- and dome-port underwater housings). Despite notable achievements in\nrefractive multi-view geometry over the past decade, a robust, complete and\npublicly available solution for such tasks is not available at present, and\noften practical applications have to resort to approximating refraction effects\nby the intrinsic (distortion) parameters of a pinhole camera model. To fill\nthis gap, we have integrated refraction considerations throughout the entire\nSfM process within the state-of-the-art, open-source SfM framework COLMAP.\nNumerical simulations and reconstruction results on synthetically generated but\nphoto-realistic images with ground truth validate that enabling refraction does\nnot compromise accuracy or robustness as compared to in-air reconstructions.\nFinally, we demonstrate the capability of our approach for large-scale\nrefractive scenarios using a dataset consisting of nearly 6000 images. The\nimplementation is released as open-source at:\nhttps://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.\n"
    },
    {
        "title": "HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map\n  Construction",
        "published_time": "2024-03-13T15:51:23Z",
        "abstract": "  Vectorized High-Definition (HD) map construction requires predictions of the\ncategory and point coordinates of map elements (e.g. road boundary, lane\ndivider, pedestrian crossing, etc.). State-of-the-art methods are mainly based\non point-level representation learning for regressing accurate point\ncoordinates. However, this pipeline has limitations in obtaining element-level\ninformation and handling element-level failures, e.g. erroneous element shape\nor entanglement between elements. To tackle the above issues, we propose a\nsimple yet effective HybrId framework named HIMap to sufficiently learn and\ninteract both point-level and element-level information. Concretely, we\nintroduce a hybrid representation called HIQuery to represent all map elements,\nand propose a point-element interactor to interactively extract and encode the\nhybrid information of elements, e.g. point position and element shape, into the\nHIQuery. Additionally, we present a point-element consistency constraint to\nenhance the consistency between the point-level and element-level information.\nFinally, the output point-element integrated HIQuery can be directly converted\ninto map elements' class, point coordinates, and mask. We conduct extensive\nexperiments and consistently outperform previous methods on both nuScenes and\nArgoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes\ndataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.\n"
    },
    {
        "title": "A Decade's Battle on Dataset Bias: Are We There Yet?",
        "published_time": "2024-03-13T15:46:37Z",
        "abstract": "  We revisit the \"dataset classification\" experiment suggested by Torralba and\nEfros a decade ago, in the new era with large-scale, diverse, and hopefully\nless biased datasets as well as more capable neural network architectures.\nSurprisingly, we observe that modern neural networks can achieve excellent\naccuracy in classifying which dataset an image is from: e.g., we report 84.7%\naccuracy on held-out validation data for the three-way classification problem\nconsisting of the YFCC, CC, and DataComp datasets. Our further experiments show\nthat such a dataset classifier could learn semantic features that are\ngeneralizable and transferable, which cannot be simply explained by\nmemorization. We hope our discovery will inspire the community to rethink the\nissue involving dataset bias and model capabilities.\n"
    },
    {
        "title": "Scaling Up Dynamic Human-Scene Interaction Modeling",
        "published_time": "2024-03-13T15:45:04Z",
        "abstract": "  Confronting the challenges of data scarcity and advanced motion synthesis in\nhuman-scene interaction modeling, we introduce the TRUMANS dataset alongside a\nnovel HSI motion synthesis method. TRUMANS stands as the most comprehensive\nmotion-captured HSI dataset currently available, encompassing over 15 hours of\nhuman interactions across 100 indoor scenes. It intricately captures whole-body\nhuman motions and part-level object dynamics, focusing on the realism of\ncontact. This dataset is further scaled up by transforming physical\nenvironments into exact virtual models and applying extensive augmentations to\nappearance and motion for both humans and objects while maintaining interaction\nfidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model\nthat efficiently generates HSI sequences of any length, taking into account\nboth scene context and intended actions. In experiments, our approach shows\nremarkable zero-shot generalizability on a range of 3D scene datasets (e.g.,\nPROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic\noriginal motion-captured sequences, as confirmed by quantitative experiments\nand human studies.\n"
    },
    {
        "title": "ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning\n  in Instructional Videos",
        "published_time": "2024-03-13T14:54:04Z",
        "abstract": "  We present ActionDiffusion -- a novel diffusion model for procedure planning\nin instructional videos that is the first to take temporal inter-dependencies\nbetween actions into account in a diffusion model for procedure planning. This\napproach is in stark contrast to existing methods that fail to exploit the rich\ninformation content available in the particular order in which actions are\nperformed. Our method unifies the learning of temporal dependencies between\nactions and denoising of the action plan in the diffusion process by projecting\nthe action information into the noise space. This is achieved 1) by adding\naction embeddings in the noise masks in the noise-adding phase and 2) by\nintroducing an attention mechanism in the noise prediction network to learn the\ncorrelations between different action steps. We report extensive experiments on\nthree instructional video benchmark datasets (CrossTask, Coin, and NIV) and\nshow that our method outperforms previous state-of-the-art methods on all\nmetrics on CrossTask and NIV and all metrics except accuracy on Coin dataset.\nWe show that by adding action embeddings into the noise mask the diffusion\nmodel can better learn action temporal dependencies and increase the\nperformances on procedure planning.\n"
    },
    {
        "title": "PRAGO: Differentiable Multi-View Pose Optimization From Objectness\n  Detections",
        "published_time": "2024-03-13T14:42:55Z",
        "abstract": "  Robustly estimating camera poses from a set of images is a fundamental task\nwhich remains challenging for differentiable methods, especially in the case of\nsmall and sparse camera pose graphs. To overcome this challenge, we propose\nPose-refined Rotation Averaging Graph Optimization (PRAGO). From a set of\nobjectness detections on unordered images, our method reconstructs the\nrotational pose, and in turn, the absolute pose, in a differentiable manner\nbenefiting from the optimization of a sequence of geometrical tasks. We show\nhow our objectness pose-refinement module in PRAGO is able to refine the\ninherent ambiguities in pairwise relative pose estimation without removing\nedges and avoiding making early decisions on the viability of graph edges.\nPRAGO then refines the absolute rotations through iterative graph construction,\nreweighting the graph edges to compute the final rotational pose, which can be\nconverted into absolute poses using translation averaging. We show that PRAGO\nis able to outperform non-differentiable solvers on small and sparse scenes\nextracted from 7-Scenes achieving a relative improvement of 21% for rotations\nwhile achieving similar translation estimates.\n"
    },
    {
        "title": "Leveraging Compressed Frame Sizes For Ultra-Fast Video Classification",
        "published_time": "2024-03-13T14:35:13Z",
        "abstract": "  Classifying videos into distinct categories, such as Sport and Music Video,\nis crucial for multimedia understanding and retrieval, especially when an\nimmense volume of video content is being constantly generated. Traditional\nmethods require video decompression to extract pixel-level features like color,\ntexture, and motion, thereby increasing computational and storage demands.\nMoreover, these methods often suffer from performance degradation in\nlow-quality videos. We present a novel approach that examines only the\npost-compression bitstream of a video to perform classification, eliminating\nthe need for bitstream decoding. To validate our approach, we built a\ncomprehensive data set comprising over 29,000 YouTube video clips, totaling\n6,000 hours and spanning 11 distinct categories. Our evaluations indicate\nprecision, accuracy, and recall rates consistently above 80%, many exceeding\n90%, and some reaching 99%. The algorithm operates approximately 15,000 times\nfaster than real-time for 30fps videos, outperforming traditional Dynamic Time\nWarping (DTW) algorithm by seven orders of magnitude.\n"
    },
    {
        "title": "A Novel Implicit Neural Representation for Volume Data",
        "published_time": "2024-03-13T14:22:13Z",
        "abstract": "  The storage of medical images is one of the challenges in the medical imaging\nfield. There are variable works that use implicit neural representation (INR)\nto compress volumetric medical images. However, there is room to improve the\ncompression rate for volumetric medical images. Most of the INR techniques need\na huge amount of GPU memory and a long training time for high-quality medical\nvolume rendering. In this paper, we present a novel implicit neural\nrepresentation to compress volume data using our proposed architecture, that\nis, the Lanczos downsampling scheme, SIREN deep network, and SRDenseNet\nhigh-resolution scheme. Our architecture can effectively reduce training time,\nand gain a high compression rate while retaining the final rendering quality.\nMoreover, it can save GPU memory in comparison with the existing works. The\nexperiments show that the quality of reconstructed images and training speed\nusing our architecture is higher than current works which use the SIREN only.\nBesides, the GPU memory cost is evidently decreased\n"
    },
    {
        "title": "Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation",
        "published_time": "2024-02-29T07:26:23Z",
        "abstract": "  Although 3D shape matching and interpolation are highly interrelated, they\nare often studied separately and applied sequentially to relate different 3D\nshapes, thus resulting in sub-optimal performance. In this work we present a\nunified framework to predict both point-wise correspondences and shape\ninterpolation between 3D shapes. To this end, we combine the deep functional\nmap framework with classical surface deformation models to map shapes in both\nspectral and spatial domains. On the one hand, by incorporating spatial maps,\nour method obtains more accurate and smooth point-wise correspondences compared\nto previous functional map methods for shape matching. On the other hand, by\nintroducing spectral maps, our method gets rid of commonly used but\ncomputationally expensive geodesic distance constraints that are only valid for\nnear-isometric shape deformations. Furthermore, we propose a novel test-time\nadaptation scheme to capture both pose-dominant and shape-dominant\ndeformations. Using different challenging datasets, we demonstrate that our\nmethod outperforms previous state-of-the-art methods for both shape matching\nand interpolation, even compared to supervised approaches.\n"
    },
    {
        "title": "Occluded Cloth-Changing Person Re-Identification",
        "published_time": "2024-03-13T14:08:45Z",
        "abstract": "  Cloth-changing person re-identification aims to retrieve and identify\nspe-cific pedestrians by using cloth-irrelevant features in person\ncloth-changing scenarios. However, pedestrian images captured by surveillance\nprobes usually contain occlusions in real-world scenarios. The perfor-mance of\nexisting cloth-changing re-identification methods is significantly degraded due\nto the reduction of discriminative cloth-irrelevant features caused by\nocclusion. We define cloth-changing person re-identification in occlusion\nscenarios as occluded cloth-changing person re-identification (Occ-CC-ReID),\nand to the best of our knowledge, we are the first to pro-pose occluded\ncloth-changing person re-identification as a new task. We constructed two\noccluded cloth-changing person re-identification datasets for different\nocclusion scenarios: Occluded-PRCC and Occluded-LTCC. The datasets can be\nobtained from the following link:\nhttps://github.com/1024AILab/Occluded-Cloth-Changing-Person- Re-Identification.\n"
    },
    {
        "title": "SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple\n  Cameras and Scenes by One Model",
        "published_time": "2024-03-13T14:08:25Z",
        "abstract": "  The generalization of monocular metric depth estimation (MMDE) has been a\nlongstanding challenge. Recent methods made progress by combining relative and\nmetric depth or aligning input image focal length. However, they are still\nbeset by challenges in camera, scene, and data levels: (1) Sensitivity to\ndifferent cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on\nmassive training data. This paper proposes SM4Depth, a seamless MMDE method, to\naddress all the issues above within a single network. First, we reveal that a\nconsistent field of view (FOV) is the key to resolve ``metric ambiguity''\nacross cameras, which guides us to propose a more straightforward preprocessing\nunit. Second, to achieve consistently high accuracy across scenes, we\nexplicitly model the metric scale determination as discretizing the depth\ninterval into bins and propose variation-based unnormalized depth bins. This\nmethod bridges the depth gap of diverse scenes by reducing the ambiguity of the\nconventional metric bin. Third, to reduce the reliance on massive training\ndata, we propose a ``divide and conquer\" solution. Instead of estimating\ndirectly from the vast solution space, the correct metric bins are estimated\nfrom multiple solution sub-spaces for complexity reduction. Finally, with just\n150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves\nstate-of-the-art performance on most previously unseen datasets, especially\nsurpassing ZoeDepth and Metric3D on mRI$_\\theta$. The code can be found at\nhttps://github.com/1hao-Liu/SM4Depth.\n"
    },
    {
        "title": "CINA: Conditional Implicit Neural Atlas for Spatio-Temporal\n  Representation of Fetal Brains",
        "published_time": "2024-03-13T14:02:42Z",
        "abstract": "  We introduce a conditional implicit neural atlas (CINA) for spatio-temporal\natlas generation from Magnetic Resonance Images (MRI) of the neurotypical and\npathological fetal brain, that is fully independent of affine or non-rigid\nregistration. During training, CINA learns a general representation of the\nfetal brain and encodes subject specific information into latent code. After\ntraining, CINA can construct a faithful atlas with tissue probability maps of\nthe fetal brain for any gestational age (GA) and anatomical variation covered\nwithin the training domain. Thus, CINA is competent to represent both,\nneurotypical and pathological brains. Furthermore, a trained CINA model can be\nfit to brain MRI of unseen subjects via test-time optimization of the latent\ncode. CINA can then produce probabilistic tissue maps tailored to a particular\nsubject. We evaluate our method on a total of 198 T2 weighted MRI of normal and\nabnormal fetal brains from the dHCP and FeTA datasets. We demonstrate CINA's\ncapability to represent a fetal brain atlas that can be flexibly conditioned on\nGA and on anatomical variations like ventricular volume or degree of cortical\nfolding, making it a suitable tool for modeling both neurotypical and\npathological brains. We quantify the fidelity of our atlas by means of tissue\nsegmentation and age prediction and compare it to an established baseline. CINA\ndemonstrates superior accuracy for neurotypical brains and pathological brains\nwith ventriculomegaly. Moreover, CINA scores a mean absolute error of 0.23\nweeks in fetal brain age prediction, further confirming an accurate\nrepresentation of fetal brain development.\n"
    },
    {
        "title": "AIGCs Confuse AI Too: Investigating and Explaining Synthetic\n  Image-induced Hallucinations in Large Vision-Language Models",
        "published_time": "2024-03-13T13:56:34Z",
        "abstract": "  The evolution of Artificial Intelligence Generated Contents (AIGCs) is\nadvancing towards higher quality. The growing interactions with AIGCs present a\nnew challenge to the data-driven AI community: While AI-generated contents have\nplayed a crucial role in a wide range of AI models, the potential hidden risks\nthey introduce have not been thoroughly examined. Beyond human-oriented forgery\ndetection, AI-generated content poses potential issues for AI models originally\ndesigned to process natural data. In this study, we underscore the exacerbated\nhallucination phenomena in Large Vision-Language Models (LVLMs) caused by\nAI-synthetic images. Remarkably, our findings shed light on a consistent AIGC\n\\textbf{hallucination bias}: the object hallucinations induced by synthetic\nimages are characterized by a greater quantity and a more uniform position\ndistribution, even these synthetic images do not manifest unrealistic or\nadditional relevant visual features compared to natural images. Moreover, our\ninvestigations on Q-former and Linear projector reveal that synthetic images\nmay present token deviations after visual projection, thereby amplifying the\nhallucination bias.\n"
    },
    {
        "title": "HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional\n  Image Classifiers",
        "published_time": "2024-03-13T13:51:02Z",
        "abstract": "  Convolutional Neural Networks (CNNs) are nowadays the model of choice in\nComputer Vision, thanks to their ability to automatize the feature extraction\nprocess in visual tasks. However, the knowledge acquired during training is\nfully subsymbolic, and hence difficult to understand and explain to end users.\nIn this paper, we propose a new technique called HOLMES (HOLonym-MEronym based\nSemantic inspection) that decomposes a label into a set of related concepts,\nand provides component-level explanations for an image classification model.\nSpecifically, HOLMES leverages ontologies, web scraping and transfer learning\nto automatically construct meronym (parts)-based detectors for a given holonym\n(class). Then, it produces heatmaps at the meronym level and finally, by\nprobing the holonym CNN with occluded images, it highlights the importance of\neach part on the classification output. Compared to state-of-the-art saliency\nmethods, HOLMES takes a step further and provides information about both where\nand what the holonym CNN is looking at, without relying on densely annotated\ndatasets and without forcing concepts to be associated to single computational\nunits. Extensive experimental evaluation on different categories of objects\n(animals, tools and vehicles) shows the feasibility of our approach. On\naverage, HOLMES explanations include at least two meronyms, and the ablation of\na single meronym roughly halves the holonym model confidence. The resulting\nheatmaps were quantitatively evaluated using the\ndeletion/insertion/preservation curves. All metrics were comparable to those\nachieved by GradCAM, while offering the advantage of further decomposing the\nheatmap in human-understandable concepts, thus highlighting both the relevance\nof meronyms to object classification, as well as HOLMES ability to capture it.\nThe code is available at https://github.com/FrancesC0de/HOLMES.\n"
    },
    {
        "title": "Pig aggression classification using CNN, Transformers and Recurrent\n  Networks",
        "published_time": "2024-03-13T13:38:58Z",
        "abstract": "  The development of techniques that can be used to analyze and detect animal\nbehavior is a crucial activity for the livestock sector, as it is possible to\nmonitor the stress and animal welfare and contributes to decision making in the\nfarm. Thus, the development of applications can assist breeders in making\ndecisions to improve production performance and reduce costs, once the animal\nbehavior is analyzed by humans and this can lead to susceptible errors and time\nconsumption. Aggressiveness in pigs is an example of behavior that is studied\nto reduce its impact through animal classification and identification. However,\nthis process is laborious and susceptible to errors, which can be reduced\nthrough automation by visually classifying videos captured in controlled\nenvironment. The captured videos can be used for training and, as a result, for\nclassification through computer vision and artificial intelligence, employing\nneural network techniques. The main techniques utilized in this study are\nvariants of transformers: STAM, TimeSformer, and ViViT, as well as techniques\nusing convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm. These\ntechniques were employed for pig video classification with the objective of\nidentifying aggressive and non-aggressive behaviors. In this work, various\ntechniques were compared to analyze the contribution of using transformers, in\naddition to the effectiveness of the convolution technique in video\nclassification. The performance was evaluated using accuracy, precision, and\nrecall. The TimerSformer technique showed the best results in video\nclassification, with median accuracy of 0.729.\n"
    },
    {
        "title": "UniLiDAR: Bridge the domain gap among different LiDARs for continual\n  learning",
        "published_time": "2024-03-13T13:23:05Z",
        "abstract": "  LiDAR-based 3D perception algorithms have evolved rapidly alongside the\nemergence of large datasets. Nonetheless, considerable performance degradation\noften ensues when models trained on a specific dataset are applied to other\ndatasets or real-world scenarios with different LiDAR. This paper aims to\ndevelop a unified model capable of handling different LiDARs, enabling\ncontinual learning across diverse LiDAR datasets and seamless deployment across\nheterogeneous platforms. We observe that the gaps among datasets primarily\nmanifest in geometric disparities (such as variations in beams and point\ncounts) and semantic inconsistencies (taxonomy conflicts). To this end, this\npaper proposes UniLiDAR, an occupancy prediction pipeline that leverages\ngeometric realignment and semantic label mapping to facilitate multiple\ndatasets training and mitigate performance degradation during deployment on\nheterogeneous platforms. Moreover, our method can be easily combined with\nexisting 3D perception models. The efficacy of the proposed approach in\nbridging LiDAR domain gaps is verified by comprehensive experiments on two\nprominent datasets: OpenOccupancy-nuScenes and SemanticKITTI. UniLiDAR elevates\nthe mIoU of occupancy prediction by 15.7% and 12.5%, respectively, compared to\nthe model trained on the directly merged dataset. Moreover, it outperforms\nseveral SOTA methods trained on individual datasets. We expect our research to\nfacilitate further study of 3D generalization, the code will be available soon.\n"
    },
    {
        "title": "A Multimodal Fusion Network For Student Emotion Recognition Based on\n  Transformer and Tensor Product",
        "published_time": "2024-03-13T13:16:26Z",
        "abstract": "  In recent years, there have been frequent incidents of foreign objects\nintruding into railway and Airport runways. These objects can include\npedestrians, vehicles, animals, and debris. This paper introduces an improved\nYOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance\nthe detection of foreign objects on railways and Airport runways. This study\nproposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which\ncombines two public datasets for detecting foreign objects in aviation and\nrailway systems. The dataset aims to improve the recognition capabilities of\nforeign object targets. Experimental results on this large dataset have\ndemonstrated significant performance improvements of the proposed model over\nthe baseline YOLOv5 model, reducing computational requirements. improved YOLO\nmodel shows a significant improvement in precision by 1.2%, recall rate by\n1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters\nwere reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%.\nIn the ablation experiment, it is found that the FasterNet module can\nsignificantly reduce the number of parameters of the model, and the reference\nof the attention mechanism can slow down the performance loss caused by\nlightweight.\n"
    },
    {
        "title": "De-Confusing Pseudo-Labels in Source-Free Domain Adaptation",
        "published_time": "2024-01-03T10:07:11Z",
        "abstract": "  Source-free domain adaptation (SFDA) aims to adapt a source-trained model to\nan unlabeled target domain without access to the source data. SFDA has\nattracted growing attention in recent years, where existing approaches focus on\nself-training that usually includes pseudo-labeling techniques. In this paper,\nwe introduce a novel noise-learning approach tailored to address noise\ndistribution in domain adaptation settings and learn to de-confuse the\npseudo-labels. More specifically, we learn a noise transition matrix of the\npseudo-labels to capture the label corruption of each class and learn the\nunderlying true label distribution. Estimating the noise transition matrix\nenables a better true class-posterior estimation, resulting in better\nprediction accuracy. We demonstrate the effectiveness of our approach when\ncombined with several SFDA methods: SHOT, SHOT++, and AaD. We obtain\nstate-of-the-art results on three domain adaptation datasets: VisDA, DomainNet,\nand OfficeHome.\n"
    },
    {
        "title": "Content-aware Masked Image Modeling Transformer for Stereo Image\n  Compression",
        "published_time": "2024-03-13T13:12:57Z",
        "abstract": "  Existing learning-based stereo image codec adopt sophisticated transformation\nwith simple entropy models derived from single image codecs to encode latent\nrepresentations. However, those entropy models struggle to effectively capture\nthe spatial-disparity characteristics inherent in stereo images, which leads to\nsuboptimal rate-distortion results. In this paper, we propose a stereo image\ncompression framework, named CAMSIC. CAMSIC independently transforms each image\nto latent representation and employs a powerful decoder-free Transformer\nentropy model to capture both spatial and disparity dependencies, by\nintroducing a novel content-aware masked image modeling (MIM) technique. Our\ncontent-aware MIM facilitates efficient bidirectional interaction between prior\ninformation and estimated tokens, which naturally obviates the need for an\nextra Transformer decoder. Experiments show that our stereo image codec\nachieves state-of-the-art rate-distortion performance on two stereo image\ndatasets Cityscapes and InStereo2K with fast encoding and decoding speed.\n"
    },
    {
        "title": "OccFiner: Offboard Occupancy Refinement with Hybrid Propagation",
        "published_time": "2024-03-13T13:12:42Z",
        "abstract": "  Vision-based occupancy prediction, also known as 3D Semantic Scene Completion\n(SSC), presents a significant challenge in computer vision. Previous methods,\nconfined to onboard processing, struggle with simultaneous geometric and\nsemantic estimation, continuity across varying viewpoints, and single-view\nocclusion. Our paper introduces OccFiner, a novel offboard framework designed\nto enhance the accuracy of vision-based occupancy predictions. OccFiner\noperates in two hybrid phases: 1) a multi-to-multi local propagation network\nthat implicitly aligns and processes multiple local frames for correcting\nonboard model errors and consistently enhancing occupancy accuracy across all\ndistances. 2) the region-centric global propagation, focuses on refining labels\nusing explicit multi-view geometry and integrating sensor bias, especially to\nincrease the accuracy of distant occupied voxels. Extensive experiments\ndemonstrate that OccFiner improves both geometric and semantic accuracy across\nvarious types of coarse occupancy, setting a new state-of-the-art performance\non the SemanticKITTI dataset. Notably, OccFiner elevates vision-based SSC\nmodels to a level even surpassing that of LiDAR-based onboard SSC models.\n"
    },
    {
        "title": "Masked Generative Story Transformer with Character Guidance and Caption\n  Augmentation",
        "published_time": "2024-03-13T13:10:20Z",
        "abstract": "  Story Visualization (SV) is a challenging generative vision task, that\nrequires both visual quality and consistency between different frames in\ngenerated image sequences. Previous approaches either employ some kind of\nmemory mechanism to maintain context throughout an auto-regressive generation\nof the image sequence, or model the generation of the characters and their\nbackground separately, to improve the rendering of characters. On the contrary,\nwe embrace a completely parallel transformer-based approach, exclusively\nrelying on Cross-Attention with past and future captions to achieve\nconsistency. Additionally, we propose a Character Guidance technique to focus\non the generation of characters in an implicit manner, by forming a combination\nof text-conditional and character-conditional logits in the logit space. We\nalso employ a caption-augmentation technique, carried out by a Large Language\nModel (LLM), to enhance the robustness of our approach. The combination of\nthese methods culminates into state-of-the-art (SOTA) results over various\nmetrics in the most prominent SV benchmark (Pororo-SV), attained with\nconstraint resources while achieving superior computational complexity compared\nto previous arts. The validity of our quantitative results is supported by a\nhuman survey.\n"
    },
    {
        "title": "Improved YOLOv5 Based on Attention Mechanism and FasterNet for Foreign\n  Object Detection on Railway and Airway tracks",
        "published_time": "2024-03-13T13:07:14Z",
        "abstract": "  In recent years, there have been frequent incidents of foreign objects\nintruding into railway and Airport runways. These objects can include\npedestrians, vehicles, animals, and debris. This paper introduces an improved\nYOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance\nthe detection of foreign objects on railways and Airport runways. This study\nproposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which\ncombines two public datasets for detecting foreign objects in aviation and\nrailway systems.The dataset aims to improve the recognition capabilities of\nforeign object targets. Experimental results on this large dataset have\ndemonstrated significant performance improvements of the proposed model over\nthe baseline YOLOv5 model, reducing computational requirements.Improved YOLO\nmodel shows a significant improvement in precision by 1.2%, recall rate by\n1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters\nwere reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%.\nIn the ablation experiment, it is found that the FasterNet module can\nsignificantly reduce the number of parameters of the model, and the reference\nof the attention mechanism can slow down the performance loss caused by\nlightweight.\n"
    },
    {
        "title": "Gaussian Splatting in Style",
        "published_time": "2024-03-13T13:06:31Z",
        "abstract": "  Scene stylization extends the work of neural style transfer to three spatial\ndimensions. A vital challenge in this problem is to maintain the uniformity of\nthe stylized appearance across a multi-view setting. A vast majority of the\nprevious works achieve this by optimizing the scene with a specific style\nimage. In contrast, we propose a novel architecture trained on a collection of\nstyle images, that at test time produces high quality stylized novel views. Our\nwork builds up on the framework of 3D Gaussian splatting. For a given scene, we\ntake the pretrained Gaussians and process them using a multi resolution hash\ngrid and a tiny MLP to obtain the conditional stylised views. The explicit\nnature of 3D Gaussians give us inherent advantages over NeRF-based methods\nincluding geometric consistency, along with having a fast training and\nrendering regime. This enables our method to be useful for vast practical use\ncases such as in augmented or virtual reality applications. Through our\nexperiments, we show our methods achieve state-of-the-art performance with\nsuperior visual quality on various indoor and outdoor real-world data.\n"
    },
    {
        "title": "Model Will Tell: Training Membership Inference for Diffusion Models",
        "published_time": "2024-03-13T12:52:37Z",
        "abstract": "  Diffusion models pose risks of privacy breaches and copyright disputes,\nprimarily stemming from the potential utilization of unauthorized data during\nthe training phase. The Training Membership Inference (TMI) task aims to\ndetermine whether a specific sample has been used in the training process of a\ntarget model, representing a critical tool for privacy violation verification.\nHowever, the increased stochasticity inherent in diffusion renders traditional\nshadow-model-based or metric-based methods ineffective when applied to\ndiffusion models. Moreover, existing methods only yield binary classification\nlabels which lack necessary comprehensibility in practical applications. In\nthis paper, we explore a novel perspective for the TMI task by leveraging the\nintrinsic generative priors within the diffusion model. Compared with unseen\nsamples, training samples exhibit stronger generative priors within the\ndiffusion model, enabling the successful reconstruction of substantially\ndegraded training images. Consequently, we propose the Degrade Restore Compare\n(DRC) framework. In this framework, an image undergoes sequential degradation\nand restoration, and its membership is determined by comparing it with the\nrestored counterpart. Experimental results verify that our approach not only\nsignificantly outperforms existing methods in terms of accuracy but also\nprovides comprehensible decision criteria, offering evidence for potential\nprivacy violations.\n"
    },
    {
        "title": "RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein\n  Segmentation and Classification",
        "published_time": "2024-02-05T16:35:29Z",
        "abstract": "  The caliber and configuration of retinal blood vessels serve as important\nbiomarkers for various diseases and medical conditions. A thorough analysis of\nthe retinal vasculature requires the segmentation of the blood vessels and\ntheir classification into arteries and veins, typically performed on color\nfundus images obtained by retinography. However, manually performing these\ntasks is labor-intensive and prone to human error. While several automated\nmethods have been proposed to address this task, the current state of art faces\nchallenges due to manifest classification errors affecting the topological\nconsistency of segmentation maps. In this work, we introduce RRWNet, a novel\nend-to-end deep learning framework that addresses this limitation. The\nframework consists of a fully convolutional neural network that recursively\nrefines semantic segmentation maps, correcting manifest classification errors\nand thus improving topological consistency. In particular, RRWNet is composed\nof two specialized subnetworks: a Base subnetwork that generates base\nsegmentation maps from the input images, and a Recursive Refinement subnetwork\nthat iteratively and recursively improves these maps. Evaluation on three\ndifferent public datasets demonstrates the state-of-the-art performance of the\nproposed method, yielding more topologically consistent segmentation maps with\nfewer manifest classification errors than existing approaches. In addition, the\nRecursive Refinement module within RRWNet proves effective in post-processing\nsegmentation maps from other methods, further demonstrating its potential. The\nmodel code, weights, and predictions will be publicly available at\nhttps://github.com/j-morano/rrwnet.\n"
    },
    {
        "title": "MD-Dose: A Diffusion Model based on the Mamba for Radiotherapy Dose\n  Prediction",
        "published_time": "2024-03-13T12:46:36Z",
        "abstract": "  Radiation therapy is crucial in cancer treatment. Experienced experts\ntypically iteratively generate high-quality dose distribution maps, forming the\nbasis for excellent radiation therapy plans. Therefore, automated prediction of\ndose distribution maps is significant in expediting the treatment process and\nproviding a better starting point for developing radiation therapy plans. With\nthe remarkable results of diffusion models in predicting high-frequency regions\nof dose distribution maps, dose prediction methods based on diffusion models\nhave been extensively studied. However, existing methods mainly utilize CNNs or\nTransformers as denoising networks. CNNs lack the capture of global receptive\nfields, resulting in suboptimal prediction performance. Transformers excel in\nglobal modeling but face quadratic complexity with image size, resulting in\nsignificant computational overhead. To tackle these challenges, we introduce a\nnovel diffusion model, MD-Dose, based on the Mamba architecture for predicting\nradiation therapy dose distribution in thoracic cancer patients. In the forward\nprocess, MD-Dose adds Gaussian noise to dose distribution maps to obtain pure\nnoise images. In the backward process, MD-Dose utilizes a noise predictor based\non the Mamba to predict the noise, ultimately outputting the dose distribution\nmaps. Furthermore, We develop a Mamba encoder to extract structural information\nand integrate it into the noise predictor for localizing dose regions in the\nplanning target volume (PTV) and organs at risk (OARs). Through extensive\nexperiments on a dataset of 300 thoracic tumor patients, we showcase the\nsuperiority of MD-Dose in various metrics and time consumption.\n"
    },
    {
        "title": "Unleashing the Power of Meta-tuning for Few-shot Generalization Through\n  Sparse Interpolated Experts",
        "published_time": "2024-03-13T12:46:03Z",
        "abstract": "  Conventional wisdom suggests parameter-efficient fine-tuning of foundation\nmodels as the state-of-the-art method for transfer learning in vision,\nreplacing the rich literature of alternatives such as meta-learning. In trying\nto harness the best of both worlds, meta-tuning introduces a subsequent\noptimization stage of foundation models but has so far only shown limited\nsuccess and crucially tends to underperform on out-of-domain (OOD) tasks. In\nthis paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse\nmixture-of-experts approaches and trained to isolate subsets of pre-trained\nparameters automatically for meta-tuning on each task. SMAT successfully\novercomes OOD sensitivity and delivers on the promise of enhancing the transfer\nabilities of vision foundation models beyond parameter-efficient finetuning. We\nestablish new state-of-the-art results on a challenging combination of\nMeta-Dataset augmented with additional OOD tasks in both zero-shot and\ngradient-based adaptation settings. In addition, we provide a thorough analysis\nof the superiority of learned over hand-designed sparsity patterns for sparse\nexpert methods and the pivotal importance of the sparsity level in balancing\nbetween in-domain and out-of-domain generalization. Our code is publicly\navailable.\n"
    },
    {
        "title": "DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with\n  Global-Local Depth Normalization",
        "published_time": "2024-03-11T17:02:11Z",
        "abstract": "  Radiance fields have demonstrated impressive performance in synthesizing\nnovel views from sparse input views, yet prevailing methods suffer from high\ntraining costs and slow inference speed. This paper introduces DNGaussian, a\ndepth-regularized framework based on 3D Gaussian radiance fields, offering\nreal-time and high-quality few-shot novel view synthesis at low costs. Our\nmotivation stems from the highly efficient representation and surprising\nquality of the recent 3D Gaussian Splatting, despite it will encounter a\ngeometry degradation when input views decrease. In the Gaussian radiance\nfields, we find this degradation in scene geometry primarily lined to the\npositioning of Gaussian primitives and can be mitigated by depth constraint.\nConsequently, we propose a Hard and Soft Depth Regularization to restore\naccurate scene geometry under coarse monocular depth supervision while\nmaintaining a fine-grained color appearance. To further refine detailed\ngeometry reshaping, we introduce Global-Local Depth Normalization, enhancing\nthe focus on small local depth changes. Extensive experiments on LLFF, DTU, and\nBlender datasets demonstrate that DNGaussian outperforms state-of-the-art\nmethods, achieving comparable or better results with significantly reduced\nmemory cost, a $25 \\times$ reduction in training time, and over $3000 \\times$\nfaster rendering speed.\n"
    },
    {
        "title": "NoiseDiffusion: Correcting Noise for Image Interpolation with Diffusion\n  Models beyond Spherical Linear Interpolation",
        "published_time": "2024-03-13T12:32:25Z",
        "abstract": "  Image interpolation based on diffusion models is promising in creating fresh\nand interesting images. Advanced interpolation methods mainly focus on\nspherical linear interpolation, where images are encoded into the noise space\nand then interpolated for denoising to images. However, existing methods face\nchallenges in effectively interpolating natural images (not generated by\ndiffusion models), thereby restricting their practical applicability. Our\nexperimental investigations reveal that these challenges stem from the\ninvalidity of the encoding noise, which may no longer obey the expected noise\ndistribution, e.g., a normal distribution. To address these challenges, we\npropose a novel approach to correct noise for image interpolation,\nNoiseDiffusion. Specifically, NoiseDiffusion approaches the invalid noise to\nthe expected distribution by introducing subtle Gaussian noise and introduces a\nconstraint to suppress noise with extreme values. In this context, promoting\nnoise validity contributes to mitigating image artifacts, but the constraint\nand introduced exogenous noise typically lead to a reduction in signal-to-noise\nratio, i.e., loss of original image information. Hence, NoiseDiffusion performs\ninterpolation within the noisy image space and injects raw images into these\nnoisy counterparts to address the challenge of information loss. Consequently,\nNoiseDiffusion enables us to interpolate natural images without causing\nartifacts or information loss, thus achieving the best interpolation results.\n"
    },
    {
        "title": "Diffusion Models with Implicit Guidance for Medical Anomaly Detection",
        "published_time": "2024-03-13T12:26:55Z",
        "abstract": "  Diffusion models have advanced unsupervised anomaly detection by improving\nthe transformation of pathological images into pseudo-healthy equivalents.\nNonetheless, standard approaches may compromise critical information during\npathology removal, leading to restorations that do not align with unaffected\nregions in the original scans. Such discrepancies can inadvertently increase\nfalse positive rates and reduce specificity, complicating radiological\nevaluations. This paper introduces Temporal Harmonization for Optimal\nRestoration (THOR), which refines the de-noising process by integrating\nimplicit guidance through temporal anomaly maps. THOR aims to preserve the\nintegrity of healthy tissue in areas unaffected by pathology. Comparative\nevaluations show that THOR surpasses existing diffusion-based methods in\ndetecting and segmenting anomalies in brain MRIs and wrist X-rays. Code:\nhttps://github.com/ci-ber/THOR_DDPM.\n"
    },
    {
        "title": "Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal\n  Diffusion Model",
        "published_time": "2024-03-13T12:20:20Z",
        "abstract": "  Millimeter wave (mmWave) radars have attracted significant attention from\nboth academia and industry due to their capability to operate in extreme\nweather conditions. However, they face challenges in terms of sparsity and\nnoise interference, which hinder their application in the field of micro aerial\nvehicle (MAV) autonomous navigation. To this end, this paper proposes a novel\napproach to dense and accurate mmWave radar point cloud construction via\ncross-modal learning. Specifically, we introduce diffusion models, which\npossess state-of-the-art performance in generative modeling, to predict\nLiDAR-like point clouds from paired raw radar data. We also incorporate the\nmost recent diffusion model inference accelerating techniques to ensure that\nthe proposed method can be implemented on MAVs with limited computing\nresources.We validate the proposed method through extensive benchmark\ncomparisons and real-world experiments, demonstrating its superior performance\nand generalization ability. Code and pretrained models will be available at\nhttps://github.com/ZJU-FAST-Lab/Radar-Diffusion.\n"
    },
    {
        "title": "Better Fit: Accommodate Variations in Clothing Types for Virtual Try-on",
        "published_time": "2024-03-13T12:07:14Z",
        "abstract": "  Image-based virtual try-on aims to transfer target in-shop clothing to a\ndressed model image, the objectives of which are totally taking off original\nclothing while preserving the contents outside of the try-on area, naturally\nwearing target clothing and correctly inpainting the gap between target\nclothing and original clothing. Tremendous efforts have been made to facilitate\nthis popular research area, but cannot keep the type of target clothing with\nthe try-on area affected by original clothing. In this paper, we focus on the\nunpaired virtual try-on situation where target clothing and original clothing\non the model are different, i.e., the practical scenario. To break the\ncorrelation between the try-on area and the original clothing and make the\nmodel learn the correct information to inpaint, we propose an adaptive mask\ntraining paradigm that dynamically adjusts training masks. It not only improves\nthe alignment and fit of clothing but also significantly enhances the fidelity\nof virtual try-on experience. Furthermore, we for the first time propose two\nmetrics for unpaired try-on evaluation, the Semantic-Densepose-Ratio (SDR) and\nSkeleton-LPIPS (S-LPIPS), to evaluate the correctness of clothing type and the\naccuracy of clothing texture. For unpaired try-on validation, we construct a\ncomprehensive cross-try-on benchmark (Cross-27) with distinctive clothing items\nand model physiques, covering a broad try-on scenarios. Experiments demonstrate\nthe effectiveness of the proposed methods, contributing to the advancement of\nvirtual try-on technology and offering new insights and tools for future\nresearch in the field. The code, model and benchmark will be publicly released.\n"
    },
    {
        "title": "PFStorer: Personalized Face Restoration and Super-Resolution",
        "published_time": "2024-03-13T11:39:30Z",
        "abstract": "  Recent developments in face restoration have achieved remarkable results in\nproducing high-quality and lifelike outputs. The stunning results however often\nfail to be faithful with respect to the identity of the person as the models\nlack necessary context. In this paper, we explore the potential of personalized\nface restoration with diffusion models. In our approach a restoration model is\npersonalized using a few images of the identity, leading to tailored\nrestoration with respect to the identity while retaining fine-grained details.\nBy using independent trainable blocks for personalization, the rich prior of a\nbase restoration model can be exploited to its fullest. To avoid the model\nrelying on parts of identity left in the conditioning low-quality images, a\ngenerative regularizer is employed. With a learnable parameter, the model\nlearns to balance between the details generated based on the input image and\nthe degree of personalization. Moreover, we improve the training pipeline of\nface restoration models to enable an alignment-free approach. We showcase the\nrobust capabilities of our approach in several real-world scenarios with\nmultiple identities, demonstrating our method's ability to generate\nfine-grained details with faithful restoration. In the user study we evaluate\nthe perceptual quality and faithfulness of the genereated details, with our\nmethod being voted best 61% of the time compared to the second best with 25% of\nthe votes.\n"
    },
    {
        "title": "CSCNET: Class-Specified Cascaded Network for Compositional Zero-Shot\n  Learning",
        "published_time": "2024-03-09T14:18:41Z",
        "abstract": "  Attribute and object (A-O) disentanglement is a fundamental and critical\nproblem for Compositional Zero-shot Learning (CZSL), whose aim is to recognize\nnovel A-O compositions based on foregone knowledge. Existing methods based on\ndisentangled representation learning lose sight of the contextual dependency\nbetween the A-O primitive pairs. Inspired by this, we propose a novel A-O\ndisentangled framework for CZSL, namely Class-specified Cascaded Network\n(CSCNet). The key insight is to firstly classify one primitive and then\nspecifies the predicted class as a priori for guiding another primitive\nrecognition in a cascaded fashion. To this end, CSCNet constructs\nAttribute-to-Object and Object-to-Attribute cascaded branches, in addition to a\ncomposition branch modeling the two primitives as a whole. Notably, we devise a\nparametric classifier (ParamCls) to improve the matching between visual and\nsemantic embeddings. By improving the A-O disentanglement, our framework\nachieves superior results than previous competitive methods.\n"
    },
    {
        "title": "An Empirical Study of Parameter Efficient Fine-tuning on Vision-Language\n  Pre-train Model",
        "published_time": "2024-03-13T11:33:38Z",
        "abstract": "  Recent studies applied Parameter Efficient Fine-Tuning techniques (PEFTs) to\nefficiently narrow the performance gap between pre-training and downstream.\nThere are two important factors for various PEFTs, namely, the accessible data\nsize and fine-tunable parameter size. A natural expectation for PEFTs is that\nthe performance of various PEFTs is positively related to the data size and\nfine-tunable parameter size. However, according to the evaluation of five PEFTs\non two downstream vision-language (VL) tasks, we find that such an intuition\nholds only if the downstream data and task are not consistent with\npre-training. For downstream fine-tuning consistent with pre-training, data\nsize no longer affects the performance, while the influence of fine-tunable\nparameter size is not monotonous. We believe such an observation could guide\nthe choice of training strategy for various PEFTs.\n"
    },
    {
        "title": "Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation",
        "published_time": "2024-03-13T11:23:55Z",
        "abstract": "  The pre-trained vision-language model, exemplified by CLIP, advances\nzero-shot semantic segmentation by aligning visual features with class\nembeddings through a transformer decoder to generate semantic masks. Despite\nits effectiveness, prevailing methods within this paradigm encounter\nchallenges, including overfitting on seen classes and small fragmentation in\nmasks. To mitigate these issues, we propose a Language-Driven Visual Consensus\n(LDVC) approach, fostering improved alignment of semantic and visual\ninformation.Specifically, we leverage class embeddings as anchors due to their\ndiscrete and abstract nature, steering vision features toward class embeddings.\nMoreover, to circumvent noisy alignments from the vision part due to its\nredundant nature, we introduce route attention into self-attention for finding\nvisual consensus, thereby enhancing semantic consistency within the same\nobject. Equipped with a vision-language prompting strategy, our approach\nsignificantly boosts the generalization capacity of segmentation models for\nunseen classes. Experimental results underscore the effectiveness of our\napproach, showcasing mIoU gains of 4.5 on the PASCAL VOC 2012 and 3.6 on the\nCOCO-Stuff 164k for unseen classes compared with the state-of-the-art methods.\n"
    },
    {
        "title": "Equipping Computational Pathology Systems with Artifact Processing\n  Pipelines: A Showcase for Computation and Performance Trade-offs",
        "published_time": "2024-03-12T15:22:05Z",
        "abstract": "  Histopathology is a gold standard for cancer diagnosis under a microscopic\nexamination. However, histological tissue processing procedures result in\nartifacts, which are ultimately transferred to the digitized version of glass\nslides, known as whole slide images (WSIs). Artifacts are diagnostically\nirrelevant areas and may result in wrong deep learning (DL) algorithms\npredictions. Therefore, detecting and excluding artifacts in the computational\npathology (CPATH) system is essential for reliable automated diagnosis. In this\npaper, we propose a mixture of experts (MoE) scheme for detecting five notable\nartifacts, including damaged tissue, blur, folded tissue, air bubbles, and\nhistologically irrelevant blood from WSIs. First, we train independent binary\nDL models as experts to capture particular artifact morphology. Then, we\nensemble their predictions using a fusion mechanism. We apply probabilistic\nthresholding over the final probability distribution to improve the sensitivity\nof the MoE. We developed DL pipelines using two MoEs and two multiclass models\nof state-of-the-art deep convolutional neural networks (DCNNs) and vision\ntransformers (ViTs). DCNNs-based MoE and ViTs-based MoE schemes outperformed\nsimpler multiclass models and were tested on datasets from different hospitals\nand cancer types, where MoE using DCNNs yielded the best results. The proposed\nMoE yields 86.15% F1 and 97.93% sensitivity scores on unseen data, retaining\nless computational cost for inference than MoE using ViTs. This best\nperformance of MoEs comes with relatively higher computational trade-offs than\nmulticlass models. The proposed artifact detection pipeline will not only\nensure reliable CPATH predictions but may also provide quality control.\n"
    },
    {
        "title": "Low-Cost and Real-Time Industrial Human Action Recognitions Based on\n  Large-Scale Foundation Models",
        "published_time": "2024-03-13T11:11:59Z",
        "abstract": "  Industrial managements, including quality control, cost and safety\noptimization, etc., heavily rely on high quality industrial human action\nrecognitions (IHARs) which were hard to be implemented in large-scale\nindustrial scenes due to their high costs and poor real-time performance. In\nthis paper, we proposed a large-scale foundation model(LSFM)-based IHAR method,\nwherein various LSFMs and lightweight methods were jointly used, for the first\ntime, to fulfill low-cost dataset establishment and real-time IHARs.\nComprehensive tests on in-situ large-scale industrial manufacturing lines\nelucidated that the proposed method realized great reduction on employment\ncosts, superior real-time performance, and satisfactory accuracy and\ngeneralization capabilities, indicating its great potential as a backbone IHAR\nmethod, especially for large-scale industrial applications.\n"
    },
    {
        "title": "Generalizable Two-Branch Framework for Image Class-Incremental Learning",
        "published_time": "2024-02-28T06:18:33Z",
        "abstract": "  Deep neural networks often severely forget previously learned knowledge when\nlearning new knowledge. Various continual learning (CL) methods have been\nproposed to handle such a catastrophic forgetting issue from different\nperspectives and achieved substantial improvements. In this paper, a novel\ntwo-branch continual learning framework is proposed to further enhance most\nexisting CL methods. Specifically, the main branch can be any existing CL model\nand the newly introduced side branch is a lightweight convolutional network.\nThe output of each main branch block is modulated by the output of the\ncorresponding side branch block. Such a simple two-branch model can then be\neasily implemented and learned with the vanilla optimization setting without\nwhistles and bells. Extensive experiments with various settings on multiple\nimage datasets show that the proposed framework yields consistent improvements\nover state-of-the-art methods.\n"
    },
    {
        "title": "The Development and Performance of a Machine Learning Based Mobile\n  Platform for Visually Determining the Etiology of Penile Pathology",
        "published_time": "2024-03-13T11:05:40Z",
        "abstract": "  Machine-learning algorithms can facilitate low-cost, user-guided visual\ndiagnostic platforms for addressing disparities in access to sexual health\nservices. We developed a clinical image dataset using original and augmented\nimages for five penile diseases: herpes eruption, syphilitic chancres, penile\ncandidiasis, penile cancer, and genital warts. We used a U-net architecture\nmodel for semantic pixel segmentation into background or subject image, the\nInception-ResNet version 2 neural architecture to classify each pixel as\ndiseased or non-diseased, and a salience map using GradCAM++. We trained the\nmodel on a random 91% sample of the image database using 150 epochs per image,\nand evaluated the model on the remaining 9% of images, assessing recall (or\nsensitivity), precision, specificity, and F1-score (accuracy). Of the 239\nimages in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%)\nwere of HSV infection, 29 (12.1%) were of penile cancer, 40 (16.7%) were of\npenile candidiasis, 37 (15.5%) were of syphilitic chancres, and 45 (18.8%) were\nof non-diseased penises. The overall accuracy of the model for correctly\nclassifying the diseased image was 0.944. Between July 1st and October 1st\n2023, there were 2,640 unique users of the mobile platform. Among a random\nsample of submissions (n=437), 271 (62.0%) were from the United States, 64\n(14.6%) from Singapore, 41 (9.4%) from Candia, 40 (9.2%) from the United\nKingdom, and 21 (4.8%) from Vietnam. The majority (n=277 [63.4%]) were between\n18 and 30 years old. We report on the development of a machine-learning model\nfor classifying five penile diseases, which demonstrated excellent performance\non a validation dataset. That model is currently in use globally and has the\npotential to improve access to diagnostic services for penile diseases.\n"
    },
    {
        "title": "Iterative Online Image Synthesis via Diffusion Model for Imbalanced\n  Classification",
        "published_time": "2024-03-13T10:51:18Z",
        "abstract": "  Accurate and robust classification of diseases is important for proper\ndiagnosis and treatment. However, medical datasets often face challenges\nrelated to limited sample sizes and inherent imbalanced distributions, due to\ndifficulties in data collection and variations in disease prevalence across\ndifferent types. In this paper, we introduce an Iterative Online Image\nSynthesis (IOIS) framework to address the class imbalance problem in medical\nimage classification. Our framework incorporates two key modules, namely Online\nImage Synthesis (OIS) and Accuracy Adaptive Sampling (AAS), which collectively\ntarget the imbalance classification issue at both the instance level and the\nclass level. The OIS module alleviates the data insufficiency problem by\ngenerating representative samples tailored for online training of the\nclassifier. On the other hand, the AAS module dynamically balances the\nsynthesized samples among various classes, targeting those with low training\naccuracy. To evaluate the effectiveness of our proposed method in addressing\nimbalanced classification, we conduct experiments on the HAM10000 and APTOS\ndatasets. The results obtained demonstrate the superiority of our approach over\nstate-of-the-art methods as well as the effectiveness of each component. The\nsource code will be released upon acceptance.\n"
    },
    {
        "title": "AADNet: Attention aware Demoir\u00e9ing Network",
        "published_time": "2024-03-13T09:48:11Z",
        "abstract": "  Moire pattern frequently appears in photographs captured with mobile devices\nand digital cameras, potentially degrading image quality. Despite recent\nadvancements in computer vision, image demoire'ing remains a challenging task\ndue to the dynamic textures and variations in colour, shape, and frequency of\nmoire patterns. Most existing methods struggle to generalize to unseen\ndatasets, limiting their effectiveness in removing moire patterns from\nreal-world scenarios. In this paper, we propose a novel lightweight\narchitecture, AADNet (Attention Aware Demoireing Network), for high-resolution\nimage demoire'ing that effectively works across different frequency bands and\ngeneralizes well to unseen datasets. Extensive experiments conducted on the\nUHDM dataset validate the effectiveness of our approach, resulting in\nhigh-fidelity images.\n"
    },
    {
        "title": "RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion\n  Attack in Federated Learning",
        "published_time": "2024-03-13T09:48:04Z",
        "abstract": "  Federated learning (FL) empowers privacy-preservation in model training by\nonly exposing users' model gradients. Yet, FL users are susceptible to the\ngradient inversion (GI) attack which can reconstruct ground-truth training data\nsuch as images based on model gradients. However, reconstructing\nhigh-resolution images by existing GI attack works faces two challenges:\ninferior accuracy and slow-convergence, especially when the context is\ncomplicated, e.g., the training batch size is much greater than 1 on each FL\nuser. To address these challenges, we present a Robust, Accurate and\nFast-convergent GI attack algorithm, called RAF-GI, with two components: 1)\nAdditional Convolution Block (ACB) which can restore labels with up to 20%\nimprovement compared with existing works; 2) Total variance, three-channel mEan\nand cAnny edge detection regularization term (TEA), which is a white-box attack\nstrategy to reconstruct images based on labels inferred by ACB. Moreover,\nRAF-GI is robust that can still accurately reconstruct ground-truth data when\nthe users' training batch size is no more than 48. Our experimental results\nmanifest that RAF-GI can diminish 94% time costs while achieving superb\ninversion quality in ImageNet dataset. Notably, with a batch size of 1, RAF-GI\nexhibits a 7.89 higher Peak Signal-to-Noise Ratio (PSNR) compared to the\nstate-of-the-art baselines.\n"
    },
    {
        "title": "Tackling the Singularities at the Endpoints of Time Intervals in\n  Diffusion Models",
        "published_time": "2024-03-13T09:47:04Z",
        "abstract": "  Most diffusion models assume that the reverse process adheres to a Gaussian\ndistribution. However, this approximation has not been rigorously validated,\nespecially at singularities, where t=0 and t=1. Improperly dealing with such\nsingularities leads to an average brightness issue in applications, and limits\nthe generation of images with extreme brightness or darkness. We primarily\nfocus on tackling singularities from both theoretical and practical\nperspectives. Initially, we establish the error bounds for the reverse process\napproximation, and showcase its Gaussian characteristics at singularity time\nsteps. Based on this theoretical insight, we confirm the singularity at t=1 is\nconditionally removable while it at t=0 is an inherent property. Upon these\nsignificant conclusions, we propose a novel plug-and-play method SingDiffusion\nto address the initial singular time step sampling, which not only effectively\nresolves the average brightness issue for a wide range of diffusion models\nwithout extra training efforts, but also enhances their generation capability\nin achieving notable lower FID scores. Code and models are released at\nhttps://github.com/PangzeCheung/SingDiffusion.\n"
    },
    {
        "title": "Mitigate Target-level Insensitivity of Infrared Small Target Detection\n  via Posterior Distribution Modeling",
        "published_time": "2024-03-13T09:45:30Z",
        "abstract": "  Infrared Small Target Detection (IRSTD) aims to segment small targets from\ninfrared clutter background. Existing methods mainly focus on discriminative\napproaches, i.e., a pixel-level front-background binary segmentation. Since\ninfrared small targets are small and low signal-to-clutter ratio, empirical\nrisk has few disturbances when a certain false alarm and missed detection\nexist, which seriously affect the further improvement of such methods.\nMotivated by the dense prediction generative methods, in this paper, we propose\na diffusion model framework for Infrared Small Target Detection which\ncompensates pixel-level discriminant with mask posterior distribution modeling.\nFurthermore, we design a Low-frequency Isolation in the wavelet domain to\nsuppress the interference of intrinsic infrared noise on the diffusion noise\nestimation. This transition from the discriminative paradigm to generative one\nenables us to bypass the target-level insensitivity. Experiments show that the\nproposed method achieves competitive performance gains over state-of-the-art\nmethods on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets. Code are available at\nhttps://github.com/Li-Haoqing/IRSTD-Diff.\n"
    },
    {
        "title": "A Generalized Framework with Adaptive Weighted Soft-Margin for\n  Imbalanced SVM Classification",
        "published_time": "2024-03-13T09:43:14Z",
        "abstract": "  Category imbalance is one of the most popular and important issues in the\ndomain of classification. In this paper, we present a new generalized framework\nwith Adaptive Weight function for soft-margin Weighted SVM (AW-WSVM), which\naims to enhance the issue of imbalance and outlier sensitivity in standard\nsupport vector machine (SVM) for classifying two-class data. The weight\ncoefficient is introduced into the unconstrained soft-margin support vector\nmachines, and the sample weights are updated before each training. The Adaptive\nWeight function (AW function) is constructed from the distance between the\nsamples and the decision hyperplane, assigning different weights to each\nsample. A weight update method is proposed, taking into account the proximity\nof the support vectors to the decision hyperplane. Before training, the weights\nof the corresponding samples are initialized according to different categories.\nSubsequently, the samples close to the decision hyperplane are identified and\nassigned more weights. At the same time, lower weights are assigned to samples\nthat are far from the decision hyperplane. Furthermore, we also put forward an\neffective way to eliminate noise. To evaluate the strength of the proposed\ngeneralized framework, we conducted experiments on standard datasets and\nemotion classification datasets with different imbalanced ratios (IR). The\nexperimental results prove that the proposed generalized framework outperforms\nin terms of accuracy, recall metrics and G-mean, validating the effectiveness\nof the weighted strategy provided in this paper in enhancing support vector\nmachines.\n"
    },
    {
        "title": "MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D\n  Sparse Convolutions",
        "published_time": "2024-03-12T12:25:54Z",
        "abstract": "  This paper presents MinkUNeXt, an effective and efficient architecture for\nplace-recognition from point clouds entirely based on the new 3D MinkNeXt\nBlock, a residual block composed of 3D sparse convolutions that follows the\nphilosophy established by recent Transformers but purely using simple 3D\nconvolutions. Feature extraction is performed at different scales by a U-Net\nencoder-decoder network and the feature aggregation of those features into a\nsingle descriptor is carried out by a Generalized Mean Pooling (GeM). The\nproposed architecture demonstrates that it is possible to surpass the current\nstate-of-the-art by only relying on conventional 3D sparse convolutions without\nmaking use of more complex and sophisticated proposals such as Transformers,\nAttention-Layers or Deformable Convolutions. A thorough assessment of the\nproposal has been carried out using the Oxford RobotCar and the In-house\ndatasets. As a result, MinkUNeXt proves to outperform other methods in the\nstate-of-the-art.\n"
    },
    {
        "title": "METER: a mobile vision transformer architecture for monocular depth\n  estimation",
        "published_time": "2024-03-13T09:30:08Z",
        "abstract": "  Depth estimation is a fundamental knowledge for autonomous systems that need\nto assess their own state and perceive the surrounding environment. Deep\nlearning algorithms for depth estimation have gained significant interest in\nrecent years, owing to the potential benefits of this methodology in overcoming\nthe limitations of active depth sensing systems. Moreover, due to the low cost\nand size of monocular cameras, researchers have focused their attention on\nmonocular depth estimation (MDE), which consists in estimating a dense depth\nmap from a single RGB video frame. State of the art MDE models typically rely\non vision transformers (ViT) architectures that are highly deep and complex,\nmaking them unsuitable for fast inference on devices with hardware constraints.\nPurposely, in this paper, we address the problem of exploiting ViT in MDE on\nembedded devices. Those systems are usually characterized by limited memory\ncapabilities and low-power CPU/GPU. We propose METER, a novel lightweight\nvision transformer architecture capable of achieving state of the art\nestimations and low latency inference performances on the considered embedded\nhardwares: NVIDIA Jetson TX1 and NVIDIA Jetson Nano. We provide a solution\nconsisting of three alternative configurations of METER, a novel loss function\nto balance pixel estimation and reconstruction of image details, and a new data\naugmentation strategy to improve the overall final predictions. The proposed\nmethod outperforms previous lightweight works over the two benchmark datasets:\nthe indoor NYU Depth v2 and the outdoor KITTI.\n"
    },
    {
        "title": "Improved Image-based Pose Regressor Models for Underwater Environments",
        "published_time": "2024-03-13T09:20:43Z",
        "abstract": "  We investigate the performance of image-based pose regressor models in\nunderwater environments for relocalization. Leveraging PoseNet and PoseLSTM, we\nregress a 6-degree-of-freedom pose from single RGB images with high accuracy.\nAdditionally, we explore data augmentation with stereo camera images to improve\nmodel accuracy. Experimental results demonstrate that the models achieve high\naccuracy in both simulated and clear waters, promising effective real-world\nunderwater navigation and inspection applications.\n"
    },
    {
        "title": "NaturalVLM: Leveraging Fine-grained Natural Language for\n  Affordance-Guided Visual Manipulation",
        "published_time": "2024-03-13T09:12:16Z",
        "abstract": "  Enabling home-assistant robots to perceive and manipulate a diverse range of\n3D objects based on human language instructions is a pivotal challenge. Prior\nresearch has predominantly focused on simplistic and task-oriented\ninstructions, i.e., \"Slide the top drawer open\". However, many real-world tasks\ndemand intricate multi-step reasoning, and without human instructions, these\nwill become extremely difficult for robot manipulation. To address these\nchallenges, we introduce a comprehensive benchmark, NrVLM, comprising 15\ndistinct manipulation tasks, containing over 4500 episodes meticulously\nannotated with fine-grained language instructions. We split the long-term task\nprocess into several steps, with each step having a natural language\ninstruction. Moreover, we propose a novel learning framework that completes the\nmanipulation task step-by-step according to the fine-grained instructions.\nSpecifically, we first identify the instruction to execute, taking into account\nvisual observations and the end-effector's current state. Subsequently, our\napproach facilitates explicit learning through action-prompts and\nperception-prompts to promote manipulation-aware cross-modality alignment.\nLeveraging both visual observations and linguistic guidance, our model outputs\na sequence of actionable predictions for manipulation, including contact points\nand end-effector poses. We evaluate our method and baselines using the proposed\nbenchmark NrVLM. The experimental results demonstrate the effectiveness of our\napproach. For additional details, please refer to\nhttps://sites.google.com/view/naturalvlm.\n"
    },
    {
        "title": "Data augmentation with automated machine learning: approaches and\n  performance comparison with classical data augmentation methods",
        "published_time": "2024-03-13T09:00:38Z",
        "abstract": "  Data augmentation is arguably the most important regularization technique\ncommonly used to improve generalization performance of machine learning models.\nIt primarily involves the application of appropriate data transformation\noperations to create new data samples with desired properties. Despite its\neffectiveness, the process is often challenging because of the time-consuming\ntrial and error procedures for creating and testing different candidate\naugmentations and their hyperparameters manually. Automated data augmentation\nmethods aim to automate the process. State-of-the-art approaches typically rely\non automated machine learning (AutoML) principles. This work presents a\ncomprehensive survey of AutoML-based data augmentation techniques. We discuss\nvarious approaches for accomplishing data augmentation with AutoML, including\ndata manipulation, data integration and data synthesis techniques. We present\nextensive discussion of techniques for realizing each of the major subtasks of\nthe data augmentation process: search space design, hyperparameter optimization\nand model evaluation. Finally, we carried out an extensive comparison and\nanalysis of the performance of automated data augmentation techniques and\nstate-of-the-art methods based on classical augmentation approaches. The\nresults show that AutoML methods for data augmentation currently outperform\nstate-of-the-art techniques based on conventional approaches.\n"
    },
    {
        "title": "CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large\n  Language Model",
        "published_time": "2024-03-13T08:54:31Z",
        "abstract": "  Instruction tuning represents a prevalent strategy employed by Multimodal\nLarge Language Models (MLLMs) to align with human instructions and adapt to new\ntasks. Nevertheless, MLLMs encounter the challenge of adapting to users'\nevolving knowledge and demands. Therefore, how to retain existing skills while\nacquiring new knowledge needs to be investigated. In this paper, we present a\ncomprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess\nexisting MLLMs in the sequential instruction tuning paradigm. CoIN comprises 10\ncommonly used datasets spanning 8 task categories, ensuring a diverse range of\ninstructions and tasks. Besides, the trained model is evaluated from two\naspects: Instruction Following and General Knowledge, which assess the\nalignment with human intention and knowledge preserved for reasoning,\nrespectively. Experiments on CoIN demonstrate that current powerful MLLMs still\nsuffer catastrophic forgetting, and the failure in intention alignment assumes\nthe main responsibility, instead of the knowledge forgetting. To this end, we\nintroduce MoELoRA to MLLMs which is effective to retain the previous\ninstruction alignment. Experimental results consistently illustrate the\nforgetting decreased from this method on CoIN.\n"
    },
    {
        "title": "STMPL: Human Soft-Tissue Simulation",
        "published_time": "2024-03-13T08:49:40Z",
        "abstract": "  In various applications, such as virtual reality and gaming, simulating the\ndeformation of soft tissues in the human body during interactions with external\nobjects is essential. Traditionally, Finite Element Methods (FEM) have been\nemployed for this purpose, but they tend to be slow and resource-intensive. In\nthis paper, we propose a unified representation of human body shape and soft\ntissue with a data-driven simulator of non-rigid deformations. This approach\nenables rapid simulation of realistic interactions.\n  Our method builds upon the SMPL model, which generates human body shapes\nconsidering rigid transformations. We extend SMPL by incorporating a soft\ntissue layer and an intuitive representation of external forces applied to the\nbody during object interactions. Specifically, we mapped the 3D body shape and\nsoft tissue and applied external forces to 2D UV maps. Leveraging a UNET\narchitecture designed for 2D data, our approach achieves high-accuracy\ninference in real time. Our experiment shows that our method achieves plausible\ndeformation of the soft tissue layer, even for unseen scenarios.\n"
    },
    {
        "title": "Activating Wider Areas in Image Super-Resolution",
        "published_time": "2024-03-13T08:29:58Z",
        "abstract": "  The prevalence of convolution neural networks (CNNs) and vision transformers\n(ViTs) has markedly revolutionized the area of single-image super-resolution\n(SISR). To further boost the SR performances, several techniques, such as\nresidual learning and attention mechanism, are introduced, which can be largely\nattributed to a wider range of activated area, that is, the input pixels that\nstrongly influence the SR results. However, the possibility of further\nimproving SR performance through another versatile vision backbone remains an\nunresolved challenge. To address this issue, in this paper, we unleash the\nrepresentation potential of the modern state space model, i.e., Vision Mamba\n(Vim), in the context of SISR. Specifically, we present three recipes for\nbetter utilization of Vim-based models: 1) Integration into a MetaFormer-style\nblock; 2) Pre-training on a larger and broader dataset; 3) Employing\ncomplementary attention mechanism, upon which we introduce the MMA. The\nresulting network MMA is capable of finding the most relevant and\nrepresentative input pixels to reconstruct the corresponding high-resolution\nimages. Comprehensive experimental analysis reveals that MMA not only achieves\ncompetitive or even superior performance compared to state-of-the-art SISR\nmethods but also maintains relatively low memory and computational overheads\n(e.g., +0.5 dB PSNR elevation on Manga109 dataset with 19.8 M parameters at the\nscale of 2). Furthermore, MMA proves its versatility in lightweight SR\napplications. Through this work, we aim to illuminate the potential\napplications of state space models in the broader realm of image processing\nrather than SISR, encouraging further exploration in this innovative direction.\n"
    },
    {
        "title": "Modular Blind Video Quality Assessment",
        "published_time": "2024-02-29T15:44:00Z",
        "abstract": "  Blind video quality assessment (BVQA) plays a pivotal role in evaluating and\nimproving the viewing experience of end-users across a wide range of\nvideo-based platforms and services. Contemporary deep learning-based models\nprimarily analyze the video content in its aggressively downsampled format,\nwhile being blind to the impact of actual spatial resolution and frame rate on\nvideo quality. In this paper, we propose a modular BVQA model, and a method of\ntraining it to improve its modularity. Specifically, our model comprises a base\nquality predictor, a spatial rectifier, and a temporal rectifier, responding to\nthe visual content and distortion, spatial resolution, and frame rate changes\non video quality, respectively. During training, spatial and temporal\nrectifiers are dropped out with some probabilities so as to make the base\nquality predictor a standalone BVQA model, which should work better with the\nrectifiers. Extensive experiments on both professionally-generated content and\nuser generated content video databases show that our quality model achieves\nsuperior or comparable performance to current methods. Furthermore, the\nmodularity of our model offers a great opportunity to analyze existing video\nquality databases in terms of their spatial and temporal complexities. Last,\nour BVQA model is cost-effective to add other quality-relevant video attributes\nsuch as dynamic range and color gamut as additional rectifiers.\n"
    },
    {
        "title": "An Image Enhancement Method for Improving Small Intestinal Villi Clarity",
        "published_time": "2024-02-25T03:43:07Z",
        "abstract": "  This paper presents, for the first time, an image enhancement methodology\ndesigned to enhance the clarity of small intestinal villi in Wireless Capsule\nEndoscopy (WCE) images. This method first separates the low-frequency and\nhigh-frequency components of small intestinal villi images using guided\nfiltering. Subsequently, an adaptive light gain factor is generated based on\nthe low-frequency component, and an adaptive gradient gain factor is derived\nfrom the convolution results of the Laplacian operator in different regions of\nsmall intestinal villi images. The obtained light gain factor and gradient gain\nfactor are then combined to enhance the high-frequency components. Finally, the\nenhanced high-frequency component is fused with the original image to achieve\nadaptive sharpening of the edges of WCE small intestinal villi images. The\nexperiments affirm that, compared to established WCE image enhancement methods,\nour approach not only accentuates the edge details of WCE small intestine villi\nimages but also skillfully suppresses noise amplification, thereby preventing\nthe occurrence of edge overshooting.\n"
    },
    {
        "title": "ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic\n  Manipulation",
        "published_time": "2024-03-13T08:06:41Z",
        "abstract": "  Performing language-conditioned robotic manipulation tasks in unstructured\nenvironments is highly demanded for general intelligent robots. Conventional\nrobotic manipulation methods usually learn semantic representation of the\nobservation for action prediction, which ignores the scene-level spatiotemporal\ndynamics for human goal completion. In this paper, we propose a dynamic\nGaussian Splatting method named ManiGaussian for multi-task robotic\nmanipulation, which mines scene dynamics via future scene reconstruction.\nSpecifically, we first formulate the dynamic Gaussian Splatting framework that\ninfers the semantics propagation in the Gaussian embedding space, where the\nsemantic representation is leveraged to predict the optimal robot action. Then,\nwe build a Gaussian world model to parameterize the distribution in our dynamic\nGaussian Splatting framework, which provides informative supervision in the\ninteractive environment via future scene reconstruction. We evaluate our\nManiGaussian on 10 RLBench tasks with 166 variations, and the results\ndemonstrate our framework can outperform the state-of-the-art methods by 13.1\\%\nin average success rate.\n"
    },
    {
        "title": "LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content",
        "published_time": "2024-03-09T09:52:15Z",
        "abstract": "  Long-tail recognition is challenging because it requires the model to learn\ngood representations from tail categories and address imbalances across all\ncategories. In this paper, we propose a novel generative and fine-tuning\nframework, LTGC, to handle long-tail recognition via leveraging generated\ncontent. Firstly, inspired by the rich implicit knowledge in large-scale models\n(e.g., large language models, LLMs), LTGC leverages the power of these models\nto parse and reason over the original tail data to produce diverse tail-class\ncontent. We then propose several novel designs for LTGC to ensure the quality\nof the generated data and to efficiently fine-tune the model using both the\ngenerated and original data. The visualization demonstrates the effectiveness\nof the generation module in LTGC, which produces accurate and diverse tail\ndata. Additionally, the experimental results demonstrate that our LTGC\noutperforms existing state-of-the-art methods on popular long-tailed\nbenchmarks.\n"
    },
    {
        "title": "DrFER: Learning Disentangled Representations for 3D Facial Expression\n  Recognition",
        "published_time": "2024-03-13T08:00:07Z",
        "abstract": "  Facial Expression Recognition (FER) has consistently been a focal point in\nthe field of facial analysis. In the context of existing methodologies for 3D\nFER or 2D+3D FER, the extraction of expression features often gets entangled\nwith identity information, compromising the distinctiveness of these features.\nTo tackle this challenge, we introduce the innovative DrFER method, which\nbrings the concept of disentangled representation learning to the field of 3D\nFER. DrFER employs a dual-branch framework to effectively disentangle\nexpression information from identity information. Diverging from prior\ndisentanglement endeavors in the 3D facial domain, we have carefully\nreconfigured both the loss functions and network structure to make the overall\nframework adaptable to point cloud data. This adaptation enhances the\ncapability of the framework in recognizing facial expressions, even in cases\ninvolving varying head poses. Extensive evaluations conducted on the BU-3DFE\nand Bosphorus datasets substantiate that DrFER surpasses the performance of\nother 3D FER methods.\n"
    },
    {
        "title": "SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM",
        "published_time": "2024-02-05T18:03:53Z",
        "abstract": "  We present SGS-SLAM, the first semantic visual SLAM system based on Gaussian\nSplatting. It incorporates appearance, geometry, and semantic features through\nmulti-channel optimization, addressing the oversmoothing limitations of neural\nimplicit SLAM systems in high-quality rendering, scene understanding, and\nobject-level geometry. We introduce a unique semantic feature loss that\neffectively compensates for the shortcomings of traditional depth and color\nlosses in object optimization. Through a semantic-guided keyframe selection\nstrategy, we prevent erroneous reconstructions caused by cumulative errors.\nExtensive experiments demonstrate that SGS-SLAM delivers state-of-the-art\nperformance in camera pose estimation, map reconstruction, precise semantic\nsegmentation, and object-level geometric accuracy, while ensuring real-time\nrendering capabilities.\n"
    },
    {
        "title": "FSC: Few-point Shape Completion",
        "published_time": "2024-03-12T06:45:34Z",
        "abstract": "  While previous studies have demonstrated successful 3D object shape\ncompletion with a sufficient number of points, they often fail in scenarios\nwhen a few points, e.g. tens of points, are observed. Surprisingly, via entropy\nanalysis, we find that even a few points, e.g. 64 points, could retain\nsubstantial information to help recover the 3D shape of the object. To address\nthe challenge of shape completion with very sparse point clouds, we then\npropose Few-point Shape Completion (FSC) model, which contains a novel\ndual-branch feature extractor for handling extremely sparse inputs, coupled\nwith an extensive branch for maximal point utilization with a saliency branch\nfor dynamic importance assignment. This model is further bolstered by a\ntwo-stage revision network that refines both the extracted features and the\ndecoder output, enhancing the detail and authenticity of the completed point\ncloud. Our experiments demonstrate the feasibility of recovering 3D shapes from\na few points. The proposed Few-point Shape Completion (FSC) model outperforms\nprevious methods on both few-point inputs and many-point inputs, and shows good\ngeneralizability to different object categories.\n"
    },
    {
        "title": "StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance\n  Fields",
        "published_time": "2024-03-13T07:42:21Z",
        "abstract": "  4D style transfer aims at transferring arbitrary visual style to the\nsynthesized novel views of a dynamic 4D scene with varying viewpoints and\ntimes. Existing efforts on 3D style transfer can effectively combine the visual\nfeatures of style images and neural radiance fields (NeRF) but fail to handle\nthe 4D dynamic scenes limited by the static scene assumption. Consequently, we\naim to handle the novel challenging problem of 4D style transfer for the first\ntime, which further requires the consistency of stylized results on dynamic\nobjects. In this paper, we introduce StyleDyRF, a method that represents the 4D\nfeature space by deforming a canonical feature volume and learns a linear style\ntransformation matrix on the feature volume in a data-driven fashion. To obtain\nthe canonical feature volume, the rays at each time step are deformed with the\ngeometric prior of a pre-trained dynamic NeRF to render the feature map under\nthe supervision of pre-trained visual encoders. With the content and style cues\nin the canonical feature volume and the style image, we can learn the style\ntransformation matrix from their covariance matrices with lightweight neural\nnetworks. The learned style transformation matrix can reflect a direct matching\nof feature covariance from the content volume to the given style pattern, in\nanalogy with the optimization of the Gram matrix in traditional 2D neural style\ntransfer. The experimental results show that our method not only renders 4D\nphotorealistic style transfer results in a zero-shot manner but also\noutperforms existing methods in terms of visual quality and consistency.\n"
    },
    {
        "title": "Tissue Artifact Segmentation and Severity Analysis for Automated\n  Diagnosis Using Whole Slide Images",
        "published_time": "2024-01-01T19:58:36Z",
        "abstract": "  Traditionally, pathological analysis and diagnosis are performed by manually\neyeballing glass slide specimens under a microscope by an expert. The whole\nslide image is the digital specimen produced from the glass slide. Whole slide\nimage enabled specimens to be observed on a computer screen and led to\ncomputational pathology where computer vision and artificial intelligence are\nutilized for automated analysis and diagnosis. With the current computational\nadvancement, the entire whole slide image can be analyzed autonomously without\nhuman supervision. However, the analysis could fail or lead to wrong diagnosis\nif the whole slide image is affected by tissue artifacts such as tissue fold or\nair bubbles depending on the severity. Existing artifact detection methods rely\non experts for severity assessment to eliminate artifact affected regions from\nthe analysis. This process is time consuming, exhausting and undermines the\ngoal of automated analysis or removal of artifacts without evaluating their\nseverity, which could result in the loss of diagnostically important data.\nTherefore, it is necessary to detect artifacts and then assess their severity\nautomatically. In this paper, we propose a system that incorporates severity\nevaluation with artifact detection utilizing convolutional neural networks. The\nproposed system uses DoubleUNet to segment artifacts and an ensemble network of\nsix fine tuned convolutional neural network models to determine severity. This\nmethod outperformed current state of the art in accuracy by 9 percent for\nartifact segmentation and achieved a strong correlation of 97 percent with the\nevaluation of pathologists for severity assessment. The robustness of the\nsystem was demonstrated using our proposed heterogeneous dataset and practical\nusability was ensured by integrating it with an automated analysis system.\n"
    },
    {
        "title": "Attack Deterministic Conditional Image Generative Models for Diverse and\n  Controllable Generation",
        "published_time": "2024-03-13T06:57:23Z",
        "abstract": "  Existing generative adversarial network (GAN) based conditional image\ngenerative models typically produce fixed output for the same conditional\ninput, which is unreasonable for highly subjective tasks, such as large-mask\nimage inpainting or style transfer. On the other hand, GAN-based diverse image\ngenerative methods require retraining/fine-tuning the network or designing\ncomplex noise injection functions, which is computationally expensive,\ntask-specific, or struggle to generate high-quality results. Given that many\ndeterministic conditional image generative models have been able to produce\nhigh-quality yet fixed results, we raise an intriguing question: is it possible\nfor pre-trained deterministic conditional image generative models to generate\ndiverse results without changing network structures or parameters? To answer\nthis question, we re-examine the conditional image generation tasks from the\nperspective of adversarial attack and propose a simple and efficient plug-in\nprojected gradient descent (PGD) like method for diverse and controllable image\ngeneration. The key idea is attacking the pre-trained deterministic generative\nmodels by adding a micro perturbation to the input condition. In this way,\ndiverse results can be generated without any adjustment of network structures\nor fine-tuning of the pre-trained models. In addition, we can also control the\ndiverse results to be generated by specifying the attack direction according to\na reference text or image. Our work opens the door to applying adversarial\nattack to low-level vision tasks, and experiments on various conditional image\ngeneration tasks demonstrate the effectiveness and superiority of the proposed\nmethod.\n"
    },
    {
        "title": "MGIC: A Multi-Label Gradient Inversion Attack based on Canny Edge\n  Detection on Federated Learning",
        "published_time": "2024-03-13T06:34:49Z",
        "abstract": "  As a new distributed computing framework that can protect data privacy,\nfederated learning (FL) has attracted more and more attention in recent years.\nIt receives gradients from users to train the global model and releases the\ntrained global model to working users. Nonetheless, the gradient inversion (GI)\nattack reflects the risk of privacy leakage in federated learning. Attackers\nonly need to use gradients through hundreds of thousands of simple iterations\nto obtain relatively accurate private data stored on users' local devices. For\nthis, some works propose simple but effective strategies to obtain user data\nunder a single-label dataset. However, these strategies induce a satisfactory\nvisual effect of the inversion image at the expense of higher time costs. Due\nto the semantic limitation of a single label, the image obtained by gradient\ninversion may have semantic errors. We present a novel gradient inversion\nstrategy based on canny edge detection (MGIC) in both the multi-label and\nsingle-label datasets. To reduce semantic errors caused by a single label, we\nadd new convolution layers' blocks in the trained model to obtain the image's\nmulti-label. Through multi-label representation, serious semantic errors in\ninversion images are reduced. Then, we analyze the impact of parameters on the\ndifficulty of input image reconstruction and discuss how image multi-subjects\naffect the inversion performance. Our proposed strategy has better visual\ninversion image results than the most widely used ones, saving more than 78% of\ntime costs in the ImageNet dataset.\n"
    },
    {
        "title": "Optimized Detection and Classification on GTRSB: Advancing Traffic Sign\n  Recognition with Convolutional Neural Networks",
        "published_time": "2024-03-13T06:28:37Z",
        "abstract": "  In the rapidly evolving landscape of transportation, the proliferation of\nautomobiles has made road traffic more complex, necessitating advanced\nvision-assisted technologies for enhanced safety and navigation. These\ntechnologies are imperative for providing critical traffic sign information,\ninfluencing driver behavior, and supporting vehicle control, especially for\ndrivers with disabilities and in the burgeoning field of autonomous vehicles.\nTraffic sign detection and recognition have emerged as key areas of research\ndue to their essential roles in ensuring road safety and compliance with\ntraffic regulations. Traditional computer vision methods have faced challenges\nin achieving optimal accuracy and speed due to real-world variabilities.\nHowever, the advent of deep learning and Convolutional Neural Networks (CNNs)\nhas revolutionized this domain, offering solutions that significantly surpass\nprevious capabilities in terms of speed and reliability. This paper presents an\ninnovative approach leveraging CNNs that achieves an accuracy of nearly 96\\%,\nhighlighting the potential for even greater precision through advanced\nlocalization techniques. Our findings not only contribute to the ongoing\nadvancement of traffic sign recognition technology but also underscore the\ncritical impact of these developments on road safety and the future of\nautonomous driving.\n"
    },
    {
        "title": "Hierarchical Auto-Organizing System for Open-Ended Multi-Agent\n  Navigation",
        "published_time": "2024-03-13T06:22:17Z",
        "abstract": "  Navigating complex environments in Minecraft poses significant challenges for\nmulti-agent systems due to the game's dynamic and unpredictable open-world\nsetting. Agents need to interact with the environment and coordinate their\nactions with other agents to achieve common objectives. However, traditional\napproaches often struggle to efficiently manage inter-agent communication and\ntask distribution, which are crucial for effective multi-agent navigation.\nFurthermore, processing and integrating multi-modal information (such as\nvisual, textual, and auditory data) is essential for agents to fully comprehend\ntheir goals and navigate the environment successfully. To address this issue,\nwe design the HAS framework to auto-organize groups of LLM-based agents to\ncomplete Navigation tasks. In our approach, we devise a hierarchical\nauto-organizing navigation system, which is characterized by 1) a hierarchical\nsystem for multi-agent organization, ensuring centralized planning and\ndecentralized execution; 2) an auto-organizing and intra-communication\nmechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal\ninformation platform, facilitating multi-modal perception to perform the three\nnavigation tasks with one system. To assess organizational behavior, we design\na series of navigation tasks in the Minecraft environment, which includes\nsearching and exploring. We aim to develop embodied organizations that push the\nboundaries of embodied AI, moving it towards a more human-like organizational\nstructure.\n"
    },
    {
        "title": "Pre-examinations Improve Automated Metastases Detection on Cranial MRI",
        "published_time": "2024-03-13T06:18:08Z",
        "abstract": "  Materials and methods: First, a dual-time approach was assessed, for which\nthe CNN was provided sequences of the MRI that initially depicted new MM\n(diagnosis MRI) as well as of a prediagnosis MRI: inclusion of only\ncontrast-enhanced T1-weighted images (CNNdual_ce) was compared with inclusion\nof also the native T1-weighted images, T2-weighted images, and FLAIR sequences\nof both time points (CNNdual_all).Second, results were compared with the\ncorresponding single time approaches, in which the CNN was provided exclusively\nthe respective sequences of the diagnosis MRI.Casewise diagnostic performance\nparameters were calculated from 5-fold cross-validation.\n  Results: In total, 94 cases with 494 MMs were included. Overall, the highest\ndiagnostic performance was achieved by inclusion of only the contrast-enhanced\nT1-weighted images of the diagnosis and of a prediagnosis MRI (CNNdual_ce,\nsensitivity = 73%, PPV = 25%, F1-score = 36%). Using exclusively\ncontrast-enhanced T1-weighted images as input resulted in significantly less\nfalse-positives (FPs) compared with inclusion of further sequences beyond\ncontrast-enhanced T1-weighted images (FPs = 5/7 for CNNdual_ce/CNNdual_all, P <\n1e-5). Comparison of contrast-enhanced dual and mono time approaches revealed\nthat exclusion of prediagnosis MRI significantly increased FPs (FPs = 5/10 for\nCNNdual_ce/CNNce, P < 1e-9).Approaches with only native sequences were clearly\ninferior to CNNs that were provided contrast-enhanced sequences.\n  Conclusions: Automated MM detection on contrast-enhanced T1-weighted images\nperformed with high sensitivity. Frequent FPs due to artifacts and vessels were\nsignificantly reduced by additional inclusion of prediagnosis MRI, but not by\ninclusion of further sequences beyond contrast-enhanced T1-weighted images.\nFuture studies might investigate different change detection architectures for\ncomputer-aided detection.\n"
    },
    {
        "title": "VIGFace: Virtual Identity Generation Model for Face Image Synthesis",
        "published_time": "2024-03-13T06:11:41Z",
        "abstract": "  Deep learning-based face recognition continues to face challenges due to its\nreliance on huge datasets obtained from web crawling, which can be costly to\ngather and raise significant real-world privacy concerns. To address this\nissue, we propose VIGFace, a novel framework capable of generating synthetic\nfacial images. Initially, we train the face recognition model using a real face\ndataset and create a feature space for both real and virtual IDs where virtual\nprototypes are orthogonal to other prototypes. Subsequently, we generate\nsynthetic images by using the diffusion model based on the feature space. Our\nproposed framework provides two significant benefits. Firstly, it allows for\ncreating virtual facial images without concerns about portrait rights,\nguaranteeing that the generated virtual face images are clearly differentiated\nfrom existing individuals. Secondly, it serves as an effective augmentation\nmethod by incorporating real existing images. Further experiments demonstrate\nthe efficacy of our framework, achieving state-of-the-art results from both\nperspectives without any external data.\n"
    },
    {
        "title": "LiqD: A Dynamic Liquid Level Detection Model under Tricky Small\n  Containers",
        "published_time": "2024-03-13T05:53:25Z",
        "abstract": "  In daily life and industrial production, it is crucial to accurately detect\nchanges in liquid level in containers. Traditional contact measurement methods\nhave some limitations, while emerging non-contact image processing technology\nshows good application prospects. This paper proposes a container dynamic\nliquid level detection model based on U^2-Net. This model uses the SAM model to\ngenerate an initial data set, and then evaluates and filters out high-quality\npseudo-label images through the SemiReward framework to build an exclusive data\nset. The model uses U^2-Net to extract mask images of containers from the data\nset, and uses morphological processing to compensate for mask defects.\nSubsequently, the model calculates the grayscale difference between adjacent\nvideo frame images at the same position, segments the liquid level change area\nby setting a difference threshold, and finally uses a lightweight neural\nnetwork to classify the liquid level state. This approach not only mitigates\nthe impact of intricate surroundings, but also reduces the demand for training\ndata, showing strong robustness and versatility. A large number of experimental\nresults show that the proposed model can effectively detect the dynamic liquid\nlevel changes of the liquid in the container, providing a novel and efficient\nsolution for related fields.\n"
    },
    {
        "title": "Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained\n  Ship Classification",
        "published_time": "2024-03-13T05:48:58Z",
        "abstract": "  Fine-grained ship classification in remote sensing (RS-FGSC) poses a\nsignificant challenge due to the high similarity between classes and the\nlimited availability of labeled data, limiting the effectiveness of traditional\nsupervised classification methods. Recent advancements in large pre-trained\nVision-Language Models (VLMs) have demonstrated impressive capabilities in\nfew-shot or zero-shot learning, particularly in understanding image content.\nThis study delves into harnessing the potential of VLMs to enhance\nclassification accuracy for unseen ship categories, which holds considerable\nsignificance in scenarios with restricted data due to cost or privacy\nconstraints. Directly fine-tuning VLMs for RS-FGSC often encounters the\nchallenge of overfitting the seen classes, resulting in suboptimal\ngeneralization to unseen classes, which highlights the difficulty in\ndifferentiating complex backgrounds and capturing distinct ship features. To\naddress these issues, we introduce a novel prompt tuning technique that employs\na hierarchical, multi-granularity prompt design. Our approach integrates remote\nsensing ship priors through bias terms, learned from a small trainable network.\nThis strategy enhances the model's generalization capabilities while improving\nits ability to discern intricate backgrounds and learn discriminative ship\nfeatures. Furthermore, we contribute to the field by introducing a\ncomprehensive dataset, FGSCM-52, significantly expanding existing datasets with\nmore extensive data and detailed annotations for less common ship classes.\nExtensive experimental evaluations demonstrate the superiority of our proposed\nmethod over current state-of-the-art techniques. The source code will be made\npublicly available.\n"
    },
    {
        "title": "Identity-aware Dual-constraint Network for Cloth-Changing Person\n  Re-identification",
        "published_time": "2024-03-13T05:46:36Z",
        "abstract": "  Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify\nthe target person in more realistic surveillance scenarios, where pedestrians\nusually change their clothing. Despite great progress, limited cloth-changing\ntraining samples in existing CC-ReID datasets still prevent the model from\nadequately learning cloth-irrelevant features. In addition, due to the absence\nof explicit supervision to keep the model constantly focused on\ncloth-irrelevant areas, existing methods are still hampered by the disruption\nof clothing variations. To solve the above issues, we propose an Identity-aware\nDual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the\nmodel extract cloth-irrelevant clues, we propose a Clothes Diversity\nAugmentation (CDA), which generates more realistic cloth-changing samples by\nenriching the clothing color while preserving the texture. In addition, a\nMulti-scale Constraint Block (MCB) is designed, which extracts fine-grained\nidentity-related features and effectively transfers cloth-irrelevant knowledge.\nMoreover, a Counterfactual-guided Attention Module (CAM) is presented, which\nlearns cloth-irrelevant features from channel and space dimensions and utilizes\nthe counterfactual intervention for supervising the attention map to highlight\nidentity-related regions. Finally, a Semantic Alignment Constraint (SAC) is\ndesigned to facilitate high-level semantic feature interaction. Comprehensive\nexperiments on four CC-ReID datasets indicate that our method outperforms prior\nstate-of-the-art approaches.\n"
    },
    {
        "title": "Follow-Your-Click: Open-domain Regional Image Animation via Short\n  Prompts",
        "published_time": "2024-03-13T05:44:37Z",
        "abstract": "  Despite recent advances in image-to-video generation, better controllability\nand local animation are less explored. Most existing image-to-video methods are\nnot locally aware and tend to move the entire scene. However, human artists may\nneed to control the movement of different objects or regions. Additionally,\ncurrent I2V methods require users not only to describe the target motion but\nalso to provide redundant detailed descriptions of frame contents. These two\nissues hinder the practical utilization of current I2V tools. In this paper, we\npropose a practical framework, named Follow-Your-Click, to achieve image\nanimation with a simple user click (for specifying what to move) and a short\nmotion prompt (for specifying how to move). Technically, we propose the\nfirst-frame masking strategy, which significantly improves the video generation\nquality, and a motion-augmented module equipped with a short motion prompt\ndataset to improve the short prompt following abilities of our model. To\nfurther control the motion speed, we propose flow-based motion magnitude\ncontrol to control the speed of target movement more precisely. Our framework\nhas simpler yet precise user control and better generation performance than\nprevious methods. Extensive experiments compared with 7 baselines, including\nboth commercial tools and research methods on 8 metrics, suggest the\nsuperiority of our approach. Project Page: https://follow-your-click.github.io/\n"
    },
    {
        "title": "Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models",
        "published_time": "2024-03-13T05:33:52Z",
        "abstract": "  While manga is a popular entertainment form, creating manga is tedious,\nespecially adding screentones to the created sketch, namely manga screening.\nUnfortunately, there is no existing method that tailors for automatic manga\nscreening, probably due to the difficulty of generating high-quality shaded\nhigh-frequency screentones. The classic manga screening approaches generally\nrequire user input to provide screentone exemplars or a reference manga image.\nThe recent deep learning models enables the automatic generation by learning\nfrom a large-scale dataset. However, the state-of-the-art models still fail to\ngenerate high-quality shaded screentones due to the lack of a tailored model\nand high-quality manga training data. In this paper, we propose a novel\nsketch-to-manga framework that first generates a color illustration from the\nsketch and then generates a screentoned manga based on the intensity guidance.\nOur method significantly outperforms existing methods in generating\nhigh-quality manga with shaded high-frequency screentones.\n"
    },
    {
        "title": "BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands\n  from a Single Image",
        "published_time": "2024-03-13T05:25:49Z",
        "abstract": "  Creating personalized hand avatars is important to offer a realistic\nexperience to users on AR / VR platforms. While most prior studies focused on\nreconstructing 3D hand shapes, some recent work has tackled the reconstruction\nof hand textures on top of shapes. However, these methods are often limited to\ncapturing pixels on the visible side of a hand, requiring diverse views of the\nhand in a video or multiple images as input. In this paper, we propose a novel\nmethod, BiTT(Bi-directional Texture reconstruction of Two hands), which is the\nfirst end-to-end trainable method for relightable, pose-free texture\nreconstruction of two interacting hands taking only a single RGB image, by\nthree novel components: 1)\\ bi-directional (left $\\leftrightarrow$ right)\ntexture reconstruction using the texture symmetry of left / right hands, 2)\nutilizing a texture parametric model for hand texture recovery, and 3)\\ the\noverall coarse-to-fine stage pipeline for reconstructing personalized texture\nof two interacting hands. BiTT first estimates the scene light condition and\nalbedo image from an input image, then reconstructs the texture of both hands\nthrough the texture parametric model and bi-directional texture reconstructor.\nIn experiments using InterHand2.6M and RGB2Hands datasets, our method\nsignificantly outperforms state-of-the-art hand texture reconstruction methods\nquantitatively and qualitatively. The code is available at\nhttps://github.com/yunminjin2/BiTT\n"
    },
    {
        "title": "CoroNetGAN: Controlled Pruning of GANs via Hypernetworks",
        "published_time": "2024-03-13T05:24:28Z",
        "abstract": "  Generative Adversarial Networks (GANs) have proven to exhibit remarkable\nperformance and are widely used across many generative computer vision\napplications. However, the unprecedented demand for the deployment of GANs on\nresource-constrained edge devices still poses a challenge due to huge number of\nparameters involved in the generation process. This has led to focused\nattention on the area of compressing GANs. Most of the existing works use\nknowledge distillation with the overhead of teacher dependency. Moreover, there\nis no ability to control the degree of compression in these methods. Hence, we\npropose CoroNet-GAN for compressing GAN using the combined strength of\ndifferentiable pruning method via hypernetworks. The proposed method provides\nthe advantage of performing controllable compression while training along with\nreducing training time by a substantial factor. Experiments have been done on\nvarious conditional GAN architectures (Pix2Pix and CycleGAN) to signify the\neffectiveness of our approach on multiple benchmark datasets such as\nEdges-to-Shoes, Horse-to-Zebra and Summer-to-Winter. The results obtained\nillustrate that our approach succeeds to outperform the baselines on\nZebra-to-Horse and Summer-to-Winter achieving the best FID score of 32.3 and\n72.3 respectively, yielding high-fidelity images across all the datasets.\nAdditionally, our approach also outperforms the state-of-the-art methods in\nachieving better inference time on various smart-phone chipsets and data-types\nmaking it a feasible solution for deployment on edge devices.\n"
    },
    {
        "title": "TINA: Think, Interaction, and Action Framework for Zero-Shot Vision\n  Language Navigation",
        "published_time": "2024-03-13T05:22:39Z",
        "abstract": "  Zero-shot navigation is a critical challenge in Vision-Language Navigation\n(VLN) tasks, where the ability to adapt to unfamiliar instructions and to act\nin unknown environments is essential. Existing supervised learning-based\nmodels, trained using annotated data through reinforcement learning, exhibit\nlimitations in generalization capabilities. Large Language Models (LLMs), with\ntheir extensive knowledge and emergent reasoning abilities, present a potential\npathway for achieving zero-shot navigation. This paper presents a VLN agent\nbased on LLMs, exploring approaches to the zero-shot navigation problem. To\ncompensate for the shortcomings of LLMs in environmental perception, we propose\nthe Thinking, Interacting, and Action (TINA) framework. TINA enables the agent\nto scrutinize perceptual information and autonomously query key clues within\nthe environment through an introduced question-answering module, thereby\naligning instructions with specific perceptual data. The navigation agent's\nperceptual abilities are enhanced through the TINA framework, while the\nexplicit thought and query processes also improve the navigational procedure's\nexplainability and transparency. We evaluate the performance of our method on\nthe Room-to-Room dataset. The experiment results indicate that our approach\nimproves the navigation performance of LLM-based agents. Our approach also\noutperformed some supervised learning-based methods, highlighting its efficacy\nin zero-shot navigation.\n"
    },
    {
        "title": "IG-FIQA: Improving Face Image Quality Assessment through Intra-class\n  Variance Guidance robust to Inaccurate Pseudo-Labels",
        "published_time": "2024-03-13T05:15:43Z",
        "abstract": "  In the realm of face image quality assesment (FIQA), method based on sample\nrelative classification have shown impressive performance. However, the quality\nscores used as pseudo-labels assigned from images of classes with low\nintra-class variance could be unrelated to the actual quality in this method.\nTo address this issue, we present IG-FIQA, a novel approach to guide FIQA\ntraining, introducing a weight parameter to alleviate the adverse impact of\nthese classes. This method involves estimating sample intra-class variance at\neach iteration during training, ensuring minimal computational overhead and\nstraightforward implementation. Furthermore, this paper proposes an on-the-fly\ndata augmentation methodology for improved generalization performance in FIQA.\nOn various benchmark datasets, our proposed method, IG-FIQA, achieved novel\nstate-of-the-art (SOTA) performance.\n"
    },
    {
        "title": "Make Me Happier: Evoking Emotions Through Image Diffusion Models",
        "published_time": "2024-03-13T05:13:17Z",
        "abstract": "  Despite the rapid progress in image generation, emotional image editing\nremains under-explored. The semantics, context, and structure of an image can\nevoke emotional responses, making emotional image editing techniques valuable\nfor various real-world applications, including treatment of psychological\ndisorders, commercialization of products, and artistic design. For the first\ntime, we present a novel challenge of emotion-evoked image generation, aiming\nto synthesize images that evoke target emotions while retaining the semantics\nand structures of the original scenes. To address this challenge, we propose a\ndiffusion model capable of effectively understanding and editing source images\nto convey desired emotions and sentiments. Moreover, due to the lack of emotion\nediting datasets, we provide a unique dataset consisting of 340,000 pairs of\nimages and their emotion annotations. Furthermore, we conduct human\npsychophysics experiments and introduce four new evaluation metrics to\nsystematically benchmark all the methods. Experimental results demonstrate that\nour method surpasses all competitive baselines. Our diffusion model is capable\nof identifying emotional cues from original images, editing images that elicit\ndesired emotions, and meanwhile, preserving the semantic structure of the\noriginal images. All code, model, and data will be made public.\n"
    },
    {
        "title": "PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style\n  Mapping",
        "published_time": "2024-03-13T05:08:47Z",
        "abstract": "  3D scene stylization refers to transform the appearance of a 3D scene to\nmatch a given style image, ensuring that images rendered from different\nviewpoints exhibit the same style as the given style image, while maintaining\nthe 3D consistency of the stylized scene. Several existing methods have\nobtained impressive results in stylizing 3D scenes. However, the models\nproposed by these methods need to be re-trained when applied to a new scene. In\nother words, their models are coupled with a specific scene and cannot adapt to\narbitrary other scenes. To address this issue, we propose a novel 3D scene\nstylization framework to transfer an arbitrary style to an arbitrary scene,\nwithout any style-related or scene-related re-training. Concretely, we first\nmap the appearance of the 3D scene into a 2D style pattern space, which\nrealizes complete disentanglement of the geometry and appearance of the 3D\nscene and makes our model be generalized to arbitrary 3D scenes. Then we\nstylize the appearance of the 3D scene in the 2D style pattern space via a\nprompt-based 2D stylization algorithm. Experimental results demonstrate that\nour proposed framework is superior to SOTA methods in both visual quality and\ngeneralization.\n"
    },
    {
        "title": "A Dual-domain Regularization Method for Ring Artifact Removal of X-ray\n  CT",
        "published_time": "2024-03-13T05:01:37Z",
        "abstract": "  Ring artifacts in computed tomography images, arising from the undesirable\nresponses of detector units, significantly degrade image quality and diagnostic\nreliability. To address this challenge, we propose a dual-domain regularization\nmodel to effectively remove ring artifacts, while maintaining the integrity of\nthe original CT image. The proposed model corrects the vertical stripe\nartifacts on the sinogram by innovatively updating the response inconsistency\ncompensation coefficients of detector units, which is achieved by employing the\ngroup sparse constraint and the projection-view direction sparse constraint on\nthe stripe artifacts. Simultaneously, we apply the sparse constraint on the\nreconstructed image to further rectified ring artifacts in the image domain.\nThe key advantage of the proposed method lies in considering the relationship\nbetween the response inconsistency compensation coefficients of the detector\nunits and the projection views, which enables a more accurate correction of the\nresponse of the detector units. An alternating minimization method is designed\nto solve the model. Comparative experiments on real photon counting detector\ndata demonstrate that the proposed method not only surpasses existing methods\nin removing ring artifacts but also excels in preserving structural details and\nimage fidelity.\n"
    },
    {
        "title": "Continuous Object State Recognition for Cooking Robots Using Pre-Trained\n  Vision-Language Models and Black-box Optimization",
        "published_time": "2024-03-13T04:45:40Z",
        "abstract": "  The state recognition of the environment and objects by robots is generally\nbased on the judgement of the current state as a classification problem. On the\nother hand, state changes of food in cooking happen continuously and need to be\ncaptured not only at a certain time point but also continuously over time. In\naddition, the state changes of food are complex and cannot be easily described\nby manual programming. Therefore, we propose a method to recognize the\ncontinuous state changes of food for cooking robots through the spoken language\nusing pre-trained large-scale vision-language models. By using models that can\ncompute the similarity between images and texts continuously over time, we can\ncapture the state changes of food while cooking. We also show that by adjusting\nthe weighting of each text prompt based on fitting the similarity changes to a\nsigmoid function and then performing black-box optimization, more accurate and\nrobust continuous state recognition can be achieved. We demonstrate the\neffectiveness and limitations of this method by performing the recognition of\nwater boiling, butter melting, egg cooking, and onion stir-frying.\n"
    },
    {
        "title": "Point Cloud Compression via Constrained Optimal Transport",
        "published_time": "2024-03-13T04:36:24Z",
        "abstract": "  This paper presents a novel point cloud compression method COT-PCC by\nformulating the task as a constrained optimal transport (COT) problem. COT-PCC\ntakes the bitrate of compressed features as an extra constraint of optimal\ntransport (OT) which learns the distribution transformation between original\nand reconstructed points. Specifically, the formulated COT is implemented with\na generative adversarial network (GAN) and a bitrate loss for training. The\ndiscriminator measures the Wasserstein distance between input and reconstructed\npoints, and a generator calculates the optimal mapping between distributions of\ninput and reconstructed point cloud. Moreover, we introduce a learnable\nsampling module for downsampling in the compression procedure. Extensive\nresults on both sparse and dense point cloud datasets demonstrate that COT-PCC\noutperforms state-of-the-art methods in terms of both CD and PSNR metrics.\nSource codes are available at \\url{https://github.com/cognaclee/PCC-COT}.\n"
    },
    {
        "title": "Matching Non-Identical Objects",
        "published_time": "2024-03-13T04:11:38Z",
        "abstract": "  Not identical but similar objects are everywhere in the world. Examples\ninclude four-legged animals such as dogs and cats, cars of different models,\nakin flowers in various colors, and countless others. In this study, we address\na novel task of matching such non-identical objects. We propose a simple\nweighting scheme of descriptors that enhance various sparse image matching\nmethods, which are originally designed for matching identical objects captured\nfrom different perspectives, and achieve semantically robust matching. The\nexperiments show successful matching between non-identical objects in various\ncases including domain shift. Further, we present a first evaluation of the\nrobustness of the image matching methods under common corruptions, which is a\nsort of domain shift, and the proposed method improves the matching in this\ncase as well.\n"
    },
    {
        "title": "REPAIR: Rank Correlation and Noisy Pair Half-replacing with Memory for\n  Noisy Correspondence",
        "published_time": "2024-03-13T04:01:20Z",
        "abstract": "  The presence of noise in acquired data invariably leads to performance\ndegradation in cross-modal matching. Unfortunately, obtaining precise\nannotations in the multimodal field is expensive, which has prompted some\nmethods to tackle the mismatched data pair issue in cross-modal matching\ncontexts, termed as noisy correspondence. However, most of these existing noisy\ncorrespondence methods exhibit the following limitations: a) the problem of\nself-reinforcing error accumulation, and b) improper handling of noisy data\npair. To tackle the two problems, we propose a generalized framework termed as\nRank corrElation and noisy Pair hAlf-replacing wIth memoRy (REPAIR), which\nbenefits from maintaining a memory bank for features of matched pairs.\nSpecifically, we calculate the distances between the features in the memory\nbank and those of the target pair for each respective modality, and use the\nrank correlation of these two sets of distances to estimate the soft\ncorrespondence label of the target pair. Estimating soft correspondence based\non memory bank features rather than using a similarity network can avoid the\naccumulation of errors due to incorrect network identifications. For pairs that\nare completely mismatched, REPAIR searches the memory bank for the most\nmatching feature to replace one feature of one modality, instead of using the\noriginal pair directly or merely discarding the mismatched pair. We conduct\nexperiments on three cross-modal datasets, i.e., Flickr30K, MSCOCO, and CC152K,\nproving the effectiveness and robustness of our REPAIR on synthetic and\nreal-world noise.\n"
    },
    {
        "title": "S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes",
        "published_time": "2024-03-10T13:04:01Z",
        "abstract": "  Current 3D stylization methods often assume static scenes, which violates the\ndynamic nature of our real world. To address this limitation, we present\nS-DyRF, a reference-based spatio-temporal stylization method for dynamic neural\nradiance fields. However, stylizing dynamic 3D scenes is inherently challenging\ndue to the limited availability of stylized reference images along the temporal\naxis. Our key insight lies in introducing additional temporal cues besides the\nprovided reference. To this end, we generate temporal pseudo-references from\nthe given stylized reference. These pseudo-references facilitate the\npropagation of style information from the reference to the entire dynamic 3D\nscene. For coarse style transfer, we enforce novel views and times to mimic the\nstyle details present in pseudo-references at the feature level. To preserve\nhigh-frequency details, we create a collection of stylized temporal pseudo-rays\nfrom temporal pseudo-references. These pseudo-rays serve as detailed and\nexplicit stylization guidance for achieving fine style transfer. Experiments on\nboth synthetic and real-world datasets demonstrate that our method yields\nplausible stylized results of space-time view synthesis on dynamic 3D scenes.\n"
    },
    {
        "title": "PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise",
        "published_time": "2024-03-13T03:28:39Z",
        "abstract": "  Normalizing flow is a generative modeling approach with efficient sampling.\nHowever, Flow-based models suffer two issues, which are manifold and discrete\ndata. If the target distribution is a manifold, which means the dimension of\nthe latent target distribution and the dimension of the data distribution are\nunmatched, flow-based models might perform badly. Discrete data makes\nflow-based models collapse into a degenerate mixture of point masses. In this\npaper, to sidestep such two issues we propose PaddingFlow, a novel\ndequantization method, which improves normalizing flows with\npadding-dimensional noise. PaddingFlow is easy to implement, computationally\ncheap, widely suitable for various tasks, and generates samples that are\nunbiased estimations of the data. Especially, our method can overcome the\nlimitation of existing dequantization methods that have to change the data\ndistribution, which might degrade performance. We validate our method on the\nmain benchmarks of unconditional density estimation, including five tabular\ndatasets and four image datasets for VAE models, and the IK experiments which\nare conditional density estimation. The results show that PaddingFlow can\nprovide improvement on all tasks in this paper.\n"
    },
    {
        "title": "LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual\n  Semantic Segmentation for Autonomous Driving",
        "published_time": "2024-03-13T03:24:36Z",
        "abstract": "  Despite the impressive performance achieved by data-fusion networks with\nduplex encoders for visual semantic segmentation, they become ineffective when\nspatial geometric data are not available. Implicitly infusing the spatial\ngeometric prior knowledge acquired by a duplex-encoder teacher model into a\nsingle-encoder student model is a practical, albeit less explored research\navenue. This paper delves into this topic and resorts to knowledge distillation\napproaches to address this problem. We introduce the Learning to Infuse \"X\"\n(LIX) framework, with novel contributions in both logit distillation and\nfeature distillation aspects. We present a mathematical proof that underscores\nthe limitation of using a single fixed weight in decoupled knowledge\ndistillation and introduce a logit-wise dynamic weight controller as a solution\nto this issue. Furthermore, we develop an adaptively-recalibrated feature\ndistillation algorithm, including two technical novelties: feature\nrecalibration via kernel regression and in-depth feature consistency\nquantification via centered kernel alignment. Extensive experiments conducted\nwith intermediate-fusion and late-fusion networks across various public\ndatasets provide both quantitative and qualitative evaluations, demonstrating\nthe superior performance of our LIX framework when compared to other\nstate-of-the-art approaches.\n"
    },
    {
        "title": "P2LHAP:Wearable sensor-based human activity recognition, segmentation\n  and forecast through Patch-to-Label Seq2Seq Transformer",
        "published_time": "2024-03-13T03:23:50Z",
        "abstract": "  Traditional deep learning methods struggle to simultaneously segment,\nrecognize, and forecast human activities from sensor data. This limits their\nusefulness in many fields such as healthcare and assisted living, where\nreal-time understanding of ongoing and upcoming activities is crucial. This\npaper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles\nall three tasks in a efficient single-task model. P2LHAP divides sensor data\nstreams into a sequence of \"patches\", served as input tokens, and outputs a\nsequence of patch-level activity labels including the predicted future\nactivities. A unique smoothing technique based on surrounding patch labels, is\nproposed to identify activity boundaries accurately. Additionally, P2LHAP\nlearns patch-level representation by sensor signal channel-independent\nTransformer encoders and decoders. All channels share embedding and Transformer\nweights across all sequences. Evaluated on three public datasets, P2LHAP\nsignificantly outperforms the state-of-the-art in all three tasks,\ndemonstrating its effectiveness and potential for real-world applications.\n"
    },
    {
        "title": "DragAnything: Motion Control for Anything using Entity Representation",
        "published_time": "2024-03-12T08:57:29Z",
        "abstract": "  We introduce DragAnything, which utilizes a entity representation to achieve\nmotion control for any object in controllable video generation. Comparison to\nexisting motion control methods, DragAnything offers several advantages.\nFirstly, trajectory-based is more userfriendly for interaction, when acquiring\nother guidance signals (e.g., masks, depth maps) is labor-intensive. Users only\nneed to draw a line (trajectory) during interaction. Secondly, our entity\nrepresentation serves as an open-domain embedding capable of representing any\nobject, enabling the control of motion for diverse entities, including\nbackground. Lastly, our entity representation allows simultaneous and distinct\nmotion control for multiple objects. Extensive experiments demonstrate that our\nDragAnything achieves state-of-the-art performance for FVD, FID, and User\nStudy, particularly in terms of object motion control, where our method\nsurpasses the previous methods (e.g., DragNUWA) by 26% in human voting.\n"
    },
    {
        "title": "Text2QR: Harmonizing Aesthetic Customization and Scanning Robustness for\n  Text-Guided QR Code Generation",
        "published_time": "2024-03-11T06:03:31Z",
        "abstract": "  In the digital era, QR codes serve as a linchpin connecting virtual and\nphysical realms. Their pervasive integration across various applications\nhighlights the demand for aesthetically pleasing codes without compromised\nscannability. However, prevailing methods grapple with the intrinsic challenge\nof balancing customization and scannability. Notably, stable-diffusion models\nhave ushered in an epoch of high-quality, customizable content generation. This\npaper introduces Text2QR, a pioneering approach leveraging these advancements\nto address a fundamental challenge: concurrently achieving user-defined\naesthetics and scanning robustness. To ensure stable generation of aesthetic QR\ncodes, we introduce the QR Aesthetic Blueprint (QAB) module, generating a\nblueprint image exerting control over the entire generation process.\nSubsequently, the Scannability Enhancing Latent Refinement (SELR) process\nrefines the output iteratively in the latent space, enhancing scanning\nrobustness. This approach harnesses the potent generation capabilities of\nstable-diffusion models, navigating the trade-off between image aesthetics and\nQR code scannability. Our experiments demonstrate the seamless fusion of visual\nappeal with the practical utility of aesthetic QR codes, markedly outperforming\nprior methods. Codes are available at \\url{https://github.com/mulns/Text2QR}\n"
    },
    {
        "title": "Advancing Security in AI Systems: A Novel Approach to Detecting\n  Backdoors in Deep Neural Networks",
        "published_time": "2024-03-13T03:10:11Z",
        "abstract": "  In the rapidly evolving landscape of communication and network security, the\nincreasing reliance on deep neural networks (DNNs) and cloud services for data\nprocessing presents a significant vulnerability: the potential for backdoors\nthat can be exploited by malicious actors. Our approach leverages advanced\ntensor decomposition algorithms Independent Vector Analysis (IVA), Multiset\nCanonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2)\nto meticulously analyze the weights of pre-trained DNNs and distinguish between\nbackdoored and clean models effectively. The key strengths of our method lie in\nits domain independence, adaptability to various network architectures, and\nability to operate without access to the training data of the scrutinized\nmodels. This not only ensures versatility across different application\nscenarios but also addresses the challenge of identifying backdoors without\nprior knowledge of the specific triggers employed to alter network behavior. We\nhave applied our detection pipeline to three distinct computer vision datasets,\nencompassing both image classification and object detection tasks. The results\ndemonstrate a marked improvement in both accuracy and efficiency over existing\nbackdoor detection methods. This advancement enhances the security of deep\nlearning and AI in networked systems, providing essential cybersecurity against\nevolving threats in emerging technologies.\n"
    },
    {
        "title": "Classes Are Not Equal: An Empirical Study on Image Recognition Fairness",
        "published_time": "2024-02-28T07:54:50Z",
        "abstract": "  In this paper, we present an empirical study on image recognition fairness,\ni.e., extreme class accuracy disparity on balanced data like ImageNet. We\nexperimentally demonstrate that classes are not equal and the fairness issue is\nprevalent for image classification models across various datasets, network\narchitectures, and model capacities. Moreover, several intriguing properties of\nfairness are identified. First, the unfairness lies in problematic\nrepresentation rather than classifier bias. Second, with the proposed concept\nof Model Prediction Bias, we investigate the origins of problematic\nrepresentation during optimization. Our findings reveal that models tend to\nexhibit greater prediction biases for classes that are more challenging to\nrecognize. It means that more other classes will be confused with harder\nclasses. Then the False Positives (FPs) will dominate the learning in\noptimization, thus leading to their poor accuracy. Further, we conclude that\ndata augmentation and representation learning algorithms improve overall\nperformance by promoting fairness to some degree in image classification. The\nCode is available at\nhttps://github.com/dvlab-research/Parametric-Contrastive-Learning.\n"
    },
    {
        "title": "AutoDFP: Automatic Data-Free Pruning via Channel Similarity\n  Reconstruction",
        "published_time": "2024-03-13T02:56:31Z",
        "abstract": "  Structured pruning methods are developed to bridge the gap between the\nmassive scale of neural networks and the limited hardware resources. Most\ncurrent structured pruning methods rely on training datasets to fine-tune the\ncompressed model, resulting in high computational burdens and being\ninapplicable for scenarios with stringent requirements on privacy and security.\nAs an alternative, some data-free methods have been proposed, however, these\nmethods often require handcraft parameter tuning and can only achieve\ninflexible reconstruction. In this paper, we propose the Automatic Data-Free\nPruning (AutoDFP) method that achieves automatic pruning and reconstruction\nwithout fine-tuning. Our approach is based on the assumption that the loss of\ninformation can be partially compensated by retaining focused information from\nsimilar channels. Specifically, We formulate data-free pruning as an\noptimization problem, which can be effectively addressed through reinforcement\nlearning. AutoDFP assesses the similarity of channels for each layer and\nprovides this information to the reinforcement learning agent, guiding the\npruning and reconstruction process of the network. We evaluate AutoDFP with\nmultiple networks on multiple datasets, achieving impressive compression\nresults. For instance, on the CIFAR-10 dataset, AutoDFP demonstrates a 2.87\\%\nreduction in accuracy loss compared to the recently proposed data-free pruning\nmethod DFPC with fewer FLOPs on VGG-16. Furthermore, on the ImageNet dataset,\nAutoDFP achieves 43.17\\% higher accuracy than the SOTA method with the same\n80\\% preserved ratio on MobileNet-V1.\n"
    },
    {
        "title": "FSViewFusion: Few-Shots View Generation of Novel Objects",
        "published_time": "2024-03-11T02:59:30Z",
        "abstract": "  Novel view synthesis has observed tremendous developments since the arrival\nof NeRFs. However, Nerf models overfit on a single scene, lacking\ngeneralization to out of distribution objects. Recently, diffusion models have\nexhibited remarkable performance on introducing generalization in view\nsynthesis. Inspired by these advancements, we explore the capabilities of a\npretrained stable diffusion model for view synthesis without explicit 3D\npriors. Specifically, we base our method on a personalized text to image model,\nDreambooth, given its strong ability to adapt to specific novel objects with a\nfew shots. Our research reveals two interesting findings. First, we observe\nthat Dreambooth can learn the high level concept of a view, compared to\narguably more complex strategies which involve finetuning diffusions on large\namounts of multi-view data. Second, we establish that the concept of a view can\nbe disentangled and transferred to a novel object irrespective of the original\nobject's identify from which the views are learnt. Motivated by this, we\nintroduce a learning strategy, FSViewFusion, which inherits a specific view\nthrough only one image sample of a single scene, and transfers the knowledge to\na novel object, learnt from few shots, using low rank adapters. Through\nextensive experiments we demonstrate that our method, albeit simple, is\nefficient in generating reliable view samples for in the wild images. Code and\nmodels will be released.\n"
    },
    {
        "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO\n  Systems",
        "published_time": "2024-03-12T06:28:41Z",
        "abstract": "  This paper presents a finite-rate deep-learning (DL)-based channel state\ninformation (CSI) feedback method for massive multiple-input multiple-output\n(MIMO) systems. The presented method provides a finite-bit representation of\nthe latent vector based on a vector-quantized variational autoencoder (VQ-VAE)\nframework while reducing its computational complexity based on shape-gain\nvector quantization. In this method, the magnitude of the latent vector is\nquantized using a non-uniform scalar codebook with a proper transformation\nfunction, while the direction of the latent vector is quantized using a\ntrainable Grassmannian codebook. A multi-rate codebook design strategy is also\ndeveloped by introducing a codeword selection rule for a nested codebook along\nwith the design of a loss function. Simulation results demonstrate that the\nproposed method reduces the computational complexity associated with VQ-VAE\nwhile improving CSI reconstruction performance under a given feedback overhead.\n"
    },
    {
        "title": "SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph\n  Attention",
        "published_time": "2024-03-13T02:11:04Z",
        "abstract": "  3D visual grounding aims to automatically locate the 3D region of the\nspecified object given the corresponding textual description. Existing works\nfail to distinguish similar objects especially when multiple referred objects\nare involved in the description. Experiments show that direct matching of\nlanguage and visual modal has limited capacity to comprehend complex\nreferential relationships in utterances. It is mainly due to the interference\ncaused by redundant visual information in cross-modal alignment. To strengthen\nrelation-orientated mapping between different modalities, we propose SeCG, a\nsemantic-enhanced relational learning model based on a graph network with our\ndesigned memory graph attention layer. Our method replaces original\nlanguage-independent encoding with cross-modal encoding in visual analysis.\nMore text-related feature expressions are obtained through the guidance of\nglobal semantics and implicit relationships. Experimental results on ReferIt3D\nand ScanRefer benchmarks show that the proposed method outperforms the existing\nstate-of-the-art methods, particularly improving the localization performance\nfor the multi-relation challenges.\n"
    },
    {
        "title": "Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small\n  Language Models",
        "published_time": "2024-03-10T12:43:27Z",
        "abstract": "  Multimodal Large Language Models (MLLMs) have showcased impressive skills in\ntasks related to visual understanding and reasoning. Yet, their widespread\napplication faces obstacles due to the high computational demands during both\nthe training and inference phases, restricting their use to a limited audience\nwithin the research and user communities. In this paper, we investigate the\ndesign aspects of Multimodal Small Language Models (MSLMs) and propose an\nefficient multimodal assistant named Mipha, which is designed to create synergy\namong various aspects: visual representation, language models, and optimization\nstrategies. We show that without increasing the volume of training data, our\nMipha-3B outperforms the state-of-the-art large MLLMs, especially\nLLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide\ninsights and guidelines for developing strong MSLMs that rival the capabilities\nof MLLMs. Our code is available at https://github.com/zhuyiche/Mipha.\n"
    },
    {
        "title": "Versatile Defense Against Adversarial Attacks on Image Recognition",
        "published_time": "2024-03-13T01:48:01Z",
        "abstract": "  Adversarial attacks present a significant security risk to image recognition\ntasks. Defending against these attacks in a real-life setting can be compared\nto the way antivirus software works, with a key consideration being how well\nthe defense can adapt to new and evolving attacks. Another important factor is\nthe resources involved in terms of time and cost for training defense models\nand updating the model database. Training many models that are specific to each\ntype of attack can be time-consuming and expensive. Ideally, we should be able\nto train one single model that can handle a wide range of attacks. It appears\nthat a defense method based on image-to-image translation may be capable of\nthis. The proposed versatile defense approach in this paper only requires\ntraining one model to effectively resist various unknown adversarial attacks.\nThe trained model has successfully improved the classification accuracy from\nnearly zero to an average of 86%, performing better than other defense methods\nproposed in prior studies. When facing the PGD attack and the MI-FGSM attack,\nversatile defense model even outperforms the attack-specific models trained\nbased on these two attacks. The robustness check also shows that our versatile\ndefense model performs stably regardless with the attack strength.\n"
    },
    {
        "title": "MambaMorph: a Mamba-based Framework for Medical MR-CT Deformable\n  Registration",
        "published_time": "2024-01-25T04:16:45Z",
        "abstract": "  Capturing voxel-wise spatial correspondence across distinct modalities is\ncrucial for medical image analysis. However, current registration approaches\nare not practical enough in terms of registration accuracy and clinical\napplicability. In this paper, we introduce MambaMorph, a novel multi-modality\ndeformable registration framework. Specifically, MambaMorph utilizes a\nMamba-based registration module and a fine-grained, yet simple, feature\nextractor for efficient long-range correspondence modeling and high-dimensional\nfeature learning, respectively. Additionally, we develop a well-annotated brain\nMR-CT registration dataset, SR-Reg, to address the scarcity of data in\nmulti-modality registration. To validate MambaMorph's multi-modality\nregistration capabilities, we conduct quantitative experiments on both our\nSR-Reg dataset and a public T1-T2 dataset. The experimental results on both\ndatasets demonstrate that MambaMorph significantly outperforms the current\nstate-of-the-art learning-based registration methods in terms of registration\naccuracy. Further study underscores the efficiency of the Mamba-based\nregistration module and the lightweight feature extractor, which achieve\nnotable registration quality while maintaining reasonable computational costs\nand speeds. We believe that MambaMorph holds significant potential for\npractical applications in medical image registration. The code for MambaMorph\nis available at: https://github.com/Guo-Stone/MambaMorph.\n"
    },
    {
        "title": "Say Anything with Any Style",
        "published_time": "2024-03-11T01:20:03Z",
        "abstract": "  Generating stylized talking head with diverse head motions is crucial for\nachieving natural-looking videos but still remains challenging. Previous works\neither adopt a regressive method to capture the speaking style, resulting in a\ncoarse style that is averaged across all training data, or employ a universal\nnetwork to synthesize videos with different styles which causes suboptimal\nperformance. To address these, we propose a novel dynamic-weight method, namely\nSay Anything withAny Style (SAAS), which queries the discrete style\nrepresentation via a generative model with a learned style codebook.\nSpecifically, we develop a multi-task VQ-VAE that incorporates three closely\nrelated tasks to learn a style codebook as a prior for style extraction. This\ndiscrete prior, along with the generative model, enhances the precision and\nrobustness when extracting the speaking styles of the given style clips. By\nutilizing the extracted style, a residual architecture comprising a canonical\nbranch and style-specific branch is employed to predict the mouth shapes\nconditioned on any driving audio while transferring the speaking style from the\nsource to any desired one. To adapt to different speaking styles, we steer\nclear of employing a universal network by exploring an elaborate HyperStyle to\nproduce the style-specific weights offset for the style branch. Furthermore, we\nconstruct a pose generator and a pose codebook to store the quantized pose\nrepresentation, allowing us to sample diverse head motions aligned with the\naudio and the extracted style. Experiments demonstrate that our approach\nsurpasses state-of-theart methods in terms of both lip-synchronization and\nstylized expression. Besides, we extend our SAAS to video-driven style editing\nfield and achieve satisfactory performance.\n"
    },
    {
        "title": "SemCity: Semantic Scene Generation with Triplane Diffusion",
        "published_time": "2024-03-12T15:59:08Z",
        "abstract": "  We present \"SemCity,\" a 3D diffusion model for semantic scene generation in\nreal-world outdoor environments. Most 3D diffusion models focus on generating a\nsingle object, synthetic indoor scenes, or synthetic outdoor scenes, while the\ngeneration of real-world outdoor scenes is rarely addressed. In this paper, we\nconcentrate on generating a real-outdoor scene through learning a diffusion\nmodel on a real-world outdoor dataset. In contrast to synthetic data,\nreal-outdoor datasets often contain more empty spaces due to sensor\nlimitations, causing challenges in learning real-outdoor distributions. To\naddress this issue, we exploit a triplane representation as a proxy form of\nscene distributions to be learned by our diffusion model. Furthermore, we\npropose a triplane manipulation that integrates seamlessly with our triplane\ndiffusion model. The manipulation improves our diffusion model's applicability\nin a variety of downstream tasks related to outdoor scene generation such as\nscene inpainting, scene outpainting, and semantic scene completion refinements.\nIn experimental results, we demonstrate that our triplane diffusion model shows\nmeaningful generation results compared with existing work in a real-outdoor\ndataset, SemanticKITTI. We also show our triplane manipulation facilitates\nseamlessly adding, removing, or modifying objects within a scene. Further, it\nalso enables the expansion of scenes toward a city-level scale. Finally, we\nevaluate our method on semantic scene completion refinements where our\ndiffusion model enhances predictions of semantic scene completion networks by\nlearning scene distribution. Our code is available at\nhttps://github.com/zoomin-lee/SemCity.\n"
    },
    {
        "title": "OccFusion: A Straightforward and Effective Multi-Sensor Fusion Framework\n  for 3D Occupancy Prediction",
        "published_time": "2024-03-03T23:46:06Z",
        "abstract": "  This paper introduces OccFusion, a straightforward and efficient sensor\nfusion framework for predicting 3D occupancy. A comprehensive understanding of\n3D scenes is crucial in autonomous driving, and recent models for 3D semantic\noccupancy prediction have successfully addressed the challenge of describing\nreal-world objects with varied shapes and classes. However, existing methods\nfor 3D occupancy prediction heavily rely on surround-view camera images, making\nthem susceptible to changes in lighting and weather conditions. By integrating\nfeatures from additional sensors, such as lidar and surround view radars, our\nframework enhances the accuracy and robustness of occupancy prediction,\nresulting in top-tier performance on the nuScenes benchmark. Furthermore,\nextensive experiments conducted on the nuScenes dataset, including challenging\nnight and rainy scenarios, confirm the superior performance of our sensor\nfusion strategy across various perception ranges. The code for this framework\nwill be made available at https://github.com/DanielMing123/OCCFusion.\n"
    },
    {
        "title": "Iterative Learning for Joint Image Denoising and Motion Artifact\n  Correction of 3D Brain MRI",
        "published_time": "2024-03-13T01:18:55Z",
        "abstract": "  Image noise and motion artifacts greatly affect the quality of brain MRI and\nnegatively influence downstream medical image analysis. Previous studies often\nfocus on 2D methods that process each volumetric MR image slice-by-slice, thus\nlosing important 3D anatomical information. Additionally, these studies\ngenerally treat image denoising and artifact correction as two standalone\ntasks, without considering their potential relationship, especially on\nlow-quality images where severe noise and motion artifacts occur\nsimultaneously. To address these issues, we propose a Joint image Denoising and\nmotion Artifact Correction (JDAC) framework via iterative learning to handle\nnoisy MRIs with motion artifacts, consisting of an adaptive denoising model and\nan anti-artifact model. In the adaptive denoising model, we first design a\nnovel noise level estimation strategy, and then adaptively reduce the noise\nthrough a U-Net backbone with feature normalization conditioning on the\nestimated noise variance. The anti-artifact model employs another U-Net for\neliminating motion artifacts, incorporating a novel gradient-based loss\nfunction designed to maintain the integrity of brain anatomy during the motion\ncorrection process. These two models are iteratively employed for joint image\ndenoising and artifact correction through an iterative learning framework. An\nearly stopping strategy depending on noise level estimation is applied to\naccelerate the iteration process. The denoising model is trained with 9,544\nT1-weighted MRIs with manually added Gaussian noise as supervision. The\nanti-artifact model is trained on 552 T1-weighted MRIs with motion artifacts\nand paired motion-free images. Experimental results on a public dataset and a\nclinical study suggest the effectiveness of JDAC in both tasks of denoising and\nmotion artifact correction, compared with several state-of-the-art methods.\n"
    },
    {
        "title": "LAFS: Landmark-based Facial Self-supervised Learning for Face\n  Recognition",
        "published_time": "2024-03-13T01:07:55Z",
        "abstract": "  In this work we focus on learning facial representations that can be adapted\nto train effective face recognition models, particularly in the absence of\nlabels. Firstly, compared with existing labelled face datasets, a vastly larger\nmagnitude of unlabeled faces exists in the real world. We explore the learning\nstrategy of these unlabeled facial images through self-supervised pretraining\nto transfer generalized face recognition performance. Moreover, motivated by\none recent finding, that is, the face saliency area is critical for face\nrecognition, in contrast to utilizing random cropped blocks of images for\nconstructing augmentations in pretraining, we utilize patches localized by\nextracted facial landmarks. This enables our method - namely LAndmark-based\nFacial Self-supervised learning LAFS), to learn key representation that is more\ncritical for face recognition. We also incorporate two landmark-specific\naugmentations which introduce more diversity of landmark information to further\nregularize the learning. With learned landmark-based facial representations, we\nfurther adapt the representation for face recognition with regularization\nmitigating variations in landmark positions. Our method achieves significant\nimprovement over the state-of-the-art on multiple face recognition benchmarks,\nespecially on more challenging few-shot scenarios.\n"
    },
    {
        "title": "Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious\n  Challenges in Multimodal Reasoning",
        "published_time": "2024-03-06T17:15:04Z",
        "abstract": "  This paper introduces the novel task of multimodal puzzle solving, framed\nwithin the context of visual question-answering. We present a new dataset,\nAlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal\nlanguage models in solving algorithmic puzzles that necessitate both visual\nunderstanding, language understanding, and complex algorithmic reasoning. We\ncreate the puzzles to encompass a diverse array of mathematical and algorithmic\ntopics such as boolean logic, combinatorics, graph theory, optimization,\nsearch, etc., aiming to evaluate the gap between visual data interpretation and\nalgorithmic problem-solving skills. The dataset is generated automatically from\ncode authored by humans. All our puzzles have exact solutions that can be found\nfrom the algorithm without tedious human calculations. It ensures that our\ndataset can be scaled up arbitrarily in terms of reasoning complexity and\ndataset size. Our investigation reveals that large language models (LLMs) such\nas GPT4V and Gemini exhibit limited performance in puzzle-solving tasks. We\nfind that their performance is near random in a multi-choice question-answering\nsetup for a significant number of puzzles. The findings emphasize the\nchallenges of integrating visual, language, and algorithmic knowledge for\nsolving complex reasoning problems.\n"
    },
    {
        "title": "Multiscale Low-Frequency Memory Network for Improved Feature Extraction\n  in Convolutional Neural Networks",
        "published_time": "2024-03-13T00:48:41Z",
        "abstract": "  Deep learning and Convolutional Neural Networks (CNNs) have driven major\ntransformations in diverse research areas. However, their limitations in\nhandling low-frequency information present obstacles in certain tasks like\ninterpreting global structures or managing smooth transition images. Despite\nthe promising performance of transformer structures in numerous tasks, their\nintricate optimization complexities highlight the persistent need for refined\nCNN enhancements using limited resources. Responding to these complexities, we\nintroduce a novel framework, the Multiscale Low-Frequency Memory (MLFM)\nNetwork, with the goal to harness the full potential of CNNs while keeping\ntheir complexity unchanged. The MLFM efficiently preserves low-frequency\ninformation, enhancing performance in targeted computer vision tasks. Central\nto our MLFM is the Low-Frequency Memory Unit (LFMU), which stores various\nlow-frequency data and forms a parallel channel to the core network. A key\nadvantage of MLFM is its seamless compatibility with various prevalent\nnetworks, requiring no alterations to their original core structure. Testing on\nImageNet demonstrated substantial accuracy improvements in multiple 2D CNNs,\nincluding ResNet, MobileNet, EfficientNet, and ConvNeXt. Furthermore, we\nshowcase MLFM's versatility beyond traditional image classification by\nsuccessfully integrating it into image-to-image translation tasks, specifically\nin semantic segmentation networks like FCN and U-Net. In conclusion, our work\nsignifies a pivotal stride in the journey of optimizing the efficacy and\nefficiency of CNNs with limited resources. This research builds upon the\nexisting CNN foundations and paves the way for future advancements in computer\nvision. Our codes are available at https://github.com/AlphaWuSeu/ MLFM.\n"
    },
    {
        "title": "NeRF-Supervised Feature Point Detection and Description",
        "published_time": "2024-03-13T00:43:10Z",
        "abstract": "  Feature point detection and description is the backbone for various computer\nvision applications, such as Structure-from-Motion, visual SLAM, and visual\nplace recognition. While learning-based methods have surpassed traditional\nhandcrafted techniques, their training often relies on simplistic\nhomography-based simulations of multi-view perspectives, limiting model\ngeneralisability. This paper introduces a novel approach leveraging neural\nradiance fields (NeRFs) for realistic multi-view training data generation. We\ncreate a diverse multi-view dataset using NeRFs, consisting of indoor and\noutdoor scenes. Our proposed methodology adapts state-of-the-art feature\ndetectors and descriptors to train on NeRF-synthesised views supervised by\nperspective projective geometry. Our experiments demonstrate that the proposed\nmethods achieve competitive or superior performance on standard benchmarks for\nrelative pose estimation, point cloud registration, and homography estimation\nwhile requiring significantly less training data compared to existing\napproaches.\n"
    },
    {
        "title": "SWAP-NAS: Sample-Wise Activation Patterns for Ultra-fast NAS",
        "published_time": "2024-03-07T02:40:42Z",
        "abstract": "  Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid\nresource-intensive neural network training, especially in Neural Architecture\nSearch (NAS). Recent studies show that existing training-free metrics have\nseveral limitations, such as limited correlation and poor generalisation across\ndifferent search spaces and tasks. Hence, we propose Sample-Wise Activation\nPatterns and its derivative, SWAP-Score, a novel high-performance training-free\nmetric. It measures the expressivity of networks over a batch of input samples.\nThe SWAP-Score is strongly correlated with ground-truth performance across\nvarious search spaces and tasks, outperforming 15 existing training-free\nmetrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be\nfurther enhanced by regularisation, which leads to even higher correlations in\ncell-based search space and enables model size control during the search. For\nexample, Spearman's rank correlation coefficient between regularised SWAP-Score\nand CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90,\nsignificantly higher than 0.80 from the second-best metric, NWOT. When\nintegrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves\ncompetitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and\n9 minutes of GPU time respectively.\n"
    },
    {
        "title": "ShadowRemovalNet: Efficient Real-Time Shadow Removal",
        "published_time": "2024-03-13T00:04:07Z",
        "abstract": "  Shadows significantly impact computer vision tasks, particularly in outdoor\nenvironments. State-of-the-art shadow removal methods are typically too\ncomputationally intensive for real-time image processing on edge hardware. We\npropose ShadowRemovalNet, a novel method designed for real-time image\nprocessing on resource-constrained hardware. ShadowRemovalNet achieves\nsignificantly higher frame rates compared to existing methods, making it\nsuitable for real-time computer vision pipelines like those used in field\nrobotics. Beyond speed, ShadowRemovalNet offers advantages in efficiency and\nsimplicity, as it does not require a separate shadow mask during inference.\nShadowRemovalNet also addresses challenges associated with Generative\nAdversarial Networks (GANs) for shadow removal, including artefacts, inaccurate\nmask estimations, and inconsistent supervision between shadow and boundary\npixels. To address these limitations, we introduce a novel loss function that\nsubstantially reduces shadow removal errors. ShadowRemovalNet's efficiency and\nstraightforwardness make it a robust and effective solution for real-time\nshadow removal in outdoor robotics and edge computing applications.\n"
    },
    {
        "title": "Q-SLAM: Quadric Representations for Monocular SLAM",
        "published_time": "2024-03-12T23:27:30Z",
        "abstract": "  Monocular SLAM has long grappled with the challenge of accurately modeling 3D\ngeometries. Recent advances in Neural Radiance Fields (NeRF)-based monocular\nSLAM have shown promise, yet these methods typically focus on novel view\nsynthesis rather than precise 3D geometry modeling. This focus results in a\nsignificant disconnect between NeRF applications, i.e., novel-view synthesis\nand the requirements of SLAM. We identify that the gap results from the\nvolumetric representations used in NeRF, which are often dense and noisy. In\nthis study, we propose a novel approach that reimagines volumetric\nrepresentations through the lens of quadric forms. We posit that most scene\ncomponents can be effectively represented as quadric planes. Leveraging this\nassumption, we reshape the volumetric representations with million of cubes by\nseveral quadric planes, which leads to more accurate and efficient modeling of\n3D scenes in SLAM contexts. Our method involves two key steps: First, we use\nthe quadric assumption to enhance coarse depth estimations obtained from\ntracking modules, e.g., Droid-SLAM. This step alone significantly improves\ndepth estimation accuracy. Second, in the subsequent mapping phase, we diverge\nfrom previous NeRF-based SLAM methods that distribute sampling points across\nthe entire volume space. Instead, we concentrate sampling points around quadric\nplanes and aggregate them using a novel quadric-decomposed Transformer.\nAdditionally, we introduce an end-to-end joint optimization strategy that\nsynchronizes pose estimation with 3D reconstruction.\n"
    },
    {
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "published_time": "2024-01-03T08:33:09Z",
        "abstract": "  The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents -- it\ncan successfully complete 51.1 of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out to be not effective for web agents, and the best grounding strategy\nwe develop in this paper leverages both the HTML structure and visuals. Yet,\nthere is still a substantial gap with oracle grounding, leaving ample room for\nfurther improvement. All code, data, and evaluation tools are available at\nhttps://github.com/OSU-NLP-Group/SeeAct.\n"
    },
    {
        "title": "CMax-SLAM: Event-based Rotational-Motion Bundle Adjustment and SLAM\n  System using Contrast Maximization",
        "published_time": "2024-03-12T23:05:10Z",
        "abstract": "  Event cameras are bio-inspired visual sensors that capture pixel-wise\nintensity changes and output asynchronous event streams. They show great\npotential over conventional cameras to handle challenging scenarios in robotics\nand computer vision, such as high-speed and high dynamic range. This paper\nconsiders the problem of rotational motion estimation using event cameras.\nSeveral event-based rotation estimation methods have been developed in the past\ndecade, but their performance has not been evaluated and compared under unified\ncriteria yet. In addition, these prior works do not consider a global\nrefinement step. To this end, we conduct a systematic study of this problem\nwith two objectives in mind: summarizing previous works and presenting our own\nsolution. First, we compare prior works both theoretically and experimentally.\nSecond, we propose the first event-based rotation-only bundle adjustment (BA)\napproach. We formulate it leveraging the state-of-the-art Contrast Maximization\n(CMax) framework, which is principled and avoids the need to convert events\ninto frames. Third, we use the proposed BA to build CMax-SLAM, the first\nevent-based rotation-only SLAM system comprising a front-end and a back-end.\nOur BA is able to run both offline (trajectory smoothing) and online (CMax-SLAM\nback-end). To demonstrate the performance and versatility of our method, we\npresent comprehensive experiments on synthetic and real-world datasets,\nincluding indoor, outdoor and space scenarios. We discuss the pitfalls of\nreal-world evaluation and propose a proxy for the reprojection error as the\nfigure of merit to evaluate event-based rotation BA methods. We release the\nsource code and novel data sequences to benefit the community. We hope this\nwork leads to a better understanding and fosters further research on\nevent-based ego-motion estimation. Project page:\nhttps://github.com/tub-rip/cmax_slam\n"
    },
    {
        "title": "VANP: Learning Where to See for Navigation with Self-Supervised\n  Vision-Action Pre-Training",
        "published_time": "2024-03-12T22:33:08Z",
        "abstract": "  Humans excel at efficiently navigating through crowds without collision by\nfocusing on specific visual regions relevant to navigation. However, most\nrobotic visual navigation methods rely on deep learning models pre-trained on\nvision tasks, which prioritize salient objects -- not necessarily relevant to\nnavigation and potentially misleading. Alternative approaches train specialized\nnavigation models from scratch, requiring significant computation. On the other\nhand, self-supervised learning has revolutionized computer vision and natural\nlanguage processing, but its application to robotic navigation remains\nunderexplored due to the difficulty of defining effective self-supervision\nsignals. Motivated by these observations, in this work, we propose a\nSelf-Supervised Vision-Action Model for Visual Navigation Pre-Training (VANP).\nInstead of detecting salient objects that are beneficial for tasks such as\nclassification or detection, VANP learns to focus only on specific visual\nregions that are relevant to the navigation task. To achieve this, VANP uses a\nhistory of visual observations, future actions, and a goal image for\nself-supervision, and embeds them using two small Transformer Encoders. Then,\nVANP maximizes the information between the embeddings by using a mutual\ninformation maximization objective function. We demonstrate that most\nVANP-extracted features match with human navigation intuition. VANP achieves\ncomparable performance as models learned end-to-end with half the training time\nand models trained on a large-scale, fully supervised dataset, i.e., ImageNet,\nwith only 0.08% data.\n"
    },
    {
        "title": "TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object\n  Detection",
        "published_time": "2024-03-12T22:33:02Z",
        "abstract": "  Task-oriented object detection aims to find objects suitable for\naccomplishing specific tasks. As a challenging task, it requires simultaneous\nvisual data processing and reasoning under ambiguous semantics. Recent\nsolutions are mainly all-in-one models. However, the object detection backbones\nare pre-trained without text supervision. Thus, to incorporate task\nrequirements, their intricate models undergo extensive learning on a highly\nimbalanced and scarce dataset, resulting in capped performance, laborious\ntraining, and poor generalizability. In contrast, we propose TaskCLIP, a more\nnatural two-stage design composed of general object detection and task-guided\nobject selection. Particularly for the latter, we resort to the recently\nsuccessful large Vision-Language Models (VLMs) as our backbone, which provides\nrich semantic knowledge and a uniform embedding space for images and texts.\nNevertheless, the naive application of VLMs leads to sub-optimal quality, due\nto the misalignment between embeddings of object images and their visual\nattributes, which are mainly adjective phrases. To this end, we design a\ntransformer-based aligner after the pre-trained VLMs to re-calibrate both\nembeddings. Finally, we employ a trainable score function to post-process the\nVLM matching results for object selection. Experimental results demonstrate\nthat our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by\n3.5% and only requires a single NVIDIA RTX 4090 for both training and\ninference.\n"
    },
    {
        "title": "A dataset of over one thousand computed tomography scans of battery\n  cells",
        "published_time": "2024-03-04T22:42:17Z",
        "abstract": "  Battery technology is increasingly important for global electrification\nefforts. However, batteries are highly sensitive to small manufacturing\nvariations that can induce reliability or safety issues. An important\ntechnology for battery quality control is computed tomography (CT) scanning,\nwhich is widely used for non-destructive 3D inspection across a variety of\nclinical and industrial applications. Historically, however, the utility of CT\nscanning for high-volume manufacturing has been limited by its low throughput\nas well as the difficulty of handling its large file sizes. In this work, we\npresent a dataset of over one thousand CT scans of as-produced commercially\navailable batteries. The dataset spans various chemistries (lithium-ion and\nsodium-ion) as well as various battery form factors (cylindrical, pouch, and\nprismatic). We evaluate seven different battery types in total. The\nmanufacturing variability and the presence of battery defects can be observed\nvia this dataset. This dataset may be of interest to scientists and engineers\nworking on battery technology, computer vision, or both.\n"
    },
    {
        "title": "Mitigating the Impact of Attribute Editing on Face Recognition",
        "published_time": "2024-03-12T22:03:19Z",
        "abstract": "  Facial attribute editing using generative models can impair automated face\nrecognition. This degradation persists even with recent identity-preserving\nmodels such as InstantID. To mitigate this issue, we propose two techniques\nthat perform local and global attribute editing. Local editing operates on the\nfiner details via a regularization-free method based on ControlNet conditioned\non depth maps and auxiliary semantic segmentation masks. Global editing\noperates on coarser details via a regularization-based method guided by custom\nloss and regularization set. In this work, we empirically ablate twenty-six\nfacial semantic, demographic and expression-based attributes altered using\nstate-of-the-art generative models and evaluate them using ArcFace and AdaFace\nmatchers on CelebA, CelebAMaskHQ and LFW datasets. Finally, we use LLaVA, a\nvision-language framework for attribute prediction to validate our editing\ntechniques. Our methods outperform SoTA (BLIP, InstantID) at facial editing\nwhile retaining identity.\n"
    },
    {
        "title": "Flow-Based Visual Stream Compression for Event Cameras",
        "published_time": "2024-03-12T21:36:41Z",
        "abstract": "  As the use of neuromorphic, event-based vision sensors expands, the need for\ncompression of their output streams has increased. While their operational\nprinciple ensures event streams are spatially sparse, the high temporal\nresolution of the sensors can result in high data rates from the sensor\ndepending on scene dynamics. For systems operating in\ncommunication-bandwidth-constrained and power-constrained environments, it is\nessential to compress these streams before transmitting them to a remote\nreceiver. Therefore, we introduce a flow-based method for the real-time\nasynchronous compression of event streams as they are generated. This method\nleverages real-time optical flow estimates to predict future events without\nneeding to transmit them, therefore, drastically reducing the amount of data\ntransmitted. The flow-based compression introduced is evaluated using a variety\nof methods including spatiotemporal distance between event streams. The\nintroduced method itself is shown to achieve an average compression ratio of\n2.81 on a variety of event-camera datasets with the evaluation configuration\nused. That compression is achieved with a median temporal error of 0.48 ms and\nan average spatiotemporal event-stream distance of 3.07. When combined with\nLZMA compression for non-real-time applications, our method can achieve\nstate-of-the-art average compression ratios ranging from 10.45 to 17.24.\nAdditionally, we demonstrate that the proposed prediction algorithm is capable\nof performing real-time, low-latency event prediction.\n"
    },
    {
        "title": "A Multimodal Intermediate Fusion Network with Manifold Learning for\n  Stress Detection",
        "published_time": "2024-03-12T21:06:19Z",
        "abstract": "  Multimodal deep learning methods capture synergistic features from multiple\nmodalities and have the potential to improve accuracy for stress detection\ncompared to unimodal methods. However, this accuracy gain typically comes from\nhigh computational cost due to the high-dimensional feature spaces, especially\nfor intermediate fusion. Dimensionality reduction is one way to optimize\nmultimodal learning by simplifying data and making the features more amenable\nto processing and analysis, thereby reducing computational complexity. This\npaper introduces an intermediate multimodal fusion network with manifold\nlearning-based dimensionality reduction. The multimodal network generates\nindependent representations from biometric signals and facial landmarks through\n1D-CNN and 2D-CNN. Finally, these features are fused and fed to another 1D-CNN\nlayer, followed by a fully connected dense layer. We compared various\ndimensionality reduction techniques for different variations of unimodal and\nmultimodal networks. We observe that the intermediate-level fusion with the\nMulti-Dimensional Scaling (MDS) manifold method showed promising results with\nan accuracy of 96.00\\% in a Leave-One-Subject-Out Cross-Validation (LOSO-CV)\nparadigm over other dimensional reduction methods. MDS had the highest\ncomputational cost among manifold learning methods. However, while\noutperforming other networks, it managed to reduce the computational cost of\nthe proposed networks by 25\\% when compared to six well-known conventional\nfeature selection methods used in the preprocessing step.\n"
    },
    {
        "title": "Dynamic Cross Attention for Audio-Visual Person Verification",
        "published_time": "2024-03-07T17:07:51Z",
        "abstract": "  Although person or identity verification has been predominantly explored\nusing individual modalities such as face and voice, audio-visual fusion has\nrecently shown immense potential to outperform unimodal approaches. Audio and\nvisual modalities are often expected to pose strong complementary\nrelationships, which plays a crucial role in effective audio-visual fusion.\nHowever, they may not always strongly complement each other, they may also\nexhibit weak complementary relationships, resulting in poor audio-visual\nfeature representations. In this paper, we propose a Dynamic Cross-Attention\n(DCA) model that can dynamically select the cross-attended or unattended\nfeatures on the fly based on the strong or weak complementary relationships,\nrespectively, across audio and visual modalities. In particular, a conditional\ngating layer is designed to evaluate the contribution of the cross-attention\nmechanism and choose cross-attended features only when they exhibit strong\ncomplementary relationships, otherwise unattended features. Extensive\nexperiments are conducted on the Voxceleb1 dataset to demonstrate the\nrobustness of the proposed model. Results indicate that the proposed model\nconsistently improves the performance on multiple variants of cross-attention\nwhile outperforming the state-of-the-art methods.\n"
    },
    {
        "title": "Audio-Visual Person Verification based on Recursive Fusion of Joint\n  Cross-Attention",
        "published_time": "2024-03-07T16:57:45Z",
        "abstract": "  Person or identity verification has been recently gaining a lot of attention\nusing audio-visual fusion as faces and voices share close associations with\neach other. Conventional approaches based on audio-visual fusion rely on\nscore-level or early feature-level fusion techniques. Though existing\napproaches showed improvement over unimodal systems, the potential of\naudio-visual fusion for person verification is not fully exploited. In this\npaper, we have investigated the prospect of effectively capturing both the\nintra- and inter-modal relationships across audio and visual modalities, which\ncan play a crucial role in significantly improving the fusion performance over\nunimodal systems. In particular, we introduce a recursive fusion of a joint\ncross-attentional model, where a joint audio-visual feature representation is\nemployed in the cross-attention framework in a recursive fashion to\nprogressively refine the feature representations that can efficiently capture\nthe intra-and inter-modal relationships. To further enhance the audio-visual\nfeature representations, we have also explored BLSTMs to improve the temporal\nmodeling of audio-visual feature representations. Extensive experiments are\nconducted on the Voxceleb1 dataset to evaluate the proposed model. Results\nindicate that the proposed model shows promising improvement in fusion\nperformance by adeptly capturing the intra-and inter-modal relationships across\naudio and visual modalities.\n"
    },
    {
        "title": "FluoroSAM: A Language-aligned Foundation Model for X-ray Image\n  Segmentation",
        "published_time": "2024-03-12T20:11:38Z",
        "abstract": "  Automated X-ray image segmentation would accelerate research and development\nin diagnostic and interventional precision medicine. Prior efforts have\ncontributed task-specific models capable of solving specific image analysis\nproblems, but the utility of these models is restricted to their particular\ntask domain, and expanding to broader use requires additional data, labels, and\nretraining efforts. Recently, foundation models (FMs) -- machine learning\nmodels trained on large amounts of highly variable data thus enabling broad\napplicability -- have emerged as promising tools for automated image analysis.\nExisting FMs for medical image analysis focus on scenarios and modalities where\nobjects are clearly defined by visually apparent boundaries, such as surgical\ntool segmentation in endoscopy. X-ray imaging, by contrast, does not generally\noffer such clearly delineated boundaries or structure priors. During X-ray\nimage formation, complex 3D structures are projected in transmission onto the\nimaging plane, resulting in overlapping features of varying opacity and shape.\nTo pave the way toward an FM for comprehensive and automated analysis of\narbitrary medical X-ray images, we develop FluoroSAM, a language-aligned\nvariant of the Segment-Anything Model, trained from scratch on 1.6M synthetic\nX-ray images. FluoroSAM is trained on data including masks for 128 organ types\nand 464 non-anatomical objects, such as tools and implants. In real X-ray\nimages of cadaveric specimens, FluoroSAM is able to segment bony anatomical\nstructures based on text-only prompting with 0.51 and 0.79 DICE with\npoint-based refinement, outperforming competing SAM variants for all\nstructures. FluoroSAM is also capable of zero-shot generalization to segmenting\nclasses beyond the training set thanks to its language alignment, which we\ndemonstrate for full lung segmentation on real chest X-rays.\n"
    },
    {
        "title": "CT evaluation of 2D and 3D holistic deep learning methods for the\n  volumetric segmentation of airway lesions",
        "published_time": "2024-03-12T19:34:50Z",
        "abstract": "  This research embarked on a comparative exploration of the holistic\nsegmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D\nand 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized\ndata from two CF reference centers, covering five major CF structural changes.\nInitially, it compared the 2D and 3D models, highlighting the 3D model's\nsuperior capability in capturing complex features like mucus plugs and\nconsolidations. To improve the 2D model's performance, a loss adapted to fine\nstructures segmentation was implemented and evaluated, significantly enhancing\nits accuracy, though not surpassing the 3D model's performance. The models\nunderwent further validation through external evaluation against pulmonary\nfunction tests (PFTs), confirming the robustness of the findings. Moreover,\nthis study went beyond comparing metrics; it also included comprehensive\nassessments of the models' interpretability and reliability, providing valuable\ninsights for their clinical application.\n"
    },
    {
        "title": "LG-Traj: LLM Guided Pedestrian Trajectory Prediction",
        "published_time": "2024-03-12T19:06:23Z",
        "abstract": "  Accurate pedestrian trajectory prediction is crucial for various\napplications, and it requires a deep understanding of pedestrian motion\npatterns in dynamic environments. However, existing pedestrian trajectory\nprediction methods still need more exploration to fully leverage these motion\npatterns. This paper investigates the possibilities of using Large Language\nModels (LLMs) to improve pedestrian trajectory prediction tasks by inducing\nmotion cues. We introduce LG-Traj, a novel approach incorporating LLMs to\ngenerate motion cues present in pedestrian past/observed trajectories. Our\napproach also incorporates motion cues present in pedestrian future\ntrajectories by clustering future trajectories of training data using a mixture\nof Gaussians. These motion cues, along with pedestrian coordinates, facilitate\na better understanding of the underlying representation. Furthermore, we\nutilize singular value decomposition to augment the observed trajectories,\nincorporating them into the model learning process to further enhance\nrepresentation learning. Our method employs a transformer-based architecture\ncomprising a motion encoder to model motion patterns and a social decoder to\ncapture social interactions among pedestrians. We demonstrate the effectiveness\nof our approach on popular pedestrian trajectory prediction benchmarks, namely\nETH-UCY and SDD, and present various ablation experiments to validate our\napproach.\n"
    },
    {
        "title": "Test-Time Personalization with Meta Prompt for Gaze Estimation",
        "published_time": "2024-01-03T07:02:35Z",
        "abstract": "  Despite the recent remarkable achievement in gaze estimation, efficient and\naccurate personalization of gaze estimation without labels is a practical\nproblem but rarely touched on in the literature. To achieve efficient\npersonalization, we take inspiration from the recent advances in Natural\nLanguage Processing (NLP) by updating a negligible number of parameters,\n\"prompts\", at the test time. Specifically, the prompt is additionally attached\nwithout perturbing original network and can contain less than 1% of a\nResNet-18's parameters. Our experiments show high efficiency of the prompt\ntuning approach. The proposed one can be 10 times faster in terms of adaptation\nspeed than the methods compared. However, it is non-trivial to update the\nprompt for personalized gaze estimation without labels. At the test time, it is\nessential to ensure that the minimizing of particular unsupervised loss leads\nto the goals of minimizing gaze estimation error. To address this difficulty,\nwe propose to meta-learn the prompt to ensure that its updates align with the\ngoal. Our experiments show that the meta-learned prompt can be effectively\nadapted even with a simple symmetry loss. In addition, we experiment on four\ncross-dataset validations to show the remarkable advantages of the proposed\nmethod. Code is available at https://github.com/hmarkamcan/TPGaze.\n"
    },
    {
        "title": "MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation",
        "published_time": "2024-03-12T18:36:59Z",
        "abstract": "  We propose a single-shot approach to determining 6-DoF pose of an object with\navailable 3D computer-aided design (CAD) model from a single RGB image. Our\nmethod, dubbed MRC-Net, comprises two stages. The first performs pose\nclassification and renders the 3D object in the classified pose. The second\nstage performs regression to predict fine-grained residual pose within class.\nConnecting the two stages is a novel multi-scale residual correlation (MRC)\nlayer that captures high-and-low level correspondences between the input image\nand rendering from first stage. MRC-Net employs a Siamese network with shared\nweights between both stages to learn embeddings for input and rendered images.\nTo mitigate ambiguity when predicting discrete pose class labels on symmetric\nobjects, we use soft probabilistic labels to define pose class in the first\nstage. We demonstrate state-of-the-art accuracy, outperforming all competing\nRGB-based methods on four challenging BOP benchmark datasets: T-LESS, LM-O,\nYCB-V, and ITODD. Our method is non-iterative and requires no complex\npost-processing.\n"
    },
    {
        "title": "Learning Data Association for Multi-Object Tracking using Only\n  Coordinates",
        "published_time": "2024-03-12T18:36:18Z",
        "abstract": "  We propose a novel Transformer-based module to address the data association\nproblem for multi-object tracking. From detections obtained by a pretrained\ndetector, this module uses only coordinates from bounding boxes to estimate an\naffinity score between pairs of tracks extracted from two distinct temporal\nwindows. This module, named TWiX, is trained on sets of tracks with the\nobjective of discriminating pairs of tracks coming from the same object from\nthose which are not. Our module does not use the intersection over union\nmeasure, nor does it requires any motion priors or any camera motion\ncompensation technique. By inserting TWiX within an online cascade matching\npipeline, our tracker C-TWiX achieves state-of-the-art performance on the\nDanceTrack and KITTIMOT datasets, and gets competitive results on the MOT17\ndataset. The code will be made available upon publication.\n"
    },
    {
        "title": "Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI",
        "published_time": "2024-03-12T18:28:32Z",
        "abstract": "  Remote sensing (RS) applications in the space domain demand machine learning\n(ML) models that are reliable, robust, and quality-assured, making red teaming\na vital approach for identifying and exposing potential flaws and biases. Since\nboth fields advance independently, there is a notable gap in integrating red\nteaming strategies into RS. This paper introduces a methodology for examining\nML models operating on hyperspectral images within the HYPERVIEW challenge,\nfocusing on soil parameters' estimation. We use post-hoc explanation methods\nfrom the Explainable AI (XAI) domain to critically assess the best performing\nmodel that won the HYPERVIEW challenge and served as an inspiration for the\nmodel deployed on board the INTUITION-1 hyperspectral mission. Our approach\neffectively red teams the model by pinpointing and validating key shortcomings,\nconstructing a model that achieves comparable performance using just 1% of the\ninput features and a mere up to 5% performance loss. Additionally, we propose a\nnovel way of visualizing explanations that integrate domain-specific\ninformation about hyperspectral bands (wavelengths) and data transformations to\nbetter suit interpreting models for hyperspectral image analysis.\n"
    },
    {
        "title": "Aedes aegypti Egg Counting with Neural Networks for Object Detection",
        "published_time": "2024-03-12T18:28:13Z",
        "abstract": "  Aedes aegypti is still one of the main concerns when it comes to disease\nvectors. Among the many ways to deal with it, there are important protocols\nthat make use of egg numbers in ovitraps to calculate indices, such as the\nLIRAa and the Breteau Index, which can provide information on predictable\noutbursts and epidemics. Also, there are many research lines that require egg\nnumbers, specially when mass production of mosquitoes is needed. Egg counting\nis a laborious and error-prone task that can be automated via computer\nvision-based techniques, specially deep learning-based counting with object\ndetection. In this work, we propose a new dataset comprising field and\nlaboratory eggs, along with test results of three neural networks applied to\nthe task: Faster R-CNN, Side-Aware Boundary Localization and FoveaBox.\n"
    },
    {
        "title": "The R2D2 deep neural network series paradigm for fast precision imaging\n  in radio astronomy",
        "published_time": "2024-03-08T16:57:54Z",
        "abstract": "  Radio-interferometric (RI) imaging entails solving high-resolution\nhigh-dynamic range inverse problems from large data volumes. Recent image\nreconstruction techniques grounded in optimization theory have demonstrated\nremarkable capability for imaging precision, well beyond CLEAN's capability.\nThese range from advanced proximal algorithms propelled by handcrafted\nregularization operators, such as the SARA family, to hybrid plug-and-play\n(PnP) algorithms propelled by learned regularization denoisers, such as AIRI.\nOptimization and PnP structures are however highly iterative, which hinders\ntheir ability to handle the extreme data sizes expected from future\ninstruments. To address this scalability challenge, we introduce a novel deep\nlearning approach, dubbed ``Residual-to-Residual DNN series for high-Dynamic\nrange imaging''. R2D2's reconstruction is formed as a series of residual\nimages, iteratively estimated as outputs of Deep Neural Networks (DNNs) taking\nthe previous iteration's image estimate and associated data residual as inputs.\nIt thus takes a hybrid structure between a PnP algorithm and a learned version\nof the matching pursuit algorithm that underpins CLEAN. We present a\ncomprehensive study of our approach, featuring its multiple incarnations\ndistinguished by their DNN architectures. We provide a detailed description of\nits training process, targeting a telescope-specific approach. R2D2's\ncapability to deliver high precision is demonstrated in simulation, across a\nvariety of image and observation settings using the Very Large Array (VLA). Its\nreconstruction speed is also demonstrated: with only few iterations required to\nclean data residuals at dynamic ranges up to 100000, R2D2 opens the door to\nfast precision imaging. R2D2 codes are available in the BASPLib library on\nGitHub.\n"
    },
    {
        "title": "IndicSTR12: A Dataset for Indic Scene Text Recognition",
        "published_time": "2024-03-12T18:14:48Z",
        "abstract": "  The importance of Scene Text Recognition (STR) in today's increasingly\ndigital world cannot be overstated. Given the significance of STR, data\nintensive deep learning approaches that auto-learn feature mappings have\nprimarily driven the development of STR solutions. Several benchmark datasets\nand substantial work on deep learning models are available for Latin languages\nto meet this need. On more complex, syntactically and semantically, Indian\nlanguages spoken and read by 1.3 billion people, there is less work and\ndatasets available. This paper aims to address the Indian space's lack of a\ncomprehensive dataset by proposing the largest and most comprehensive real\ndataset - IndicSTR12 - and benchmarking STR performance on 12 major Indian\nlanguages. A few works have addressed the same issue, but to the best of our\nknowledge, they focused on a small number of Indian languages. The size and\ncomplexity of the proposed dataset are comparable to those of existing Latin\ncontemporaries, while its multilingualism will catalyse the development of\nrobust text detection and recognition models. It was created specifically for a\ngroup of related languages with different scripts. The dataset contains over\n27000 word-images gathered from various natural scenes, with over 1000\nword-images for each language. Unlike previous datasets, the images cover a\nbroader range of realistic conditions, including blur, illumination changes,\nocclusion, non-iconic texts, low resolution, perspective text etc. Along with\nthe new dataset, we provide a high-performing baseline on three models -\nPARSeq, CRNN, and STARNet.\n"
    },
    {
        "title": "Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing",
        "published_time": "2024-03-12T18:12:50Z",
        "abstract": "  The combination of language processing and image processing keeps attracting\nincreased interest given recent impressive advances that leverage the combined\nstrengths of both domains of research. Among these advances, the task of\nediting an image on the basis solely of a natural language instruction stands\nout as a most challenging endeavour. While recent approaches for this task\nresort, in one way or other, to some form of preliminary preparation, training\nor fine-tuning, this paper explores a novel approach: We propose a\npreparation-free method that permits instruction-guided image editing on the\nfly. This approach is organized along three steps properly orchestrated that\nresort to image captioning and DDIM inversion, followed by obtaining the edit\ndirection embedding, followed by image editing proper. While dispensing with\npreliminary preparation, our approach demonstrates to be effective and\ncompetitive, outperforming recent, state of the art models for this task when\nevaluated on the MAGICBRUSH dataset.\n"
    },
    {
        "title": "Real-time Surgical Instrument Segmentation in Video Using Point Tracking\n  and Segment Anything",
        "published_time": "2024-03-12T18:12:42Z",
        "abstract": "  The Segment Anything Model (SAM) is a powerful vision foundation model that\nis revolutionizing the traditional paradigm of segmentation. Despite this, a\nreliance on prompting each frame and large computational cost limit its usage\nin robotically assisted surgery. Applications, such as augmented reality\nguidance, require little user intervention along with efficient inference to be\nusable clinically. In this study, we address these limitations by adopting\nlightweight SAM variants to meet the speed requirement and employing\nfine-tuning techniques to enhance their generalization in surgical scenes.\nRecent advancements in Tracking Any Point (TAP) have shown promising results in\nboth accuracy and efficiency, particularly when points are occluded or leave\nthe field of view. Inspired by this progress, we present a novel framework that\ncombines an online point tracker with a lightweight SAM model that is\nfine-tuned for surgical instrument segmentation. Sparse points within the\nregion of interest are tracked and used to prompt SAM throughout the video\nsequence, providing temporal consistency. The quantitative results surpass the\nstate-of-the-art semi-supervised video object segmentation method on the\nEndoVis 2015 dataset, with an over 25 FPS inference speed running on a single\nGeForce RTX 4060 GPU.\n"
    },
    {
        "title": "Training Small Multimodal Models to Bridge Biomedical Competency Gap: A\n  Case Study in Radiology Imaging",
        "published_time": "2024-03-12T18:12:02Z",
        "abstract": "  The scaling laws and extraordinary performance of large foundation models\nmotivate the development and utilization of such large models in biomedicine.\nHowever, despite early promising results on some biomedical benchmarks, there\nare still major challenges that need to be addressed before these models can be\nused in real-world applications. Frontier models such as GPT-4V still have\nmajor competency gaps in multimodal capabilities for biomedical applications.\nMoreover, pragmatic issues such as access, cost, latency, and compliance make\nit hard for clinicians to use privately-hosted state-of-the-art large models\ndirectly on private patient data. In this paper, we explore training\nopen-source small multimodal models (SMMs) to bridge biomedical competency gaps\nfor unmet clinical needs. To maximize data efficiency, we adopt a modular\napproach by incorporating state-of-the-art pre-trained models for image and\ntext modalities, and focusing on training a lightweight adapter to ground each\nmodality to the text embedding space. We conduct a comprehensive study of this\napproach on radiology imaging. For training, we assemble a large dataset with\nover 1 million image-text pairs. For evaluation, we propose a clinically driven\nnovel approach using GPT-4 and demonstrate its parity with expert evaluation.\nWe also study grounding qualitatively using attention. For best practice, we\nconduct a systematic ablation study on various choices in data engineering and\nmultimodal training. The resulting LLaVA-Rad (7B) model attains\nstate-of-the-art results on radiology tasks such as report generation and\ncross-modal retrieval, even outperforming much larger models such as GPT-4V and\nMed-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in\nprivate settings, offering a promising state-of-the-art tool for real-world\nclinical applications.\n"
    },
    {
        "title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic\n  Compositional Reasoning via Counterfactual Examples",
        "published_time": "2024-02-20T18:59:55Z",
        "abstract": "  We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two critical\nunder-explored problems: the neglect of the physically grounded reasoning\n(counting and position understanding) and the potential of using highly capable\ntext and image generation models for semantic counterfactual fine-tuning. Our\nwork pioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\ngrounded image generation model GLIGEN to generate fine-tuning data, resulting\nin significant performance improvements: +33% and +37% for CLIP and LLaVA,\nrespectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we\nexploit the capabilities of high-performing text generation and image\ngeneration models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V.\n"
    },
    {
        "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension",
        "published_time": "2024-03-12T17:59:51Z",
        "abstract": "  In this work, we investigate the potential of a large language model (LLM) to\ndirectly comprehend visual signals without the necessity of fine-tuning on\nmulti-modal datasets. The foundational concept of our method views an image as\na linguistic entity, and translates it to a set of discrete words derived from\nthe LLM's vocabulary. To achieve this, we present the Vision-to-Language\nTokenizer, abbreviated as V2T Tokenizer, which transforms an image into a\n``foreign language'' with the combined aid of an encoder-decoder, the LLM\nvocabulary, and a CLIP model. With this innovative image encoding, the LLM\ngains the ability not only for visual comprehension but also for image\ndenoising and restoration in an auto-regressive fashion-crucially, without any\nfine-tuning. We undertake rigorous experiments to validate our method,\nencompassing understanding tasks like image recognition, image captioning, and\nvisual question answering, as well as image denoising tasks like inpainting,\noutpainting, deblurring, and shift restoration. Code and models are available\nat https://github.com/zh460045050/V2L-Tokenizer.\n"
    },
    {
        "title": "Bridging Different Language Models and Generative Vision Models for\n  Text-to-Image Generation",
        "published_time": "2024-03-12T17:50:11Z",
        "abstract": "  Text-to-image generation has made significant advancements with the\nintroduction of text-to-image diffusion models. These models typically consist\nof a language model that interprets user prompts and a vision model that\ngenerates corresponding images. As language and vision models continue to\nprogress in their respective domains, there is a great potential in exploring\nthe replacement of components in text-to-image diffusion models with more\nadvanced counterparts. A broader research objective would therefore be to\ninvestigate the integration of any two unrelated language and generative vision\nmodels for text-to-image generation. In this paper, we explore this objective\nand propose LaVi-Bridge, a pipeline that enables the integration of diverse\npre-trained language models and generative vision models for text-to-image\ngeneration. By leveraging LoRA and adapters, LaVi-Bridge offers a flexible and\nplug-and-play approach without requiring modifications to the original weights\nof the language and vision models. Our pipeline is compatible with various\nlanguage models and generative vision models, accommodating different\nstructures. Within this framework, we demonstrate that incorporating superior\nmodules, such as more advanced language models or generative vision models,\nresults in notable improvements in capabilities like text alignment or image\nquality. Extensive evaluations have been conducted to verify the effectiveness\nof LaVi-Bridge. Code is available at\nhttps://github.com/ShihaoZhaoZSH/LaVi-Bridge.\n"
    },
    {
        "title": "RudolfV: A Foundation Model by Pathologists for Pathologists",
        "published_time": "2024-01-08T18:31:38Z",
        "abstract": "  Histopathology plays a central role in clinical medicine and biomedical\nresearch. While artificial intelligence shows promising results on many\npathological tasks, generalization and dealing with rare diseases, where\ntraining data is scarce, remains a challenge. Distilling knowledge from\nunlabelled data into a foundation model before learning from, potentially\nlimited, labelled data provides a viable path to address these challenges. In\nthis work, we extend the state of the art of foundation models for digital\npathology whole slide images by semi-automated data curation and incorporating\npathologist domain knowledge. Specifically, we combine computational and\npathologist domain knowledge (1) to curate a diverse dataset of 133k slides\ncorresponding to 1.2 billion image patches covering data from different\nfixation, staining, and scanning protocols as well as data from different\nindications and labs across the EU and US, (2) for grouping semantically\nsimilar slides and tissue patches, and (3) to augment the input images during\ntraining. We evaluate the resulting model on a set of public and internal\nbenchmarks and show that although our foundation model is trained with an order\nof magnitude less slides, it performs on par or better than competing models.\nWe expect that scaling our approach to more data and larger models will further\nincrease its performance and capacity to deal with increasingly complex real\nworld tasks in diagnostics and biomedical research.\n"
    },
    {
        "title": "Distilling the Knowledge in Data Pruning",
        "published_time": "2024-03-12T17:44:45Z",
        "abstract": "  With the increasing size of datasets used for training neural networks, data\npruning becomes an attractive field of research. However, most current data\npruning algorithms are limited in their ability to preserve accuracy compared\nto models trained on the full data, especially in high pruning regimes. In this\npaper we explore the application of data pruning while incorporating knowledge\ndistillation (KD) when training on a pruned subset. That is, rather than\nrelying solely on ground-truth labels, we also use the soft predictions from a\nteacher network pre-trained on the complete data. By integrating KD into\ntraining, we demonstrate significant improvement across datasets, pruning\nmethods, and on all pruning fractions. We first establish a theoretical\nmotivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using\nKD, simple random pruning is comparable or superior to sophisticated pruning\nmethods across all pruning regimes. On ImageNet for example, we achieve\nsuperior accuracy despite training on a random subset of only 50% of the data.\nAdditionally, we demonstrate a crucial connection between the pruning factor\nand the optimal knowledge distillation weight. This helps mitigate the impact\nof samples with noisy labels and low-quality images retained by typical pruning\nalgorithms. Finally, we make an intriguing observation: when using lower\npruning fractions, larger teachers lead to accuracy degradation, while\nsurprisingly, employing teachers with a smaller capacity than the student's may\nimprove results. Our code will be made available.\n"
    },
    {
        "title": "12 mJ per Class On-Device Online Few-Shot Class-Incremental Learning",
        "published_time": "2024-03-12T17:43:20Z",
        "abstract": "  Few-Shot Class-Incremental Learning (FSCIL) enables machine learning systems\nto expand their inference capabilities to new classes using only a few labeled\nexamples, without forgetting the previously learned classes. Classical\nbackpropagation-based learning and its variants are often unsuitable for\nbattery-powered, memory-constrained systems at the extreme edge. In this work,\nwe introduce Online Few-Shot Class-Incremental Learning (O-FSCIL), based on a\nlightweight model consisting of a pretrained and metalearned feature extractor\nand an expandable explicit memory storing the class prototypes. The\narchitecture is pretrained with a novel feature orthogonality regularization\nand metalearned with a multi-margin loss. For learning a new class, our\napproach extends the explicit memory with novel class prototypes, while the\nremaining architecture is kept frozen. This allows learning previously unseen\nclasses based on only a few examples with one single pass (hence online).\nO-FSCIL obtains an average accuracy of 68.62% on the FSCIL CIFAR100 benchmark,\nachieving state-of-the-art results. Tailored for ultra-low-power platforms, we\nimplement O-FSCIL on the 60 mW GAP9 microcontroller, demonstrating online\nlearning capabilities within just 12 mJ per new class.\n"
    },
    {
        "title": "CarbonNet: How Computer Vision Plays a Role in Climate Change?\n  Application: Learning Geomechanics from Subsurface Geometry of CCS to\n  Mitigate Global Warming",
        "published_time": "2024-03-09T22:25:14Z",
        "abstract": "  We introduce a new approach using computer vision to predict the land surface\ndisplacement from subsurface geometry images for Carbon Capture and\nSequestration (CCS). CCS has been proved to be a key component for a carbon\nneutral society. However, scientists see there are challenges along the way\nincluding the high computational cost due to the large model scale and\nlimitations to generalize a pre-trained model with complex physics. We tackle\nthose challenges by training models directly from the subsurface geometry\nimages. The goal is to understand the respons of land surface displacement due\nto carbon injection and utilize our trained models to inform decision making in\nCCS projects.\n  We implement multiple models (CNN, ResNet, and ResNetUNet) for static\nmechanics problem, which is a image prediction problem. Next, we use the LSTM\nand transformer for transient mechanics scenario, which is a video prediction\nproblem. It shows ResNetUNet outperforms the others thanks to its architecture\nin static mechanics problem, and LSTM shows comparable performance to\ntransformer in transient problem. This report proceeds by outlining our dataset\nin detail followed by model descriptions in method section. Result and\ndiscussion state the key learning, observations, and conclusion with future\nwork rounds out the paper.\n"
    },
    {
        "title": "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with\n  Module-wise Pruning Error Metric",
        "published_time": "2024-03-12T17:24:26Z",
        "abstract": "  Vision-language pre-trained models have achieved impressive performance on\nvarious downstream tasks. However, their large model sizes hinder their\nutilization on platforms with limited computational resources. We find that\ndirectly using smaller pre-trained models and applying magnitude-based pruning\non CLIP models leads to inflexibility and inferior performance. Recent efforts\nfor VLP compression either adopt uni-modal compression metrics resulting in\nlimited performance or involve costly mask-search processes with learnable\nmasks. In this paper, we first propose the Module-wise Pruning Error (MoPE)\nmetric, accurately assessing CLIP module importance by performance decline on\ncross-modal tasks. Using the MoPE metric, we introduce a unified pruning\nframework applicable to both pre-training and task-specific fine-tuning\ncompression stages. For pre-training, MoPE-CLIP effectively leverages knowledge\nfrom the teacher model, significantly reducing pre-training costs while\nmaintaining strong zero-shot capabilities. For fine-tuning, consecutive pruning\nfrom width to depth yields highly competitive task-specific models. Extensive\nexperiments in two stages demonstrate the effectiveness of the MoPE metric, and\nMoPE-CLIP outperforms previous state-of-the-art VLP compression methods.\n"
    },
    {
        "title": "When Eye-Tracking Meets Machine Learning: A Systematic Review on\n  Applications in Medical Image Analysis",
        "published_time": "2024-03-12T17:17:20Z",
        "abstract": "  Eye-gaze tracking research offers significant promise in enhancing various\nhealthcare-related tasks, above all in medical image analysis and\ninterpretation. Eye tracking, a technology that monitors and records the\nmovement of the eyes, provides valuable insights into human visual attention\npatterns. This technology can transform how healthcare professionals and\nmedical specialists engage with and analyze diagnostic images, offering a more\ninsightful and efficient approach to medical diagnostics. Hence, extracting\nmeaningful features and insights from medical images by leveraging eye-gaze\ndata improves our understanding of how radiologists and other medical experts\nmonitor, interpret, and understand images for diagnostic purposes. Eye-tracking\ndata, with intricate human visual attention patterns embedded, provides a\nbridge to integrating artificial intelligence (AI) development and human\ncognition. This integration allows novel methods to incorporate domain\nknowledge into machine learning (ML) and deep learning (DL) approaches to\nenhance their alignment with human-like perception and decision-making.\nMoreover, extensive collections of eye-tracking data have also enabled novel\nML/DL methods to analyze human visual patterns, paving the way to a better\nunderstanding of human vision, attention, and cognition. This systematic review\ninvestigates eye-gaze tracking applications and methodologies for enhancing\nML/DL algorithms for medical image analysis in depth.\n"
    },
    {
        "title": "Label Dropout: Improved Deep Learning Echocardiography Segmentation\n  Using Multiple Datasets With Domain Shift and Partial Labelling",
        "published_time": "2024-03-12T16:57:56Z",
        "abstract": "  Echocardiography (echo) is the first imaging modality used when assessing\ncardiac function. The measurement of functional biomarkers from echo relies\nupon the segmentation of cardiac structures and deep learning models have been\nproposed to automate the segmentation process. However, in order to translate\nthese tools to widespread clinical use it is important that the segmentation\nmodels are robust to a wide variety of images (e.g. acquired from different\nscanners, by operators with different levels of expertise etc.). To achieve\nthis level of robustness it is necessary that the models are trained with\nmultiple diverse datasets. A significant challenge faced when training with\nmultiple diverse datasets is the variation in label presence, i.e. the combined\ndata are often partially-labelled. Adaptations of the cross entropy loss\nfunction have been proposed to deal with partially labelled data. In this paper\nwe show that training naively with such a loss function and multiple diverse\ndatasets can lead to a form of shortcut learning, where the model associates\nlabel presence with domain characteristics, leading to a drop in performance.\nTo address this problem, we propose a novel label dropout scheme to break the\nlink between domain characteristics and the presence or absence of labels. We\ndemonstrate that label dropout improves echo segmentation Dice score by 62% and\n25% on two cardiac structures when training using multiple diverse partially\nlabelled datasets.\n"
    },
    {
        "title": "The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition",
        "published_time": "2024-02-29T16:49:38Z",
        "abstract": "  This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW)\nCompetition, which is part of the respective Workshop held in conjunction with\nIEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges in\nunderstanding human emotions and behaviors, crucial for the development of\nhuman-centered technologies. In more detail, the Competition focuses on affect\nrelated benchmarking tasks and comprises of five sub-challenges: i)\nValence-Arousal Estimation (the target is to estimate two continuous affect\ndimensions, valence and arousal), ii) Expression Recognition (the target is to\nrecognise between the mutually exclusive classes of the 7 basic expressions and\n'other'), iii) Action Unit Detection (the target is to detect 12 action units),\niv) Compound Expression Recognition (the target is to recognise between the 7\nmutually exclusive compound expression classes), and v) Emotional Mimicry\nIntensity Estimation (the target is to estimate six continuous emotion\ndimensions). In the paper, we present these Challenges, describe their\nrespective datasets and challenge protocols (we outline the evaluation metrics)\nand present the baseline systems as well as their obtained performance. More\ninformation for the Competition can be found in:\nhttps://affective-behavior-analysis-in-the-wild.github.io/6th.\n"
    },
    {
        "title": "StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting",
        "published_time": "2024-03-12T16:44:52Z",
        "abstract": "  We introduce StyleGaussian, a novel 3D style transfer technique that allows\ninstant transfer of any image's style to a 3D scene at 10 frames per second\n(fps). Leveraging 3D Gaussian Splatting (3DGS), StyleGaussian achieves style\ntransfer without compromising its real-time rendering ability and multi-view\nconsistency. It achieves instant style transfer with three steps: embedding,\ntransfer, and decoding. Initially, 2D VGG scene features are embedded into\nreconstructed 3D Gaussians. Next, the embedded features are transformed\naccording to a reference style image. Finally, the transformed features are\ndecoded into the stylized RGB. StyleGaussian has two novel designs. The first\nis an efficient feature rendering strategy that first renders low-dimensional\nfeatures and then maps them into high-dimensional features while embedding VGG\nfeatures. It cuts the memory consumption significantly and enables 3DGS to\nrender the high-dimensional memory-intensive features. The second is a\nK-nearest-neighbor-based 3D CNN. Working as the decoder for the stylized\nfeatures, it eliminates the 2D CNN operations that compromise strict multi-view\nconsistency. Extensive experiments show that StyleGaussian achieves instant 3D\nstylization with superior stylization quality while preserving real-time\nrendering and strict multi-view consistency. Project page:\nhttps://kunhao-liu.github.io/StyleGaussian/\n"
    },
    {
        "title": "BraSyn 2023 challenge: Missing MRI synthesis and the effect of different\n  learning objectives",
        "published_time": "2024-03-12T16:36:27Z",
        "abstract": "  This work is addressing the Brain Magnetic Resonance Image Synthesis for\nTumor Segmentation (BraSyn) challenge which was hosted as part of the Brain\nTumor Segmentation challenge (BraTS) 2023. In this challenge researchers are\ninvited to work on synthesizing a missing magnetic resonance image sequence\ngiven other available sequences to facilitate tumor segmentation pipelines\ntrained on complete sets of image sequences. This problem can be addressed\nusing deep learning in the framework of paired images-to-image translation. In\nthis work, we proposed to investigate the effectiveness of a commonly-used deep\nlearning framework such as Pix2Pix trained under supervision of different\nimage-quality loss functions. Our results indicate that using different loss\nfunctions significantly affects the synthesis quality. We systematically study\nthe impact of different loss functions in the multi-sequence MR image synthesis\nsetting of the BraSyn challenge. Furthermore, we show how image synthesis\nperformance can be optimized by beneficially combining different learning\nobjectives.\n"
    },
    {
        "title": "A Fourier Transform Framework for Domain Adaptation",
        "published_time": "2024-03-12T16:35:32Z",
        "abstract": "  By using unsupervised domain adaptation (UDA), knowledge can be transferred\nfrom a label-rich source domain to a target domain that contains relevant\ninformation but lacks labels. Many existing UDA algorithms suffer from directly\nusing raw images as input, resulting in models that overly focus on redundant\ninformation and exhibit poor generalization capability. To address this issue,\nwe attempt to improve the performance of unsupervised domain adaptation by\nemploying the Fourier method (FTF).Specifically, FTF is inspired by the\namplitude of Fourier spectra, which primarily preserves low-level statistical\ninformation. In FTF, we effectively incorporate low-level information from the\ntarget domain into the source domain by fusing the amplitudes of both domains\nin the Fourier domain. Additionally, we observe that extracting features from\nbatches of images can eliminate redundant information while retaining\nclass-specific features relevant to the task. Building upon this observation,\nwe apply the Fourier Transform at the data stream level for the first time. To\nfurther align multiple sources of data, we introduce the concept of correlation\nalignment. To evaluate the effectiveness of our FTF method, we conducted\nevaluations on four benchmark datasets for domain adaptation, including\nOffice-31, Office-Home, ImageCLEF-DA, and Office-Caltech. Our results\ndemonstrate superior performance.\n"
    },
    {
        "title": "DexCap: Scalable and Portable Mocap Data Collection System for Dexterous\n  Manipulation",
        "published_time": "2024-03-12T16:23:49Z",
        "abstract": "  Imitation learning from human hand motion data presents a promising avenue\nfor imbuing robots with human-like dexterity in real-world manipulation tasks.\nDespite this potential, substantial challenges persist, particularly with the\nportability of existing hand motion capture (mocap) systems and the difficulty\nof translating mocap data into effective control policies. To tackle these\nissues, we introduce DexCap, a portable hand motion capture system, alongside\nDexIL, a novel imitation algorithm for training dexterous robot skills directly\nfrom human hand mocap data. DexCap offers precise, occlusion-resistant tracking\nof wrist and finger motions based on SLAM and electromagnetic field together\nwith 3D observations of the environment. Utilizing this rich dataset, DexIL\nemploys inverse kinematics and point cloud-based imitation learning to\nreplicate human actions with robot hands. Beyond learning from human motion,\nDexCap also offers an optional human-in-the-loop correction mechanism to refine\nand further improve robot performance. Through extensive evaluation across six\ndexterous manipulation tasks, our approach not only demonstrates superior\nperformance but also showcases the system's capability to effectively learn\nfrom in-the-wild mocap data, paving the way for future data collection methods\nfor dexterous manipulation. More details can be found at\nhttps://dex-cap.github.io\n"
    },
    {
        "title": "Generative deep learning-enabled ultra-large field-of-view lens-free\n  imaging",
        "published_time": "2024-03-12T16:20:27Z",
        "abstract": "  Advancements in high-throughput biomedical applications necessitate\nreal-time, large field-of-view (FOV) imaging capabilities. Conventional\nlens-free imaging (LFI) systems, while addressing the limitations of physical\nlenses, have been constrained by dynamic, hard-to-model optical fields,\nresulting in a limited one-shot FOV of approximately 20 $mm^2$. This\nrestriction has been a major bottleneck in applications like live-cell imaging\nand automation of microfluidic systems for biomedical research. Here, we\npresent a deep-learning(DL)-based imaging framework -- GenLFI -- leveraging\ngenerative artificial intelligence (AI) for holographic image reconstruction.\nWe demonstrate that GenLFI can achieve a real-time FOV over 550 $mm^2$,\nsurpassing the current LFI system by more than 20-fold, and even larger than\nthe world's largest confocal microscope by 1.76 times. The resolution is at the\nsub-pixel level of 5.52 $\\mu m$, without the need for a shifting light source.\nThe unsupervised learning-based reconstruction does not require optical field\nmodeling, making imaging dynamic 3D samples (e.g., droplet-based microfluidics\nand 3D cell models) in complex optical fields possible. This GenLFI framework\nunlocks the potential of LFI systems, offering a robust tool to tackle new\nfrontiers in high-throughput biomedical applications such as drug discovery.\n"
    },
    {
        "title": "Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model",
        "published_time": "2024-03-12T15:53:14Z",
        "abstract": "  Current makeup transfer methods are limited to simple makeup styles, making\nthem difficult to apply in real-world scenarios. In this paper, we introduce\nStable-Makeup, a novel diffusion-based makeup transfer method capable of\nrobustly transferring a wide range of real-world makeup, onto user-provided\nfaces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a\nDetail-Preserving (D-P) makeup encoder to encode makeup details. It also\nemploys content and structural control modules to preserve the content and\nstructural information of the source image. With the aid of our newly added\nmakeup cross-attention layers in U-Net, we can accurately transfer the detailed\nmakeup to the corresponding position in the source image. After\ncontent-structure decoupling training, Stable-Makeup can maintain content and\nthe facial structure of the source image. Moreover, our method has demonstrated\nstrong robustness and generalizability, making it applicable to varioustasks\nsuch as cross-domain makeup transfer, makeup-guided text-to-image generation\nand so on. Extensive experiments have demonstrated that our approach delivers\nstate-of-the-art (SOTA) results among existing makeup transfer methods and\nexhibits a highly promising with broad potential applications in various\nrelated fields.\n"
    },
    {
        "title": "Vision-based Vehicle Re-identification in Bridge Scenario using Flock\n  Similarity",
        "published_time": "2024-03-12T15:39:56Z",
        "abstract": "  Due to the needs of road traffic flow monitoring and public safety\nmanagement, video surveillance cameras are widely distributed in urban roads.\nHowever, the information captured directly by each camera is siloed, making it\ndifficult to use it effectively. Vehicle re-identification refers to finding a\nvehicle that appears under one camera in another camera, which can correlate\nthe information captured by multiple cameras. While license plate recognition\nplays an important role in some applications, there are some scenarios where\nre-identification method based on vehicle appearance are more suitable. The\nmain challenge is that the data of vehicle appearance has the characteristics\nof high inter-class similarity and large intra-class differences. Therefore, it\nis difficult to accurately distinguish between different vehicles by relying\nonly on vehicle appearance information. At this time, it is often necessary to\nintroduce some extra information, such as spatio-temporal information.\nNevertheless, the relative position of the vehicles rarely changes when passing\nthrough two adjacent cameras in the bridge scenario. In this paper, we present\na vehicle re-identification method based on flock similarity, which improves\nthe accuracy of vehicle re-identification by utilizing vehicle information\nadjacent to the target vehicle. When the relative position of the vehicles\nremains unchanged and flock size is appropriate, we obtain an average relative\nimprovement of 204% on VeRi dataset in our experiments. Then, the effect of the\nmagnitude of the relative position change of the vehicles as they pass through\ntwo cameras is discussed. We present two metrics that can be used to quantify\nthe difference and establish a connection between them. Although this\nassumption is based on the bridge scenario, it is often true in other scenarios\ndue to driving safety and camera location.\n"
    },
    {
        "title": "Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and\n  Image Embeddings",
        "published_time": "2024-03-12T15:36:42Z",
        "abstract": "  The creation of high-quality human-labeled image-caption datasets presents a\nsignificant bottleneck in the development of Visual-Language Models (VLMs). We\npropose a novel approach that leverages the strengths of Large Language Models\n(LLMs) and image generation models to create synthetic image-text pairs for\nefficient and effective VLM training. Our method employs pretraining a\ntext-to-image model to synthesize image embeddings starting from captions\ngenerated by an LLM. These synthetic pairs are then used to train a VLM.\nExtensive experiments demonstrate that the VLM trained with synthetic data\nexhibits comparable performance on image captioning, while requiring a fraction\nof the data used by models trained solely on human-annotated data. In\nparticular, we outperform the baseline by 17% through augmentation with a\nsynthetic dataset. Furthermore, we show that synthesizing in the image\nembedding space is 25% faster than in the pixel space. This research introduces\na promising technique for generating large-scale, customizable image datasets,\nleading to enhanced VLM performance and wider applicability across various\ndomains, all with improved data efficiency and resource utilization.\n"
    },
    {
        "title": "Unleashing HyDRa: Hybrid Fusion, Depth Consistency and Radar for Unified\n  3D Perception",
        "published_time": "2024-03-12T15:28:51Z",
        "abstract": "  Low-cost, vision-centric 3D perception systems for autonomous driving have\nmade significant progress in recent years, narrowing the gap to expensive\nLiDAR-based methods. The primary challenge in becoming a fully reliable\nalternative lies in robust depth prediction capabilities, as camera-based\nsystems struggle with long detection ranges and adverse lighting and weather\nconditions. In this work, we introduce HyDRa, a novel camera-radar fusion\narchitecture for diverse 3D perception tasks. Building upon the principles of\ndense BEV (Bird's Eye View)-based architectures, HyDRa introduces a hybrid\nfusion approach to combine the strengths of complementary camera and radar\nfeatures in two distinct representation spaces. Our Height Association\nTransformer module leverages radar features already in the perspective view to\nproduce more robust and accurate depth predictions. In the BEV, we refine the\ninitial sparse representation by a Radar-weighted Depth Consistency. HyDRa\nachieves a new state-of-the-art for camera-radar fusion of 64.2 NDS (+1.8) and\n58.4 AMOTA (+1.5) on the public nuScenes dataset. Moreover, our new\nsemantically rich and spatially accurate BEV features can be directly converted\ninto a powerful occupancy representation, beating all previous camera-based\nmethods on the Occ3D benchmark by an impressive 3.7 mIoU.\n"
    },
    {
        "title": "VideoMamba: State Space Model for Efficient Video Understanding",
        "published_time": "2024-03-11T17:59:34Z",
        "abstract": "  Addressing the dual challenges of local redundancy and global dependencies in\nvideo understanding, this work innovatively adapts the Mamba to the video\ndomain. The proposed VideoMamba overcomes the limitations of existing 3D\nconvolution neural networks and video transformers. Its linear-complexity\noperator enables efficient long-term modeling, which is crucial for\nhigh-resolution long video understanding. Extensive evaluations reveal\nVideoMamba's four core abilities: (1) Scalability in the visual domain without\nextensive dataset pretraining, thanks to a novel self-distillation technique;\n(2) Sensitivity for recognizing short-term actions even with fine-grained\nmotion differences; (3) Superiority in long-term video understanding,\nshowcasing significant advancements over traditional feature-based models; and\n(4) Compatibility with other modalities, demonstrating robustness in\nmulti-modal contexts. Through these distinct advantages, VideoMamba sets a new\nbenchmark for video understanding, offering a scalable and efficient solution\nfor comprehensive video understanding. All the code and models are available at\nhttps://github.com/OpenGVLab/VideoMamba.\n"
    },
    {
        "title": "Uncertainty Quantification with Deep Ensembles for 6D Object Pose\n  Estimation",
        "published_time": "2024-03-12T15:19:25Z",
        "abstract": "  The estimation of 6D object poses is a fundamental task in many computer\nvision applications. Particularly, in high risk scenarios such as human-robot\ninteraction, industrial inspection, and automation, reliable pose estimates are\ncrucial. In the last years, increasingly accurate and robust\ndeep-learning-based approaches for 6D object pose estimation have been\nproposed. Many top-performing methods are not end-to-end trainable but consist\nof multiple stages. In the context of deep uncertainty quantification, deep\nensembles are considered as state of the art since they have been proven to\nproduce well-calibrated and robust uncertainty estimates. However, deep\nensembles can only be applied to methods that can be trained end-to-end. In\nthis work, we propose a method to quantify the uncertainty of multi-stage 6D\nobject pose estimation approaches with deep ensembles. For the implementation,\nwe choose SurfEmb as representative, since it is one of the top-performing 6D\nobject pose estimation approaches in the BOP Challenge 2022. We apply\nestablished metrics and concepts for deep uncertainty quantification to\nevaluate the results. Furthermore, we propose a novel uncertainty calibration\nscore for regression tasks to quantify the quality of the estimated\nuncertainty.\n"
    },
    {
        "title": "DSEG-LIME - Improving Image Explanation by Hierarchical Data-Driven\n  Segmentation",
        "published_time": "2024-03-12T15:13:12Z",
        "abstract": "  Explainable Artificial Intelligence is critical in unraveling decision-making\nprocesses in complex machine learning models. LIME (Local Interpretable\nModel-agnostic Explanations) is a well-known XAI framework for image analysis.\nIt utilizes image segmentation to create features to identify relevant areas\nfor classification. Consequently, poor segmentation can compromise the\nconsistency of the explanation and undermine the importance of the segments,\naffecting the overall interpretability. Addressing these challenges, we\nintroduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a\ndata-driven segmentation for human-recognized feature generation, and ii) a\nhierarchical segmentation procedure through composition. We benchmark DSEG-LIME\non pre-trained models with images from the ImageNet dataset - scenarios without\ndomain-specific knowledge. The analysis includes a quantitative evaluation\nusing established XAI metrics, complemented by a qualitative assessment through\na user study. Our findings demonstrate that DSEG outperforms in most of the XAI\nmetrics and enhances the alignment of explanations with human-recognized\nconcepts, significantly improving interpretability. The code is available\nunder: https://github. com/patrick-knab/DSEG-LIME\n"
    },
    {
        "title": "Multi-modal Auto-regressive Modeling via Visual Words",
        "published_time": "2024-03-12T14:58:52Z",
        "abstract": "  Large Language Models (LLMs), benefiting from the auto-regressive modelling\napproach performed on massive unannotated texts corpora, demonstrates powerful\nperceptual and reasoning capabilities. However, as for extending\nauto-regressive modelling to multi-modal scenarios to build Large Multi-modal\nModels (LMMs), there lies a great difficulty that the image information is\nprocessed in the LMM as continuous visual embeddings, which cannot obtain\ndiscrete supervised labels for classification. In this paper, we successfully\nperform multi-modal auto-regressive modeling with a unified objective for the\nfirst time. Specifically, we propose the concept of visual words, which maps\nthe visual features to probability distributions over LLM's vocabulary,\nproviding supervision information for visual modelling. We further explore the\ndistribution of visual features in the semantic space within LMM and the\npossibility of using text embeddings to represent visual information.\nExperimental results and ablation studies on 5 VQA tasks and 4 benchmark\ntoolkits validate the powerful performance of our proposed approach.\n"
    },
    {
        "title": "Dynamic Graph Representation with Knowledge-aware Attention for\n  Histopathology Whole Slide Image Analysis",
        "published_time": "2024-03-12T14:58:51Z",
        "abstract": "  Histopathological whole slide images (WSIs) classification has become a\nfoundation task in medical microscopic imaging processing. Prevailing\napproaches involve learning WSIs as instance-bag representations, emphasizing\nsignificant instances but struggling to capture the interactions between\ninstances. Additionally, conventional graph representation methods utilize\nexplicit spatial positions to construct topological structures but restrict the\nflexible interaction capabilities between instances at arbitrary locations,\nparticularly when spatially distant. In response, we propose a novel dynamic\ngraph representation algorithm that conceptualizes WSIs as a form of the\nknowledge graph structure. Specifically, we dynamically construct neighbors and\ndirected edge embeddings based on the head and tail relationships between\ninstances. Then, we devise a knowledge-aware attention mechanism that can\nupdate the head node features by learning the joint attention score of each\nneighbor and edge. Finally, we obtain a graph-level embedding through the\nglobal pooling process of the updated head, serving as an implicit\nrepresentation for the WSI classification. Our end-to-end graph representation\nlearning approach has outperformed the state-of-the-art WSI analysis methods on\nthree TCGA benchmark datasets and in-house test sets. Our code is available at\nhttps://github.com/WonderLandxD/WiKG.\n"
    },
    {
        "title": "Intra-video Positive Pairs in Self-Supervised Learning for Ultrasound",
        "published_time": "2024-03-12T14:57:57Z",
        "abstract": "  Self-supervised learning (SSL) is one strategy for addressing the paucity of\nlabelled data in medical imaging by learning representations from unlabelled\nimages. Contrastive and non-contrastive SSL methods produce learned\nrepresentations that are similar for pairs of related images. Such pairs are\ncommonly constructed by randomly distorting the same image twice. The\nvideographic nature of ultrasound offers flexibility for defining the\nsimilarity relationship between pairs of images. In this study, we investigated\nthe effect of utilizing proximal, distinct images from the same B-mode\nultrasound video as pairs for SSL. Additionally, we introduced a sample\nweighting scheme that increases the weight of closer image pairs and\ndemonstrated how it can be integrated into SSL objectives. Named Intra-Video\nPositive Pairs (IVPP), the method surpassed previous ultrasound-specific\ncontrastive learning methods' average test accuracy on COVID-19 classification\nwith the POCUS dataset by $\\ge 1.3\\%$. Detailed investigations of IVPP's\nhyperparameters revealed that some combinations of IVPP hyperparameters can\nlead to improved or worsened performance, depending on the downstream task.\nGuidelines for practitioners were synthesized based on the results, such as the\nmerit of IVPP with task-specific hyperparameters, and the improved performance\nof contrastive methods for ultrasound compared to non-contrastive counterparts.\n"
    },
    {
        "title": "SSM Meets Video Diffusion Models: Efficient Video Generation with\n  Structured State Spaces",
        "published_time": "2024-03-12T14:53:56Z",
        "abstract": "  Given the remarkable achievements in image generation through diffusion\nmodels, the research community has shown increasing interest in extending these\nmodels to video generation. Recent diffusion models for video generation have\npredominantly utilized attention layers to extract temporal features. However,\nattention layers are limited by their memory consumption, which increases\nquadratically with the length of the sequence. This limitation presents\nsignificant challenges when attempting to generate longer video sequences using\ndiffusion models. To overcome this challenge, we propose leveraging state-space\nmodels (SSMs). SSMs have recently gained attention as viable alternatives due\nto their linear memory consumption relative to sequence length. In the\nexperiments, we first evaluate our SSM-based model with UCF101, a standard\nbenchmark of video generation. In addition, to investigate the potential of\nSSMs for longer video generation, we perform an experiment using the MineRL\nNavigate dataset, varying the number of frames to 64 and 150. In these\nsettings, our SSM-based model can considerably save memory consumption for\nlonger sequences, while maintaining competitive FVD scores to the\nattention-based models. Our codes are available at\nhttps://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.\n"
    },
    {
        "title": "Fast and Simple Explainability for Point Cloud Networks",
        "published_time": "2024-03-12T14:51:23Z",
        "abstract": "  We propose a fast and simple explainable AI (XAI) method for point cloud\ndata. It computes pointwise importance with respect to a trained network\ndownstream task. This allows better understanding of the network properties,\nwhich is imperative for safety-critical applications. In addition to debugging\nand visualization, our low computational complexity facilitates online feedback\nto the network at inference. This can be used to reduce uncertainty and to\nincrease robustness. In this work, we introduce \\emph{Feature Based\nInterpretability} (FBI), where we compute the features' norm, per point, before\nthe bottleneck. We analyze the use of gradients and post- and pre-bottleneck\nstrategies, showing pre-bottleneck is preferred, in terms of smoothness and\nranking. We obtain at least three orders of magnitude speedup, compared to\ncurrent XAI methods, thus, scalable for big point clouds or large-scale\narchitectures. Our approach achieves SOTA results, in terms of classification\nexplainability. We demonstrate how the proposed measure is helpful in analyzing\nand characterizing various aspects of 3D learning, such as rotation invariance,\nrobustness to out-of-distribution (OOD) outliers or domain shift and dataset\nbias.\n"
    },
    {
        "title": "Robust Synthetic-to-Real Transfer for Stereo Matching",
        "published_time": "2024-03-12T14:50:05Z",
        "abstract": "  With advancements in domain generalized stereo matching networks, models\npre-trained on synthetic data demonstrate strong robustness to unseen domains.\nHowever, few studies have investigated the robustness after fine-tuning them in\nreal-world scenarios, during which the domain generalization ability can be\nseriously degraded. In this paper, we explore fine-tuning stereo matching\nnetworks without compromising their robustness to unseen domains. Our\nmotivation stems from comparing Ground Truth (GT) versus Pseudo Label (PL) for\nfine-tuning: GT degrades, but PL preserves the domain generalization ability.\nEmpirically, we find the difference between GT and PL implies valuable\ninformation that can regularize networks during fine-tuning. We also propose a\nframework to utilize this difference for fine-tuning, consisting of a frozen\nTeacher, an exponential moving average (EMA) Teacher, and a Student network.\nThe core idea is to utilize the EMA Teacher to measure what the Student has\nlearned and dynamically improve GT and PL for fine-tuning. We integrate our\nframework with state-of-the-art networks and evaluate its effectiveness on\nseveral real-world datasets. Extensive experiments show that our method\neffectively preserves the domain generalization ability during fine-tuning.\n"
    },
    {
        "title": "CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive\n  Self-Supervised Transformers",
        "published_time": "2024-03-12T14:46:03Z",
        "abstract": "  In this paper, we introduce VoteCut, an innovative method for unsupervised\nobject discovery that leverages feature representations from multiple\nself-supervised models. VoteCut employs normalized-cut based graph\npartitioning, clustering and a pixel voting approach. Additionally, We present\nCuVLER (Cut-Vote-and-LEaRn), a zero-shot model, trained using pseudo-labels,\ngenerated by VoteCut, and a novel soft target loss to refine segmentation\naccuracy. Through rigorous evaluations across multiple datasets and several\nunsupervised setups, our methods demonstrate significant improvements in\ncomparison to previous state-of-the-art models. Our ablation studies further\nhighlight the contributions of each component, revealing the robustness and\nefficacy of our approach. Collectively, VoteCut and CuVLER pave the way for\nfuture advancements in image segmentation.\n"
    },
    {
        "title": "Vivim: a Video Vision Mamba for Medical Video Object Segmentation",
        "published_time": "2024-01-25T13:27:03Z",
        "abstract": "  Traditional convolutional neural networks have a limited receptive field\nwhile transformer-based networks are mediocre in constructing long-term\ndependency from the perspective of computational complexity. Such the\nbottleneck poses a significant challenge when processing long sequences in\nvideo analysis tasks. Very recently, the state space models (SSMs) with\nefficient hardware-aware designs, famous by Mamba, have exhibited impressive\nachievements in long sequence modeling, which facilitates the development of\ndeep neural networks on many vision tasks. To better capture available dynamic\ncues in video frames, this paper presents a generic Video Vision Mamba-based\nframework, dubbed as \\textbf{Vivim}, for medical video object segmentation\ntasks. Our Vivim can effectively compress the long-term spatiotemporal\nrepresentation into sequences at varying scales by our designed Temporal Mamba\nBlock. We also introduce a boundary-aware constraint to enhance the\ndiscriminative ability of Vivim on ambiguous lesions in medical images.\nExtensive experiments on thyroid segmentation in ultrasound videos and polyp\nsegmentation in colonoscopy videos demonstrate the effectiveness and efficiency\nof our Vivim, superior to existing methods. The code is available at:\nhttps://github.com/scott-yjyang/Vivim.\n"
    },
    {
        "title": "Masked AutoDecoder is Effective Multi-Task Vision Generalist",
        "published_time": "2024-03-12T14:36:52Z",
        "abstract": "  Inspired by the success of general-purpose models in NLP, recent studies\nattempt to unify different vision tasks in the same sequence format and employ\nautoregressive Transformers for sequence prediction. They apply uni-directional\nattention to capture sequential dependencies and generate task sequences\nrecursively. However, such autoregressive Transformers may not fit vision tasks\nwell, as vision task sequences usually lack the sequential dependencies\ntypically observed in natural languages. In this work, we design Masked\nAutoDecoder~(MAD), an effective multi-task vision generalist. MAD consists of\ntwo core designs. First, we develop a parallel decoding framework that\nintroduces bi-directional attention to capture contextual dependencies\ncomprehensively and decode vision task sequences in parallel. Second, we design\na masked sequence modeling approach that learns rich task contexts by masking\nand reconstructing task sequences. In this way, MAD handles all the tasks by a\nsingle network branch and a simple cross-entropy loss with minimal\ntask-specific designs. Extensive experiments demonstrate the great potential of\nMAD as a new paradigm for unifying various vision tasks. MAD achieves superior\nperformance and inference efficiency compared to autoregressive counterparts\nwhile obtaining competitive accuracy with task-specific models. Code will be\nreleased.\n"
    },
    {
        "title": "Annotations on a Budget: Leveraging Geo-Data Similarity to Balance Model\n  Performance and Annotation Cost",
        "published_time": "2024-03-12T14:27:17Z",
        "abstract": "  Current foundation models have shown impressive performance across various\ntasks. However, several studies have revealed that these models are not\neffective for everyone due to the imbalanced geographical and economic\nrepresentation of the data used in the training process. Most of this data\ncomes from Western countries, leading to poor results for underrepresented\ncountries. To address this issue, more data needs to be collected from these\ncountries, but the cost of annotation can be a significant bottleneck. In this\npaper, we propose methods to identify the data to be annotated to balance model\nperformance and annotation costs. Our approach first involves finding the\ncountries with images of topics (objects and actions) most visually distinct\nfrom those already in the training datasets used by current large\nvision-language foundation models. Next, we identify countries with higher\nvisual similarity for these topics and show that using data from these\ncountries to supplement the training data improves model performance and\nreduces annotation costs. The resulting lists of countries and corresponding\ntopics are made available at\nhttps://github.com/MichiganNLP/visual_diversity_budget.\n"
    },
    {
        "title": "Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for\n  Video Adverse Weather Removal",
        "published_time": "2024-03-12T14:21:30Z",
        "abstract": "  Real-world vision tasks frequently suffer from the appearance of unexpected\nadverse weather conditions, including rain, haze, snow, and raindrops. In the\nlast decade, convolutional neural networks and vision transformers have yielded\noutstanding results in single-weather video removal. However, due to the\nabsence of appropriate adaptation, most of them fail to generalize to other\nweather conditions. Although ViWS-Net is proposed to remove adverse weather\nconditions in videos with a single set of pre-trained weights, it is seriously\nblinded by seen weather at train-time and degenerates when coming to unseen\nweather during test-time. In this work, we introduce test-time adaptation into\nadverse weather removal in videos, and propose the first framework that\nintegrates test-time adaptation into the iterative diffusion reverse process.\nSpecifically, we devise a diffusion-based network with a novel temporal noise\nmodel to efficiently explore frame-correlated information in degraded video\nclips at training stage. During inference stage, we introduce a proxy task\nnamed Diffusion Tubelet Self-Calibration to learn the primer distribution of\ntest video stream and optimize the model by approximating the temporal noise\nmodel for online adaptation. Experimental results, on benchmark datasets,\ndemonstrate that our Test-Time Adaptation method with Diffusion-based\nnetwork(Diff-TTA) outperforms state-of-the-art methods in terms of restoring\nvideos degraded by seen weather conditions. Its generalizable capability is\nalso validated with unseen weather conditions in both synthesized and\nreal-world videos.\n"
    },
    {
        "title": "Deep learning for multi-label classification of coral conditions in the\n  Indo-Pacific via underwater photogrammetry",
        "published_time": "2024-03-09T14:42:16Z",
        "abstract": "  Since coral reef ecosystems face threats from human activities and climate\nchange, coral conservation programs are implemented worldwide. Monitoring coral\nhealth provides references for guiding conservation activities. However,\ncurrent labor-intensive methods result in a backlog of unsorted images,\nhighlighting the need for automated classification. Few studies have\nsimultaneously utilized accurate annotations along with updated algorithms and\ndatasets. This study aimed to create a dataset representing common coral\nconditions and associated stressors in the Indo-Pacific. Concurrently, it\nassessed existing classification algorithms and proposed a new multi-label\nmethod for automatically detecting coral conditions and extracting ecological\ninformation. A dataset containing over 20,000 high-resolution coral images of\ndifferent health conditions and stressors was constructed based on the field\nsurvey. Seven representative deep learning architectures were tested on this\ndataset, and their performance was quantitatively evaluated using the F1 metric\nand the match ratio. Based on this evaluation, a new method utilizing the\nensemble learning approach was proposed. The proposed method accurately\nclassified coral conditions as healthy, compromised, dead, and rubble; it also\nidentified corresponding stressors, including competition, disease, predation,\nand physical issues. This method can help develop the coral image archive,\nguide conservation activities, and provide references for decision-making for\nreef managers and conservationists. The proposed ensemble learning approach\noutperforms others on the dataset, showing State-Of-The-Art (SOTA) performance.\nFuture research should improve its generalizability and accuracy to support\nglobal coral conservation efforts.\n"
    },
    {
        "title": "Examining Pathological Bias in a Generative Adversarial Network\n  Discriminator: A Case Study on a StyleGAN3 Model",
        "published_time": "2024-02-15T08:34:21Z",
        "abstract": "  Generative adversarial networks (GANs) generate photorealistic faces that are\noften indistinguishable by humans from real faces. While biases in machine\nlearning models are often assumed to be due to biases in training data, we find\npathological internal color and luminance biases in the discriminator of a\npre-trained StyleGAN3-r model that are not explicable by the training data. We\nalso find that the discriminator systematically stratifies scores by both\nimage- and face-level qualities and that this disproportionately affects images\nacross gender, race, and other categories. We examine axes common in research\non stereotyping in social psychology.\n"
    },
    {
        "title": "Decomposing Disease Descriptions for Enhanced Pathology Detection: A\n  Multi-Aspect Vision-Language Matching Framework",
        "published_time": "2024-03-12T13:18:22Z",
        "abstract": "  Medical vision language pre-training (VLP) has emerged as a frontier of\nresearch, enabling zero-shot pathological recognition by comparing the query\nimage with the textual descriptions for each disease. Due to the complex\nsemantics of biomedical texts, current methods struggle to align medical images\nwith key pathological findings in unstructured reports. This leads to the\nmisalignment with the target disease's textual representation. In this paper,\nwe introduce a novel VLP framework designed to dissect disease descriptions\ninto their fundamental aspects, leveraging prior knowledge about the visual\nmanifestations of pathologies. This is achieved by consulting a large language\nmodel and medical experts. Integrating a Transformer module, our approach\naligns an input image with the diverse elements of a disease, generating\naspect-centric image representations. By consolidating the matches from each\naspect, we improve the compatibility between an image and its associated\ndisease. Additionally, capitalizing on the aspect-oriented representations, we\npresent a dual-head Transformer tailored to process known and unknown diseases,\noptimizing the comprehensive detection efficacy. Conducting experiments on\nseven downstream datasets, ours outperforms recent methods by up to 8.07% and\n11.23% in AUC scores for seen and novel categories, respectively. Our code is\nreleased at\n\\href{https://github.com/HieuPhan33/MAVL}{https://github.com/HieuPhan33/MAVL}.\n"
    },
    {
        "title": "Hunting Attributes: Context Prototype-Aware Learning for Weakly\n  Supervised Semantic Segmentation",
        "published_time": "2024-03-12T13:11:58Z",
        "abstract": "  Recent weakly supervised semantic segmentation (WSSS) methods strive to\nincorporate contextual knowledge to improve the completeness of class\nactivation maps (CAM). In this work, we argue that the knowledge bias between\ninstances and contexts affects the capability of the prototype to sufficiently\nunderstand instance semantics. Inspired by prototype learning theory, we\npropose leveraging prototype awareness to capture diverse and fine-grained\nfeature attributes of instances. The hypothesis is that contextual prototypes\nmight erroneously activate similar and frequently co-occurring object\ncategories due to this knowledge bias. Therefore, we propose to enhance the\nprototype representation ability by mitigating the bias to better capture\nspatial coverage in semantic object regions. With this goal, we present a\nContext Prototype-Aware Learning (CPAL) strategy, which leverages semantic\ncontext to enrich instance comprehension. The core of this method is to\naccurately capture intra-class variations in object features through\ncontext-aware prototypes, facilitating the adaptation to the semantic\nattributes of various instances. We design feature distribution alignment to\noptimize prototype awareness, aligning instance feature distributions with\ndense features. In addition, a unified training framework is proposed to\ncombine label-guided classification supervision and prototypes-guided\nself-supervision. Experimental results on PASCAL VOC 2012 and MS COCO 2014 show\nthat CPAL significantly improves off-the-shelf methods and achieves\nstate-of-the-art performance. The project is available at\nhttps://github.com/Barrett-python/CPAL.\n"
    },
    {
        "title": "Multiple Latent Space Mapping for Compressed Dark Image Enhancement",
        "published_time": "2024-03-12T13:05:51Z",
        "abstract": "  Dark image enhancement aims at converting dark images to normal-light images.\nExisting dark image enhancement methods take uncompressed dark images as inputs\nand achieve great performance. However, in practice, dark images are often\ncompressed before storage or transmission over the Internet. Current methods\nget poor performance when processing compressed dark images. Artifacts hidden\nin the dark regions are amplified by current methods, which results in\nuncomfortable visual effects for observers. Based on this observation, this\nstudy aims at enhancing compressed dark images while avoiding compression\nartifacts amplification. Since texture details intertwine with compression\nartifacts in compressed dark images, detail enhancement and blocking artifacts\nsuppression contradict each other in image space. Therefore, we handle the task\nin latent space. To this end, we propose a novel latent mapping network based\non variational auto-encoder (VAE). Firstly, different from previous VAE-based\nmethods with single-resolution features only, we exploit multiple latent spaces\nwith multi-resolution features, to reduce the detail blur and improve image\nfidelity. Specifically, we train two multi-level VAEs to project compressed\ndark images and normal-light images into their latent spaces respectively.\nSecondly, we leverage a latent mapping network to transform features from\ncompressed dark space to normal-light space. Specifically, since the\ndegradation models of darkness and compression are different from each other,\nthe latent mapping process is divided mapping into enlightening branch and\ndeblocking branch. Comprehensive experiments demonstrate that the proposed\nmethod achieves state-of-the-art performance in compressed dark image\nenhancement.\n"
    },
    {
        "title": "Smartphone region-wise image indoor localization using deep learning for\n  indoor tourist attraction",
        "published_time": "2024-03-12T13:04:37Z",
        "abstract": "  Smart indoor tourist attractions, such as smart museums and aquariums,\nusually require a significant investment in indoor localization devices. The\nsmartphone Global Positional Systems use is unsuitable for scenarios where\ndense materials such as concrete and metal block weaken the GPS signals, which\nis the most common scenario in an indoor tourist attraction. Deep learning\nmakes it possible to perform region-wise indoor localization using smartphone\nimages. This approach does not require any investment in infrastructure,\nreducing the cost and time to turn museums and aquariums into smart museums or\nsmart aquariums. This paper proposes using deep learning algorithms to classify\nlocations using smartphone camera images for indoor tourism attractions. We\nevaluate our proposal in a real-world scenario in Brazil. We extensively\ncollect images from ten different smartphones to classify biome-themed fish\ntanks inside the Pantanal Biopark, creating a new dataset of 3654 images. We\ntested seven state-of-the-art neural networks, three being transformer-based,\nachieving precision around 90% on average and recall and f-score around 89% on\naverage. The results indicate good feasibility of the proposal in a most indoor\ntourist attractions.\n"
    },
    {
        "title": "Toward Generalist Anomaly Detection via In-context Residual Learning\n  with Few-shot Sample Prompts",
        "published_time": "2024-03-11T08:07:46Z",
        "abstract": "  This paper explores the problem of Generalist Anomaly Detection (GAD), aiming\nto train one single detection model that can generalize to detect anomalies in\ndiverse datasets from different application domains without any further\ntraining on the target data. Some recent studies have shown that large\npre-trained Visual-Language Models (VLMs) like CLIP have strong generalization\ncapabilities on detecting industrial defects from various datasets, but their\nmethods rely heavily on handcrafted text prompts about defects, making them\ndifficult to generalize to anomalies in other applications, e.g., medical image\nanomalies or semantic anomalies in natural images. In this work, we propose to\ntrain a GAD model with few-shot normal images as sample prompts for AD on\ndiverse datasets on the fly. To this end, we introduce a novel approach that\nlearns an in-context residual learning model for GAD, termed InCTRL. It is\ntrained on an auxiliary dataset to discriminate anomalies from normal samples\nbased on a holistic evaluation of the residuals between query images and\nfew-shot normal sample prompts. Regardless of the datasets, per definition of\nanomaly, larger residuals are expected for anomalies than normal samples,\nthereby enabling InCTRL to generalize across different domains without further\ntraining. Comprehensive experiments on nine AD datasets are performed to\nestablish a GAD benchmark that encapsulate the detection of industrial defect\nanomalies, medical anomalies, and semantic anomalies in both one-vs-all and\nmulti-class setting, on which InCTRL is the best performer and significantly\noutperforms state-of-the-art competing methods.\n"
    },
    {
        "title": "Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in\n  Text-To-Image Generation",
        "published_time": "2024-03-12T12:44:34Z",
        "abstract": "  In text-to-image generation, using negative prompts, which describe\nundesirable image characteristics, can significantly boost image quality.\nHowever, producing good negative prompts is manual and tedious. To address\nthis, we propose NegOpt, a novel method for optimizing negative prompt\ngeneration toward enhanced image generation, using supervised fine-tuning and\nreinforcement learning. Our combined approach results in a substantial increase\nof 25% in Inception Score compared to other approaches and surpasses\nground-truth negative prompts from the test set. Furthermore, with NegOpt we\ncan preferentially optimize the metrics most important to us. Finally, we\nconstruct Negative Prompts DB, a dataset of negative prompts.\n"
    },
    {
        "title": "Unified Source-Free Domain Adaptation",
        "published_time": "2024-03-12T12:40:08Z",
        "abstract": "  In the pursuit of transferring a source model to a target domain without\naccess to the source training data, Source-Free Domain Adaptation (SFDA) has\nbeen extensively explored across various scenarios, including closed-set,\nopen-set, partial-set, and generalized settings. Existing methods, focusing on\nspecific scenarios, not only address only a subset of challenges but also\nnecessitate prior knowledge of the target domain, significantly limiting their\npractical utility and deployability. In light of these considerations, we\nintroduce a more practical yet challenging problem, termed unified SFDA, which\ncomprehensively incorporates all specific scenarios in a unified manner. To\ntackle this unified SFDA problem, we propose a novel approach called Latent\nCausal Factors Discovery (LCFD). In contrast to previous alternatives that\nemphasize learning the statistical description of reality, we formulate LCFD\nfrom a causality perspective. The objective is to uncover the causal\nrelationships between latent variables and model decisions, enhancing the\nreliability and robustness of the learned model against domain shifts. To\nintegrate extensive world knowledge, we leverage a pre-trained vision-language\nmodel such as CLIP. This aids in the formation and discovery of latent causal\nfactors in the absence of supervision in the variation of distribution and\nsemantics, coupled with a newly designed information bottleneck with\ntheoretical guarantees. Extensive experiments demonstrate that LCFD can achieve\nnew state-of-the-art results in distinct SFDA settings, as well as source-free\nout-of-distribution generalization.Our code and data are available at\nhttps://github.com/tntek/source-free-domain-adaptation.\n"
    },
    {
        "title": "Mondrian: On-Device High-Performance Video Analytics with Compressive\n  Packed Inference",
        "published_time": "2024-03-12T12:35:12Z",
        "abstract": "  In this paper, we present Mondrian, an edge system that enables\nhigh-performance object detection on high-resolution video streams. Many\nlightweight models and system optimization techniques have been proposed for\nresource-constrained devices, but they do not fully utilize the potential of\nthe accelerators over dynamic, high-resolution videos. To enable such\ncapability, we devise a novel Compressive Packed Inference to minimize\nper-pixel processing costs by selectively determining the necessary pixels to\nprocess and combining them to maximize processing parallelism. In particular,\nour system quickly extracts ROIs and dynamically shrinks them, reflecting the\neffect of the fast-changing characteristics of objects and scenes. It then\nintelligently combines such scaled ROIs into large canvases to maximize the\nutilization of inference accelerators such as GPU. Evaluation across various\ndatasets, models, and devices shows Mondrian outperforms state-of-the-art\nbaselines (e.g., input rescaling, ROI extractions, ROI extractions+batching) by\n15.0-19.7% higher accuracy, leading to $\\times$6.65 higher throughput than\nframe-wise inference for processing various 1080p video streams. We will\nrelease the code after the paper review.\n"
    },
    {
        "title": "Accurate Spatial Gene Expression Prediction by integrating\n  Multi-resolution features",
        "published_time": "2024-03-12T12:25:38Z",
        "abstract": "  Recent advancements in Spatial Transcriptomics (ST) technology have\nfacilitated detailed gene expression analysis within tissue contexts. However,\nthe high costs and methodological limitations of ST necessitate a more robust\npredictive model. In response, this paper introduces TRIPLEX, a novel deep\nlearning framework designed to predict spatial gene expression from Whole Slide\nImages (WSIs). TRIPLEX uniquely harnesses multi-resolution features, capturing\ncellular morphology at individual spots, the local context around these spots,\nand the global tissue organization. By integrating these features through an\neffective fusion strategy, TRIPLEX achieves accurate gene expression\nprediction. Our comprehensive benchmark study, conducted on three public ST\ndatasets and supplemented with Visium data from 10X Genomics, demonstrates that\nTRIPLEX outperforms current state-of-the-art models in Mean Squared Error\n(MSE), Mean Absolute Error (MAE), and Pearson Correlation Coefficient (PCC).\nThe model's predictions align closely with ground truth gene expression\nprofiles and tumor annotations, underscoring TRIPLEX's potential in advancing\ncancer diagnosis and treatment.\n"
    },
    {
        "title": "PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral\n  Convolution",
        "published_time": "2024-03-12T12:19:05Z",
        "abstract": "  Recently, some large kernel convnets strike back with appealing performance\nand efficiency. However, given the square complexity of convolution, scaling up\nkernels can bring about an enormous amount of parameters and the proliferated\nparameters can induce severe optimization problem. Due to these issues, current\nCNNs compromise to scale up to 51x51 in the form of stripe convolution (i.e.,\n51x5 + 5x51) and start to saturate as the kernel size continues growing. In\nthis paper, we delve into addressing these vital issues and explore whether we\ncan continue scaling up kernels for more performance gains. Inspired by human\nvision, we propose a human-like peripheral convolution that efficiently reduces\nover 90% parameter count of dense grid convolution through parameter sharing,\nand manage to scale up kernel size to extremely large. Our peripheral\nconvolution behaves highly similar to human, reducing the complexity of\nconvolution from O(K^2) to O(logK) without backfiring performance. Built on\nthis, we propose Parameter-efficient Large Kernel Network (PeLK). Our PeLK\noutperforms modern vision Transformers and ConvNet architectures like Swin,\nConvNeXt, RepLKNet and SLaK on various vision tasks including ImageNet\nclassification, semantic segmentation on ADE20K and object detection on MS\nCOCO. For the first time, we successfully scale up the kernel size of CNNs to\nan unprecedented 101x101 and demonstrate consistent improvements.\n"
    },
    {
        "title": "AACP: Aesthetics assessment of children's paintings based on\n  self-supervised learning",
        "published_time": "2024-03-12T12:07:00Z",
        "abstract": "  The Aesthetics Assessment of Children's Paintings (AACP) is an important\nbranch of the image aesthetics assessment (IAA), playing a significant role in\nchildren's education. This task presents unique challenges, such as limited\navailable data and the requirement for evaluation metrics from multiple\nperspectives. However, previous approaches have relied on training large\ndatasets and subsequently providing an aesthetics score to the image, which is\nnot applicable to AACP. To solve this problem, we construct an aesthetics\nassessment dataset of children's paintings and a model based on self-supervised\nlearning. 1) We build a novel dataset composed of two parts: the first part\ncontains more than 20k unlabeled images of children's paintings; the second\npart contains 1.2k images of children's paintings, and each image contains\neight attributes labeled by multiple design experts. 2) We design a pipeline\nthat includes a feature extraction module, perception modules and a\ndisentangled evaluation module. 3) We conduct both qualitative and quantitative\nexperiments to compare our model's performance with five other methods using\nthe AACP dataset. Our experiments reveal that our method can accurately capture\naesthetic features and achieve state-of-the-art performance.\n"
    },
    {
        "title": "FPT: Fine-grained Prompt Tuning for Parameter and Memory Efficient Fine\n  Tuning in High-resolution Medical Image Classification",
        "published_time": "2024-03-12T12:05:43Z",
        "abstract": "  Parameter-efficient fine-tuning (PEFT) is proposed as a cost-effective way to\ntransfer pre-trained models to downstream tasks, avoiding the high cost of\nupdating entire large-scale pre-trained models (LPMs). In this work, we present\nFine-grained Prompt Tuning (FPT), a novel PEFT method for medical image\nclassification. FPT significantly reduces memory consumption compared to other\nPEFT methods, especially in high-resolution contexts. To achieve this, we first\nfreeze the weights of the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM takes high-resolution images as input to extract\nfine-grained features, while the side network is fed low-resolution images to\nreduce memory usage. To allow the side network to access pre-trained knowledge,\nwe introduce fine-grained prompts that summarize information from the LPM\nthrough a fusion module. Important tokens selection and preloading techniques\nare employed to further reduce training cost and memory requirements. We\nevaluate FPT on four medical datasets with varying sizes, modalities, and\ncomplexities. Experimental results demonstrate that FPT achieves comparable\nperformance to fine-tuning the entire LPM while using only 1.8% of the\nlearnable parameters and 13% of the memory costs of an encoder ViT-B model with\na 512 x 512 input resolution.\n"
    },
    {
        "title": "An Active Contour Model Driven By the Hybrid Signed Pressure Function",
        "published_time": "2024-03-12T11:58:37Z",
        "abstract": "  Due to the influence of imaging equipment and complex imaging environments,\nmost images in daily life have features of intensity inhomogeneity and noise.\nTherefore, many scholars have designed many image segmentation algorithms to\naddress these issues. Among them, the active contour model is one of the most\neffective image segmentation algorithms.This paper proposes an active contour\nmodel driven by the hybrid signed pressure function that combines global and\nlocal information construction. Firstly, a new global region-based signed\npressure function is introduced by combining the average intensity of the inner\nand outer regions of the curve with the median intensity of the inner region of\nthe evolution curve. Then, the paper uses the energy differences between the\ninner and outer regions of the curve in the local region to design the signed\npressure function of the local term. Combine the two SPF function to obtain a\nnew signed pressure function and get the evolution equation of the new model.\nFinally, experiments and numerical analysis show that the model has excellent\nsegmentation performance for both intensity inhomogeneous images and noisy\nimages.\n"
    },
    {
        "title": "Exploring Challenges in Deep Learning of Single-Station Ground Motion\n  Records",
        "published_time": "2024-03-12T11:56:50Z",
        "abstract": "  Contemporary deep learning models have demonstrated promising results across\nvarious applications within seismology and earthquake engineering. These models\nrely primarily on utilizing ground motion records for tasks such as earthquake\nevent classification, localization, earthquake early warning systems, and\nstructural health monitoring. However, the extent to which these models\neffectively learn from these complex time-series signals has not been\nthoroughly analyzed. In this study, our objective is to evaluate the degree to\nwhich auxiliary information, such as seismic phase arrival times or seismic\nstation distribution within a network, dominates the process of deep learning\nfrom ground motion records, potentially hindering its effectiveness. We perform\na hyperparameter search on two deep learning models to assess their\neffectiveness in deep learning from ground motion records while also examining\nthe impact of auxiliary information on model performance. Experimental results\nreveal a strong reliance on the highly correlated P and S phase arrival\ninformation. Our observations highlight a potential gap in the field,\nindicating an absence of robust methodologies for deep learning of\nsingle-station ground motion recordings independent of any auxiliary\ninformation.\n"
    },
    {
        "title": "RSBuilding: Towards General Remote Sensing Image Building Extraction and\n  Change Detection with Foundation Model",
        "published_time": "2024-03-12T11:51:59Z",
        "abstract": "  The intelligent interpretation of buildings plays a significant role in urban\nplanning and management, macroeconomic analysis, population dynamics, etc.\nRemote sensing image building interpretation primarily encompasses building\nextraction and change detection. However, current methodologies often treat\nthese two tasks as separate entities, thereby failing to leverage shared\nknowledge. Moreover, the complexity and diversity of remote sensing image\nscenes pose additional challenges, as most algorithms are designed to model\nindividual small datasets, thus lacking cross-scene generalization. In this\npaper, we propose a comprehensive remote sensing image building understanding\nmodel, termed RSBuilding, developed from the perspective of the foundation\nmodel. RSBuilding is designed to enhance cross-scene generalization and task\nuniversality. Specifically, we extract image features based on the prior\nknowledge of the foundation model and devise a multi-level feature sampler to\naugment scale information. To unify task representation and integrate image\nspatiotemporal clues, we introduce a cross-attention decoder with task prompts.\nAddressing the current shortage of datasets that incorporate annotations for\nboth tasks, we have developed a federated training strategy to facilitate\nsmooth model convergence even when supervision for some tasks is missing,\nthereby bolstering the complementarity of different tasks. Our model was\ntrained on a dataset comprising up to 245,000 images and validated on multiple\nbuilding extraction and change detection datasets. The experimental results\nsubstantiate that RSBuilding can concurrently handle two structurally distinct\ntasks and exhibits robust zero-shot generalization capabilities.\n"
    },
    {
        "title": "Learning Generalizable Feature Fields for Mobile Manipulation",
        "published_time": "2024-03-12T11:51:55Z",
        "abstract": "  An open problem in mobile manipulation is how to represent objects and scenes\nin a unified manner, so that robots can use it both for navigating in the\nenvironment and manipulating objects. The latter requires capturing intricate\ngeometry while understanding fine-grained semantics, whereas the former\ninvolves capturing the complexity inherit to an expansive physical scale. In\nthis work, we present GeFF (Generalizable Feature Fields), a scene-level\ngeneralizable neural feature field that acts as a unified representation for\nboth navigation and manipulation that performs in real-time. To do so, we treat\ngenerative novel view synthesis as a pre-training task, and then align the\nresulting rich scene priors with natural language via CLIP feature\ndistillation. We demonstrate the effectiveness of this approach by deploying\nGeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's\nability to generalize to open-set objects as well as running time, when\nperforming open-vocabulary mobile manipulation in dynamic scenes.\n"
    },
    {
        "title": "WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing",
        "published_time": "2024-01-24T16:10:14Z",
        "abstract": "  WiFi-based human sensing has exhibited remarkable potential to analyze user\nbehaviors in a non-intrusive and device-free manner, benefiting applications as\ndiverse as smart homes and healthcare. However, most previous works focus on\nsingle-user sensing, which has limited practicability in scenarios involving\nmultiple users. Although recent studies have begun to investigate WiFi-based\nmulti-user sensing, there remains a lack of benchmark datasets to facilitate\nreproducible and comparable research. To bridge this gap, we present WiMANS, to\nour knowledge, the first dataset for multi-user sensing based on WiFi. WiMANS\ncontains over 9.4 hours of dual-band WiFi Channel State Information (CSI), as\nwell as synchronized videos, monitoring simultaneous activities of multiple\nusers. We exploit WiMANS to benchmark the performance of state-of-the-art\nWiFi-based human sensing models and video-based models, posing new challenges\nand opportunities for future work. We believe WiMANS can push the boundaries of\ncurrent studies and catalyze the research on WiFi-based multi-user sensing.\n"
    },
    {
        "title": "Reducing self-supervised learning complexity improves weakly-supervised\n  classification performance in computational pathology",
        "published_time": "2024-03-07T14:56:06Z",
        "abstract": "  Deep Learning models have been successfully utilized to extract clinically\nactionable insights from routinely available histology data. Generally, these\nmodels require annotations performed by clinicians, which are scarce and costly\nto generate. The emergence of self-supervised learning (SSL) methods remove\nthis barrier, allowing for large-scale analyses on non-annotated data. However,\nrecent SSL approaches apply increasingly expansive model architectures and\nlarger datasets, causing the rapid escalation of data volumes, hardware\nprerequisites, and overall expenses, limiting access to these resources to few\ninstitutions. Therefore, we investigated the complexity of contrastive SSL in\ncomputational pathology in relation to classification performance with the\nutilization of consumer-grade hardware. Specifically, we analyzed the effects\nof adaptations in data volume, architecture, and algorithms on downstream\nclassification tasks, emphasizing their impact on computational resources. We\ntrained breast cancer foundation models on a large public patient cohort and\nvalidated them on various downstream classification tasks in a weakly\nsupervised manner on two external public patient cohorts. Our experiments\ndemonstrate that we can improve downstream classification performance whilst\nreducing SSL training duration by 90%. In summary, we propose a set of\nadaptations which enable the utilization of SSL in computational pathology in\nnon-resource abundant environments.\n"
    },
    {
        "title": "The future of document indexing: GPT and Donut revolutionize table of\n  content processing",
        "published_time": "2024-03-12T11:39:18Z",
        "abstract": "  Industrial projects rely heavily on lengthy, complex specification documents,\nmaking tedious manual extraction of structured information a major bottleneck.\nThis paper introduces an innovative approach to automate this process,\nleveraging the capabilities of two cutting-edge AI models: Donut, a model that\nextracts information directly from scanned documents without OCR, and OpenAI\nGPT-3.5 Turbo, a robust large language model. The proposed methodology is\ninitiated by acquiring the table of contents (ToCs) from construction\nspecification documents and subsequently structuring the ToCs text into JSON\ndata. Remarkable accuracy is achieved, with Donut reaching 85% and GPT-3.5\nTurbo reaching 89% in effectively organizing the ToCs. This landmark\nachievement represents a significant leap forward in document indexing,\ndemonstrating the immense potential of AI to automate information extraction\ntasks across diverse document types, boosting efficiency and liberating\ncritical resources in various industries.\n"
    },
    {
        "title": "SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields",
        "published_time": "2024-03-12T11:32:57Z",
        "abstract": "  Neural radiance fields (NeRF) has attracted considerable attention for their\nexceptional ability in synthesizing novel views with high fidelity. However,\nthe presence of motion blur, resulting from slight camera movements during\nextended shutter exposures, poses a significant challenge, potentially\ncompromising the quality of the reconstructed 3D scenes. While recent studies\nhave addressed this issue, they do not consider the continuous dynamics of\ncamera movements during image acquisition, leading to inaccurate scene\nreconstruction. Additionally, these methods are plagued by slow training and\nrendering speed. To effectively handle these issues, we propose sequential\nmotion understanding radiance fields (SMURF), a novel approach that employs\nneural ordinary differential equation (Neural-ODE) to model continuous camera\nmotion and leverages the explicit volumetric representation method for faster\ntraining and robustness to motion-blurred input images. The core idea of the\nSMURF is continuous motion blurring kernel (CMBK), a unique module designed to\nmodel a continuous camera movements for processing blurry inputs. Our model,\nrigorously evaluated against benchmark datasets, demonstrates state-of-the-art\nperformance both quantitatively and qualitatively.\n"
    },
    {
        "title": "A Survey of Vision Transformers in Autonomous Driving: Current Trends\n  and Future Directions",
        "published_time": "2024-03-12T11:29:40Z",
        "abstract": "  This survey explores the adaptation of visual transformer models in\nAutonomous Driving, a transition inspired by their success in Natural Language\nProcessing. Surpassing traditional Recurrent Neural Networks in tasks like\nsequential image processing and outperforming Convolutional Neural Networks in\nglobal context capture, as evidenced in complex scene recognition, Transformers\nare gaining traction in computer vision. These capabilities are crucial in\nAutonomous Driving for real-time, dynamic visual scene processing. Our survey\nprovides a comprehensive overview of Vision Transformer applications in\nAutonomous Driving, focusing on foundational concepts such as self-attention,\nmulti-head attention, and encoder-decoder architecture. We cover applications\nin object detection, segmentation, pedestrian detection, lane detection, and\nmore, comparing their architectural merits and limitations. The survey\nconcludes with future research directions, highlighting the growing role of\nVision Transformers in Autonomous Driving.\n"
    },
    {
        "title": "IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of\n  brain MR images",
        "published_time": "2024-02-05T17:38:49Z",
        "abstract": "  In MRI studies, the aggregation of imaging data from multiple acquisition\nsites enhances sample size but may introduce site-related variabilities that\nhinder consistency in subsequent analyses. Deep learning methods for image\ntranslation have emerged as a solution for harmonizing MR images across sites.\nIn this study, we introduce IGUANe (Image Generation with Unified Adversarial\nNetworks), an original 3D model that leverages the strengths of domain\ntranslation and straightforward application of style transfer methods for\nmulticenter brain MR image harmonization. IGUANe extends CycleGAN architecture\nby integrating an arbitrary number of domains for training through a\nmany-to-one strategy. During inference, the model can be applied to any image,\neven from an unknown acquisition site, making it a universal generator for\nharmonization. Trained on a dataset comprising T1-weighted images from 11\ndifferent scanners, IGUANe was evaluated on data from unseen sites. The\nassessments included the transformation of MR images with traveling subjects,\nthe preservation of pairwise distances between MR images within domains, the\nevolution of volumetric patterns related to age and Alzheimer$^\\prime$s disease\n(AD), and the performance in age regression and patient classification tasks.\nComparisons with other harmonization and normalization methods suggest that\nIGUANe better preserves individual information in MR images and is more\nsuitable for maintaining and reinforcing variabilities related to age and AD.\nFuture studies may further assess IGUANe in other multicenter contexts, either\nusing the same model or retraining it for applications to different image\nmodalities.\n"
    },
    {
        "title": "LaB-GATr: geometric algebra transformers for large biomedical surface\n  and volume meshes",
        "published_time": "2024-03-12T11:19:46Z",
        "abstract": "  Many anatomical structures can be described by surface or volume meshes.\nMachine learning is a promising tool to extract information from these 3D\nmodels. However, high-fidelity meshes often contain hundreds of thousands of\nvertices, which creates unique challenges in building deep neural network\narchitectures. Furthermore, patient-specific meshes may not be canonically\naligned which limits the generalisation of machine learning algorithms. We\npropose LaB-GATr, a transfomer neural network with geometric tokenisation that\ncan effectively learn with large-scale (bio-)medical surface and volume meshes\nthrough sequence compression and interpolation. Our method extends the recently\nproposed geometric algebra transformer (GATr) and thus respects all Euclidean\nsymmetries, i.e. rotation, translation and reflection, effectively mitigating\nthe problem of canonical alignment between patients. LaB-GATr achieves\nstate-of-the-art results on three tasks in cardiovascular hemodynamics\nmodelling and neurodevelopmental phenotype prediction, featuring meshes of up\nto 200,000 vertices. Our results demonstrate that LaB-GATr is a powerful\narchitecture for learning with high-fidelity meshes which has the potential to\nenable interesting downstream applications. Our implementation is publicly\navailable.\n"
    },
    {
        "title": "Adaptive Fusion of Single-View and Multi-View Depth for Autonomous\n  Driving",
        "published_time": "2024-03-12T11:18:35Z",
        "abstract": "  Multi-view depth estimation has achieved impressive performance over various\nbenchmarks. However, almost all current multi-view systems rely on given ideal\ncamera poses, which are unavailable in many real-world scenarios, such as\nautonomous driving. In this work, we propose a new robustness benchmark to\nevaluate the depth estimation system under various noisy pose settings.\nSurprisingly, we find current multi-view depth estimation methods or\nsingle-view and multi-view fusion methods will fail when given noisy pose\nsettings. To address this challenge, we propose a single-view and multi-view\nfused depth estimation system, which adaptively integrates high-confident\nmulti-view and single-view results for both robust and accurate depth\nestimations. The adaptive fusion module performs fusion by dynamically\nselecting high-confidence regions between two branches based on a wrapping\nconfidence map. Thus, the system tends to choose the more reliable branch when\nfacing textureless scenes, inaccurate calibration, dynamic objects, and other\ndegradation or challenging conditions. Our method outperforms state-of-the-art\nmulti-view and fusion methods under robustness testing. Furthermore, we achieve\nstate-of-the-art performance on challenging benchmarks (KITTI and DDAD) when\ngiven accurate pose estimations. Project website:\nhttps://github.com/Junda24/AFNet/.\n"
    },
    {
        "title": "Open-World Semantic Segmentation Including Class Similarity",
        "published_time": "2024-03-12T11:11:19Z",
        "abstract": "  Interpreting camera data is key for autonomously acting systems, such as\nautonomous vehicles. Vision systems that operate in real-world environments\nmust be able to understand their surroundings and need the ability to deal with\nnovel situations. This paper tackles open-world semantic segmentation, i.e.,\nthe variant of interpreting image data in which objects occur that have not\nbeen seen during training. We propose a novel approach that performs accurate\nclosed-world semantic segmentation and, at the same time, can identify new\ncategories without requiring any additional training data. Our approach\nadditionally provides a similarity measure for every newly discovered class in\nan image to a known category, which can be useful information in downstream\ntasks such as planning or mapping. Through extensive experiments, we show that\nour model achieves state-of-the-art results on classes known from training data\nas well as for anomaly segmentation and can distinguish between different\nunknown classes.\n"
    },
    {
        "title": "COVID-19 Computer-aided Diagnosis through AI-assisted CT Imaging\n  Analysis: Deploying a Medical AI System",
        "published_time": "2024-03-10T15:54:45Z",
        "abstract": "  Computer-aided diagnosis (CAD) systems stand out as potent aids for\nphysicians in identifying the novel Coronavirus Disease 2019 (COVID-19) through\nmedical imaging modalities. In this paper, we showcase the integration and\nreliable and fast deployment of a state-of-the-art AI system designed to\nautomatically analyze CT images, offering infection probability for the swift\ndetection of COVID-19. The suggested system, comprising both classification and\nsegmentation components, is anticipated to reduce physicians' detection time\nand enhance the overall efficiency of COVID-19 detection. We successfully\nsurmounted various challenges, such as data discrepancy and anonymisation,\ntesting the time-effectiveness of the model, and data security, enabling\nreliable and scalable deployment of the system on both cloud and edge\nenvironments. Additionally, our AI system assigns a probability of infection to\neach 3D CT scan and enhances explainability through anchor set similarity,\nfacilitating timely confirmation and segregation of infected patients by\nphysicians.\n"
    },
    {
        "title": "Open-Vocabulary Scene Text Recognition via Pseudo-Image Labeling and\n  Margin Loss",
        "published_time": "2024-03-12T10:54:38Z",
        "abstract": "  Scene text recognition is an important and challenging task in computer\nvision. However, most prior works focus on recognizing pre-defined words, while\nthere are various out-of-vocabulary (OOV) words in real-world applications.\n  In this paper, we propose a novel open-vocabulary text recognition framework,\nPseudo-OCR, to recognize OOV words. The key challenge in this task is the lack\nof OOV training data. To solve this problem, we first propose a pseudo label\ngeneration module that leverages character detection and image inpainting to\nproduce substantial pseudo OOV training data from real-world images. Unlike\nprevious synthetic data, our pseudo OOV data contains real characters and\nbackgrounds to simulate real-world applications. Secondly, to reduce noises in\npseudo data, we present a semantic checking mechanism to filter semantically\nmeaningful data. Thirdly, we introduce a quality-aware margin loss to boost the\ntraining with pseudo data. Our loss includes a margin-based part to enhance the\nclassification ability, and a quality-aware part to penalize low-quality\nsamples in both real and pseudo data.\n  Extensive experiments demonstrate that our approach outperforms the\nstate-of-the-art on eight datasets and achieves the first rank in the ICDAR2022\nchallenge.\n"
    },
    {
        "title": "D4D: An RGBD diffusion model to boost monocular depth estimation",
        "published_time": "2024-03-12T10:47:53Z",
        "abstract": "  Ground-truth RGBD data are fundamental for a wide range of computer vision\napplications; however, those labeled samples are difficult to collect and\ntime-consuming to produce. A common solution to overcome this lack of data is\nto employ graphic engines to produce synthetic proxies; however, those data do\nnot often reflect real-world images, resulting in poor performance of the\ntrained models at the inference step. In this paper we propose a novel training\npipeline that incorporates Diffusion4D (D4D), a customized 4-channels diffusion\nmodel able to generate realistic RGBD samples. We show the effectiveness of the\ndeveloped solution in improving the performances of deep learning models on the\nmonocular depth estimation task, where the correspondence between RGB and depth\nmap is crucial to achieving accurate measurements. Our supervised training\npipeline, enriched by the generated samples, outperforms synthetic and original\ndata performances achieving an RMSE reduction of (8.2%, 11.9%) and (8.1%, 6.1%)\nrespectively on the indoor NYU Depth v2 and the outdoor KITTI dataset.\n"
    },
    {
        "title": "Uncertainty-guided Contrastive Learning for Single Source Domain\n  Generalisation",
        "published_time": "2024-03-12T10:47:45Z",
        "abstract": "  In the context of single domain generalisation, the objective is for models\nthat have been exclusively trained on data from a single domain to demonstrate\nstrong performance when confronted with various unfamiliar domains. In this\npaper, we introduce a novel model referred to as Contrastive Uncertainty Domain\nGeneralisation Network (CUDGNet). The key idea is to augment the source\ncapacity in both input and label spaces through the fictitious domain generator\nand jointly learn the domain invariant representation of each class through\ncontrastive learning. Extensive experiments on two Single Source Domain\nGeneralisation (SSDG) datasets demonstrate the effectiveness of our approach,\nwhich surpasses the state-of-the-art single-DG methods by up to $7.08\\%$. Our\nmethod also provides efficient uncertainty estimation at inference time from a\nsingle forward pass through the generator subnetwork.\n"
    },
    {
        "title": "Spatiotemporal Representation Learning for Short and Long Medical Image\n  Time Series",
        "published_time": "2024-03-12T10:47:29Z",
        "abstract": "  Analyzing temporal developments is crucial for the accurate prognosis of many\nmedical conditions. Temporal changes that occur over short time scales are key\nto assessing the health of physiological functions, such as the cardiac cycle.\nMoreover, tracking longer term developments that occur over months or years in\nevolving processes, such as age-related macular degeneration (AMD), is\nessential for accurate prognosis. Despite the importance of both short and long\nterm analysis to clinical decision making, they remain understudied in medical\ndeep learning. State of the art methods for spatiotemporal representation\nlearning, developed for short natural videos, prioritize the detection of\ntemporal constants rather than temporal developments. Moreover, they do not\naccount for varying time intervals between acquisitions, which are essential\nfor contextualizing observed changes. To address these issues, we propose two\napproaches. First, we combine clip-level contrastive learning with a novel\ntemporal embedding to adapt to irregular time series. Second, we propose\nmasking and predicting latent frame representations of the temporal sequence.\nOur two approaches outperform all prior methods on temporally-dependent tasks\nincluding cardiac output estimation and three prognostic AMD tasks. Overall,\nthis enables the automated analysis of temporal patterns which are typically\noverlooked in applications of deep learning to medicine.\n"
    },
    {
        "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
        "published_time": "2024-03-12T10:44:13Z",
        "abstract": "  The rise of large language models (LLMs) and instruction tuning has led to\nthe current trend of instruction-tuned large language and vision models\n(LLVMs). This trend involves either meticulously curating numerous instruction\ntuning datasets tailored to specific objectives or enlarging LLVMs to manage\nvast amounts of vision language (VL) data. However, current LLVMs have\ndisregarded the detailed and comprehensive real-world scene understanding\navailable from specialized computer vision (CV) models in visual perception\ntasks such as segmentation, detection, scene graph generation (SGG), and\noptical character recognition (OCR). Instead, the existing LLVMs rely mainly on\nthe large capacity and emergent capabilities of their LLM backbones. Therefore,\nwe present a new LLVM, Mixture of All Intelligence (MoAI), which leverages\nauxiliary visual information obtained from the outputs of external\nsegmentation, detection, SGG, and OCR models. MoAI operates through two newly\nintroduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the\noutputs of the external CV models, the MoAI-Compressor aligns and condenses\nthem to efficiently use relevant auxiliary visual information for VL tasks.\nMoAI-Mixer then blends three types of intelligence (1) visual features, (2)\nauxiliary features from the external CV models, and (3) language features by\nutilizing the concept of Mixture of Experts. Through this integration, MoAI\nsignificantly outperforms both open-source and closed-source LLVMs in numerous\nzero-shot VL tasks, particularly those related to real-world scene\nunderstanding such as object existence, positions, relations, and OCR without\nenlarging the model size or curating extra visual instruction tuning datasets.\n"
    },
    {
        "title": "Block-wise LoRA: Revisiting Fine-grained LoRA for Effective\n  Personalization and Stylization in Text-to-Image Generation",
        "published_time": "2024-03-12T10:38:03Z",
        "abstract": "  The objective of personalization and stylization in text-to-image is to\ninstruct a pre-trained diffusion model to analyze new concepts introduced by\nusers and incorporate them into expected styles. Recently, parameter-efficient\nfine-tuning (PEFT) approaches have been widely adopted to address this task and\nhave greatly propelled the development of this field. Despite their popularity,\nexisting efficient fine-tuning methods still struggle to achieve effective\npersonalization and stylization in T2I generation. To address this issue, we\npropose block-wise Low-Rank Adaptation (LoRA) to perform fine-grained\nfine-tuning for different blocks of SD, which can generate images faithful to\ninput prompts and target identity and also with desired style. Extensive\nexperiments demonstrate the effectiveness of the proposed method.\n"
    },
    {
        "title": "Effective pruning of web-scale datasets based on complexity of concept\n  clusters",
        "published_time": "2024-01-09T14:32:24Z",
        "abstract": "  Utilizing massive web-scale datasets has led to unprecedented performance\ngains in machine learning models, but also imposes outlandish compute\nrequirements for their training. In order to improve training and data\nefficiency, we here push the limits of pruning large-scale multimodal datasets\nfor training CLIP-style models. Today's most effective pruning method on\nImageNet clusters data samples into separate concepts according to their\nembedding and prunes away the most prototypical samples. We scale this approach\nto LAION and improve it by noting that the pruning rate should be\nconcept-specific and adapted to the complexity of the concept. Using a simple\nand intuitive complexity measure, we are able to reduce the training cost to a\nquarter of regular training. By filtering from the LAION dataset, we find that\ntraining on a smaller set of high-quality data can lead to higher performance\nwith significantly lower training costs. More specifically, we are able to\noutperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot\naccuracy by 1.1p.p. while only using 27.7% of the data and training compute.\nDespite a strong reduction in training cost, we also see improvements on\nImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium\nbenchmark, we achieve a new state-of-the-art\nImagehttps://info.arxiv.org/help/prep#commentsNet zero-shot accuracy and a\ncompetitive average zero-shot accuracy on 38 evaluation tasks.\n"
    },
    {
        "title": "AutoGCN -- Towards Generic Human Activity Recognition with Neural\n  Architecture Search",
        "published_time": "2024-02-02T11:07:27Z",
        "abstract": "  This paper introduces AutoGCN, a generic Neural Architecture Search (NAS)\nalgorithm for Human Activity Recognition (HAR) using Graph Convolution Networks\n(GCNs). HAR has gained attention due to advances in deep learning, increased\ndata availability, and enhanced computational capabilities. At the same time,\nGCNs have shown promising results in modeling relationships between body key\npoints in a skeletal graph. While domain experts often craft dataset-specific\nGCN-based methods, their applicability beyond this specific context is severely\nlimited. AutoGCN seeks to address this limitation by simultaneously searching\nfor the ideal hyperparameters and architecture combination within a versatile\nsearch space using a reinforcement controller while balancing optimal\nexploration and exploitation behavior with a knowledge reservoir during the\nsearch process. We conduct extensive experiments on two large-scale datasets\nfocused on skeleton-based action recognition to assess the proposed algorithm's\nperformance. Our experimental results underscore the effectiveness of AutoGCN\nin constructing optimal GCN architectures for HAR, outperforming conventional\nNAS and GCN methods, as well as random search. These findings highlight the\nsignificance of a diverse search space and an expressive input representation\nto enhance the network performance and generalizability.\n"
    },
    {
        "title": "SemGauss-SLAM: Dense Semantic Gaussian Splatting SLAM",
        "published_time": "2024-03-12T10:33:26Z",
        "abstract": "  We propose SemGauss-SLAM, the first semantic SLAM system utilizing 3D\nGaussian representation, that enables accurate 3D semantic mapping, robust\ncamera tracking, and high-quality rendering in real-time. In this system, we\nincorporate semantic feature embedding into 3D Gaussian representation, which\neffectively encodes semantic information within the spatial layout of the\nenvironment for precise semantic scene representation. Furthermore, we propose\nfeature-level loss for updating 3D Gaussian representation, enabling\nhigher-level guidance for 3D Gaussian optimization. In addition, to reduce\ncumulative drift and improve reconstruction accuracy, we introduce\nsemantic-informed bundle adjustment leveraging semantic associations for joint\noptimization of 3D Gaussian representation and camera poses, leading to more\nrobust tracking and consistent mapping. Our SemGauss-SLAM method demonstrates\nsuperior performance over existing dense semantic SLAM methods in terms of\nmapping and tracking accuracy on Replica and ScanNet datasets, while also\nshowing excellent capabilities in novel-view semantic synthesis and 3D semantic\nmapping.\n"
    },
    {
        "title": "Motion Mamba: Efficient and Long Sequence Motion Generation with\n  Hierarchical and Bidirectional Selective SSM",
        "published_time": "2024-03-12T10:25:29Z",
        "abstract": "  Human motion generation stands as a significant pursuit in generative\ncomputer vision, while achieving long-sequence and efficient motion generation\nremains challenging. Recent advancements in state space models (SSMs), notably\nMamba, have showcased considerable promise in long sequence modeling with an\nefficient hardware-aware design, which appears to be a promising direction to\nbuild motion generation model upon it. Nevertheless, adapting SSMs to motion\ngeneration faces hurdles since the lack of a specialized design architecture to\nmodel motion sequence. To address these challenges, we propose Motion Mamba, a\nsimple and efficient approach that presents the pioneering motion generation\nmodel utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba\n(HTM) block to process temporal data by ensemble varying numbers of isolated\nSSM modules across a symmetric U-Net architecture aimed at preserving motion\nconsistency between frames. We also design a Bidirectional Spatial Mamba (BSM)\nblock to bidirectionally process latent poses, to enhance accurate motion\ngeneration within a temporal frame. Our proposed method achieves up to 50% FID\nimprovement and up to 4 times faster on the HumanML3D and KIT-ML datasets\ncompared to the previous best diffusion-based method, which demonstrates strong\ncapabilities of high-quality long sequence motion modeling and real-time human\nmotion generation. See project website\nhttps://steve-zeyu-zhang.github.io/MotionMamba/\n"
    },
    {
        "title": "A Comprehensive Survey of 3D Dense Captioning: Localizing and Describing\n  Objects in 3D Scenes",
        "published_time": "2024-03-12T10:04:08Z",
        "abstract": "  Three-Dimensional (3D) dense captioning is an emerging vision-language\nbridging task that aims to generate multiple detailed and accurate descriptions\nfor 3D scenes. It presents significant potential and challenges due to its\ncloser representation of the real world compared to 2D visual captioning, as\nwell as complexities in data collection and processing of 3D point cloud\nsources. Despite the popularity and success of existing methods, there is a\nlack of comprehensive surveys summarizing the advancements in this field, which\nhinders its progress. In this paper, we provide a comprehensive review of 3D\ndense captioning, covering task definition, architecture classification,\ndataset analysis, evaluation metrics, and in-depth prosperity discussions.\nBased on a synthesis of previous literature, we refine a standard pipeline that\nserves as a common paradigm for existing methods. We also introduce a clear\ntaxonomy of existing models, summarize technologies involved in different\nmodules, and conduct detailed experiment analysis. Instead of a chronological\norder introduction, we categorize the methods into different classes to\nfacilitate exploration and analysis of the differences and connections among\nexisting techniques. We also provide a reading guideline to assist readers with\ndifferent backgrounds and purposes in reading efficiently. Furthermore, we\npropose a series of promising future directions for 3D dense captioning by\nidentifying challenges and aligning them with the development of related tasks,\noffering valuable insights and inspiring future research in this field. Our aim\nis to provide a comprehensive understanding of 3D dense captioning, foster\nfurther investigations, and contribute to the development of novel applications\nin multimedia and related domains.\n"
    },
    {
        "title": "Backdoor Attack with Mode Mixture Latent Modification",
        "published_time": "2024-03-12T09:59:34Z",
        "abstract": "  Backdoor attacks become a significant security concern for deep neural\nnetworks in recent years. An image classification model can be compromised if\nmalicious backdoors are injected into it. This corruption will cause the model\nto function normally on clean images but predict a specific target label when\ntriggers are present. Previous research can be categorized into two genres:\npoisoning a portion of the dataset with triggered images for users to train the\nmodel from scratch, or training a backdoored model alongside a triggered image\ngenerator. Both approaches require significant amount of attackable parameters\nfor optimization to establish a connection between the trigger and the target\nlabel, which may raise suspicions as more people become aware of the existence\nof backdoor attacks. In this paper, we propose a backdoor attack paradigm that\nonly requires minimal alterations (specifically, the output layer) to a clean\nmodel in order to inject the backdoor under the guise of fine-tuning. To\nachieve this, we leverage mode mixture samples, which are located between\ndifferent modes in latent space, and introduce a novel method for conducting\nbackdoor attacks. We evaluate the effectiveness of our method on four popular\nbenchmark datasets: MNIST, CIFAR-10, GTSRB, and TinyImageNet.\n"
    },
    {
        "title": "Category-Agnostic Pose Estimation for Point Clouds",
        "published_time": "2024-03-12T09:28:11Z",
        "abstract": "  The goal of object pose estimation is to visually determine the pose of a\nspecific object in the RGB-D input. Unfortunately, when faced with new\ncategories, both instance-based and category-based methods are unable to deal\nwith unseen objects of unseen categories, which is a challenge for pose\nestimation. To address this issue, this paper proposes a method to introduce\ngeometric features for pose estimation of point clouds without requiring\ncategory information. The method is based only on the patch feature of the\npoint cloud, a geometric feature with rotation invariance. After training\nwithout category information, our method achieves as good results as other\ncategory-based methods. Our method successfully achieved pose annotation of no\ncategory information instances on the CAMERA25 dataset and ModelNet40 dataset.\n"
    },
    {
        "title": "JSTR: Joint Spatio-Temporal Reasoning for Event-based Moving Object\n  Detection",
        "published_time": "2024-03-12T09:22:52Z",
        "abstract": "  Event-based moving object detection is a challenging task, where static\nbackground and moving object are mixed together. Typically, existing methods\nmainly align the background events to the same spatial coordinate system via\nmotion compensation to distinguish the moving object. However, they neglect the\npotential spatial tailing effect of moving object events caused by excessive\nmotion, which may affect the structure integrity of the extracted moving\nobject. We discover that the moving object has a complete columnar structure in\nthe point cloud composed of motion-compensated events along the timestamp.\nMotivated by this, we propose a novel joint spatio-temporal reasoning method\nfor event-based moving object detection. Specifically, we first compensate the\nmotion of background events using inertial measurement unit. In spatial\nreasoning stage, we project the compensated events into the same image\ncoordinate, discretize the timestamp of events to obtain a time image that can\nreflect the motion confidence, and further segment the moving object through\nadaptive threshold on the time image. In temporal reasoning stage, we construct\nthe events into a point cloud along timestamp, and use RANSAC algorithm to\nextract the columnar shape in the cloud for peeling off the background.\nFinally, we fuse the results from the two reasoning stages to extract the final\nmoving object region. This joint spatio-temporal reasoning framework can\neffectively detect the moving object from motion confidence and geometric\nstructure. Moreover, we conduct extensive experiments on various datasets to\nverify that the proposed method can improve the moving object detection\naccuracy by 13\\%.\n"
    },
    {
        "title": "DALSA: Domain Adaptation for Supervised Learning From Sparsely Annotated\n  MR Images",
        "published_time": "2024-03-12T09:17:21Z",
        "abstract": "  We propose a new method that employs transfer learning techniques to\neffectively correct sampling selection errors introduced by sparse annotations\nduring supervised learning for automated tumor segmentation. The practicality\nof current learning-based automated tissue classification approaches is\nseverely impeded by their dependency on manually segmented training databases\nthat need to be recreated for each scenario of application, site, or\nacquisition setup. The comprehensive annotation of reference datasets can be\nhighly labor-intensive, complex, and error-prone. The proposed method derives\nhigh-quality classifiers for the different tissue classes from sparse and\nunambiguous annotations and employs domain adaptation techniques for\neffectively correcting sampling selection errors introduced by the sparse\nsampling. The new approach is validated on labeled, multi-modal MR images of 19\npatients with malignant gliomas and by comparative analysis on the BraTS 2013\nchallenge data sets. Compared to training on fully labeled data, we reduced the\ntime for labeling and training by a factor greater than 70 and 180 respectively\nwithout sacrificing accuracy. This dramatically eases the establishment and\nconstant extension of large annotated databases in various scenarios and\nimaging setups and thus represents an important step towards practical\napplicability of learning-based approaches in tissue classification.\n"
    },
    {
        "title": "Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for\n  Scene Flow",
        "published_time": "2024-03-12T09:15:19Z",
        "abstract": "  Single RGB or LiDAR is the mainstream sensor for the challenging scene flow,\nwhich relies heavily on visual features to match motion features. Compared with\nsingle modality, existing methods adopt a fusion strategy to directly fuse the\ncross-modal complementary knowledge in motion space. However, these direct\nfusion methods may suffer the modality gap due to the visual intrinsic\nheterogeneous nature between RGB and LiDAR, thus deteriorating motion features.\nWe discover that event has the homogeneous nature with RGB and LiDAR in both\nvisual and motion spaces. In this work, we bring the event as a bridge between\nRGB and LiDAR, and propose a novel hierarchical visual-motion fusion framework\nfor scene flow, which explores a homogeneous space to fuse the cross-modal\ncomplementary knowledge for physical interpretation. In visual fusion, we\ndiscover that event has a complementarity (relative v.s. absolute) in luminance\nspace with RGB for high dynamic imaging, and has a complementarity (local\nboundary v.s. global shape) in scene structure space with LiDAR for structure\nintegrity. In motion fusion, we figure out that RGB, event and LiDAR are\ncomplementary (spatial-dense, temporal-dense v.s. spatiotemporal-sparse) to\neach other in correlation space, which motivates us to fuse their motion\ncorrelations for motion continuity. The proposed hierarchical fusion can\nexplicitly fuse the multimodal knowledge to progressively improve scene flow\nfrom visual space to motion space. Extensive experiments have been performed to\nverify the superiority of the proposed method.\n"
    },
    {
        "title": "Input Data Adaptive Learning (IDAL) for Sub-acute Ischemic Stroke Lesion\n  Segmentation",
        "published_time": "2024-03-12T09:11:02Z",
        "abstract": "  In machine learning larger databases are usually associated with higher\nclassification accuracy due to better generalization. This generalization may\nlead to non-optimal classifiers in some medical applications with highly\nvariable expressions of pathologies. This paper presents a method for learning\nfrom a large training base by adaptively selecting optimal training samples for\ngiven input data. In this way heterogeneous databases are supported two-fold.\nFirst, by being able to deal with sparsely annotated data allows a quick\ninclusion of new data set and second, by training an input-dependent\nclassifier. The proposed approach is evaluated using the SISS challenge. The\nproposed algorithm leads to a significant improvement of the classification\naccuracy.\n"
    },
    {
        "title": "NightHaze: Nighttime Image Dehazing via Self-Prior Learning",
        "published_time": "2024-03-12T08:35:42Z",
        "abstract": "  Masked autoencoder (MAE) shows that severe augmentation during training\nproduces robust representations for high-level tasks. This paper brings the\nMAE-like framework to nighttime image enhancement, demonstrating that severe\naugmentation during training produces strong network priors that are resilient\nto real-world night haze degradations. We propose a novel nighttime image\ndehazing method with self-prior learning. Our main novelty lies in the design\nof severe augmentation, which allows our model to learn robust priors. Unlike\nMAE that uses masking, we leverage two key challenging factors of nighttime\nimages as augmentation: light effects and noise. During training, we\nintentionally degrade clear images by blending them with light effects as well\nas by adding noise, and subsequently restore the clear images. This enables our\nmodel to learn clear background priors. By increasing the noise values to\napproach as high as the pixel intensity values of the glow and light effect\nblended images, our augmentation becomes severe, resulting in stronger priors.\nWhile our self-prior learning is considerably effective in suppressing glow and\nrevealing details of background scenes, in some cases, there are still some\nundesired artifacts that remain, particularly in the forms of over-suppression.\nTo address these artifacts, we propose a self-refinement module based on the\nsemi-supervised teacher-student framework. Our NightHaze, especially our\nMAE-like self-prior learning, shows that models trained with severe\naugmentation effectively improve the visibility of input haze images,\napproaching the clarity of clear nighttime images. Extensive experiments\ndemonstrate that our NightHaze achieves state-of-the-art performance,\noutperforming existing nighttime image dehazing methods by a substantial margin\nof 15.5% for MUSIQ and 23.5% for ClipIQA.\n"
    },
    {
        "title": "In-context learning enables multimodal large language models to classify\n  cancer pathology images",
        "published_time": "2024-03-12T08:34:34Z",
        "abstract": "  Medical image classification requires labeled, task-specific datasets which\nare used to train deep learning networks de novo, or to fine-tune foundation\nmodels. However, this process is computationally and technically demanding. In\nlanguage processing, in-context learning provides an alternative, where models\nlearn from within prompts, bypassing the need for parameter updates. Yet,\nin-context learning remains underexplored in medical image analysis. Here, we\nsystematically evaluate the model Generative Pretrained Transformer 4 with\nVision capabilities (GPT-4V) on cancer image processing with in-context\nlearning on three cancer histopathology tasks of high importance:\nClassification of tissue subtypes in colorectal cancer, colon polyp subtyping\nand breast tumor detection in lymph node sections. Our results show that\nin-context learning is sufficient to match or even outperform specialized\nneural networks trained for particular tasks, while only requiring a minimal\nnumber of samples. In summary, this study demonstrates that large vision\nlanguage models trained on non-domain specific data can be applied out-of-the\nbox to solve medical image-processing tasks in histopathology. This\ndemocratizes access of generalist AI models to medical experts without\ntechnical background especially for areas where annotated data is scarce.\n"
    },
    {
        "title": "FeTrIL++: Feature Translation for Exemplar-Free Class-Incremental\n  Learning with Hill-Climbing",
        "published_time": "2024-03-12T08:34:05Z",
        "abstract": "  Exemplar-free class-incremental learning (EFCIL) poses significant\nchallenges, primarily due to catastrophic forgetting, necessitating a delicate\nbalance between stability and plasticity to accurately recognize both new and\nprevious classes. Traditional EFCIL approaches typically skew towards either\nmodel plasticity through successive fine-tuning or stability by employing a\nfixed feature extractor beyond the initial incremental state. Building upon the\nfoundational FeTrIL framework, our research extends into novel experimental\ndomains to examine the efficacy of various oversampling techniques and dynamic\noptimization strategies across multiple challenging datasets and incremental\nsettings. We specifically explore how oversampling impacts accuracy relative to\nfeature availability and how different optimization methodologies, including\ndynamic recalibration and feature pool diversification, influence incremental\nlearning outcomes. The results from these comprehensive experiments, conducted\non CIFAR100, Tiny-ImageNet, and an ImageNet-Subset, under-score the superior\nperformance of FeTrIL in balancing accuracy for both new and past classes\nagainst ten contemporary methods. Notably, our extensions reveal the nuanced\nimpacts of oversampling and optimization on EFCIL, contributing to a more\nrefined understanding of feature-space manipulation for class incremental\nlearning. FeTrIL and its extended analysis in this paper FeTrIL++ pave the way\nfor more adaptable and efficient EFCIL methodologies, promising significant\nimprovements in handling catastrophic forgetting without the need for\nexemplars.\n"
    },
    {
        "title": "Frequency-Adaptive Dilated Convolution for Semantic Segmentation",
        "published_time": "2024-03-08T15:00:44Z",
        "abstract": "  Dilated convolution, which expands the receptive field by inserting gaps\nbetween its consecutive elements, is widely employed in computer vision. In\nthis study, we propose three strategies to improve individual phases of dilated\nconvolution from the view of spectrum analysis. Departing from the conventional\npractice of fixing a global dilation rate as a hyperparameter, we introduce\nFrequency-Adaptive Dilated Convolution (FADC), which dynamically adjusts\ndilation rates spatially based on local frequency components. Subsequently, we\ndesign two plug-in modules to directly enhance effective bandwidth and\nreceptive field size. The Adaptive Kernel (AdaKern) module decomposes\nconvolution weights into low-frequency and high-frequency components,\ndynamically adjusting the ratio between these components on a per-channel\nbasis. By increasing the high-frequency part of convolution weights, AdaKern\ncaptures more high-frequency components, thereby improving effective bandwidth.\nThe Frequency Selection (FreqSelect) module optimally balances high- and\nlow-frequency components in feature representations through spatially variant\nreweighting. It suppresses high frequencies in the background to encourage FADC\nto learn a larger dilation, thereby increasing the receptive field for an\nexpanded scope. Extensive experiments on segmentation and object detection\nconsistently validate the efficacy of our approach. The code is publicly\navailable at \\url{https://github.com/Linwei-Chen/FADC}.\n"
    },
    {
        "title": "From Canteen Food to Daily Meals: Generalizing Food Recognition to More\n  Practical Scenarios",
        "published_time": "2024-03-12T08:32:23Z",
        "abstract": "  The precise recognition of food categories plays a pivotal role for\nintelligent health management, attracting significant research attention in\nrecent years. Prominent benchmarks, such as Food-101 and VIREO Food-172,\nprovide abundant food image resources that catalyze the prosperity of research\nin this field. Nevertheless, these datasets are well-curated from canteen\nscenarios and thus deviate from food appearances in daily life. This\ndiscrepancy poses great challenges in effectively transferring classifiers\ntrained on these canteen datasets to broader daily-life scenarios encountered\nby humans. Toward this end, we present two new benchmarks, namely DailyFood-172\nand DailyFood-16, specifically designed to curate food images from everyday\nmeals. These two datasets are used to evaluate the transferability of\napproaches from the well-curated food image domain to the everyday-life food\nimage domain. In addition, we also propose a simple yet effective baseline\nmethod named Multi-Cluster Reference Learning (MCRL) to tackle the\naforementioned domain gap. MCRL is motivated by the observation that food\nimages in daily-life scenarios exhibit greater intra-class appearance variance\ncompared with those in well-curated benchmarks. Notably, MCRL can be seamlessly\ncoupled with existing approaches, yielding non-trivial performance\nenhancements. We hope our new benchmarks can inspire the community to explore\nthe transferability of food recognition models trained on well-curated datasets\ntoward practical real-life applications.\n"
    },
    {
        "title": "QUASAR: QUality and Aesthetics Scoring with Advanced Representations",
        "published_time": "2024-03-11T16:21:50Z",
        "abstract": "  This paper introduces a new data-driven, non-parametric method for image\nquality and aesthetics assessment, surpassing existing approaches and requiring\nno prompt engineering or fine-tuning. We eliminate the need for expressive\ntextual embeddings by proposing efficient image anchors in the data. Through\nextensive evaluations of 7 state-of-the-art self-supervised models, our method\ndemonstrates superior performance and robustness across various datasets and\nbenchmarks. Notably, it achieves high agreement with human assessments even\nwith limited data and shows high robustness to the nature of data and their\npre-processing pipeline. Our contributions offer a streamlined solution for\nassessment of images while providing insights into the perception of visual\ninformation.\n"
    },
    {
        "title": "ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature\n  Interaction for Dense Predictions",
        "published_time": "2024-03-12T07:59:41Z",
        "abstract": "  Although Vision Transformer (ViT) has achieved significant success in\ncomputer vision, it does not perform well in dense prediction tasks due to the\nlack of inner-patch information interaction and the limited diversity of\nfeature scale. Most existing studies are devoted to designing vision-specific\ntransformers to solve the above problems, which introduce additional\npre-training costs. Therefore, we present a plain, pre-training-free, and\nfeature-enhanced ViT backbone with Convolutional Multi-scale feature\ninteraction, named ViT-CoMer, which facilitates bidirectional interaction\nbetween CNN and transformer. Compared to the state-of-the-art, ViT-CoMer has\nthe following advantages: (1) We inject spatial pyramid multi-receptive field\nconvolutional features into the ViT architecture, which effectively alleviates\nthe problems of limited local information interaction and single-feature\nrepresentation in ViT. (2) We propose a simple and efficient CNN-Transformer\nbidirectional fusion interaction module that performs multi-scale fusion across\nhierarchical features, which is beneficial for handling dense prediction tasks.\n(3) We evaluate the performance of ViT-CoMer across various dense prediction\ntasks, different frameworks, and multiple advanced pre-training. Notably, our\nViT-CoMer-L achieves 64.3% AP on COCO val2017 without extra training data, and\n62.1% mIoU on ADE20K val, both of which are comparable to state-of-the-art\nmethods. We hope ViT-CoMer can serve as a new backbone for dense prediction\ntasks to facilitate future research. The code will be released at\nhttps://github.com/Traffic-X/ViT-CoMer.\n"
    },
    {
        "title": "Learning Correction Errors via Frequency-Self Attention for Blind Image\n  Super-Resolution",
        "published_time": "2024-03-12T07:58:14Z",
        "abstract": "  Previous approaches for blind image super-resolution (SR) have relied on\ndegradation estimation to restore high-resolution (HR) images from their\nlow-resolution (LR) counterparts. However, accurate degradation estimation\nposes significant challenges. The SR model's incompatibility with degradation\nestimation methods, particularly the Correction Filter, may significantly\nimpair performance as a result of correction errors. In this paper, we\nintroduce a novel blind SR approach that focuses on Learning Correction Errors\n(LCE). Our method employs a lightweight Corrector to obtain a corrected\nlow-resolution (CLR) image. Subsequently, within an SR network, we jointly\noptimize SR performance by utilizing both the original LR image and the\nfrequency learning of the CLR image. Additionally, we propose a new\nFrequency-Self Attention block (FSAB) that enhances the global information\nutilization ability of Transformer. This block integrates both self-attention\nand frequency spatial attention mechanisms. Extensive ablation and comparison\nexperiments conducted across various settings demonstrate the superiority of\nour method in terms of visual quality and accuracy. Our approach effectively\naddresses the challenges associated with degradation estimation and correction\nerrors, paving the way for more accurate blind image SR.\n"
    },
    {
        "title": "Auxiliary CycleGAN-guidance for Task-Aware Domain Translation from\n  Duplex to Monoplex IHC Images",
        "published_time": "2024-03-12T07:57:33Z",
        "abstract": "  Generative models enable the translation from a source image domain where\nreadily trained models are available to a target domain unseen during training.\nWhile Cycle Generative Adversarial Networks (GANs) are well established, the\nassociated cycle consistency constrain relies on that an invertible mapping\nexists between the two domains. This is, however, not the case for the\ntranslation between images stained with chromogenic monoplex and duplex\nimmunohistochemistry (IHC) assays. Focusing on the translation from the latter\nto the first, we propose - through the introduction of a novel training design,\nan alternative constrain leveraging a set of immunofluorescence (IF) images as\nan auxiliary unpaired image domain. Quantitative and qualitative results on a\ndownstream segmentation task show the benefit of the proposed method in\ncomparison to baseline approaches.\n"
    },
    {
        "title": "Gabor-guided transformer for single image deraining",
        "published_time": "2024-03-12T07:41:51Z",
        "abstract": "  Image deraining have have gained a great deal of attention in order to\naddress the challenges posed by the effects of harsh weather conditions on\nvisual tasks. While convolutional neural networks (CNNs) are popular, their\nlimitations in capturing global information may result in ineffective rain\nremoval. Transformer-based methods with self-attention mechanisms have\nimproved, but they tend to distort high-frequency details that are crucial for\nimage fidelity. To solve this problem, we propose the Gabor-guided tranformer\n(Gabformer) for single image deraining. The focus on local texture features is\nenhanced by incorporating the information processed by the Gabor filter into\nthe query vector, which also improves the robustness of the model to noise due\nto the properties of the filter. Extensive experiments on the benchmarks\ndemonstrate that our method outperforms state-of-the-art approaches.\n"
    },
    {
        "title": "Ray Denoising: Depth-aware Hard Negative Sampling for Multi-view 3D\n  Object Detection",
        "published_time": "2024-02-06T02:17:44Z",
        "abstract": "  Multi-view 3D object detection systems often struggle with generating precise\npredictions due to the challenges in estimating depth from images, increasing\nredundant and incorrect detections. Our paper presents Ray Denoising, an\ninnovative method that enhances detection accuracy by strategically sampling\nalong camera rays to construct hard negative examples. These examples, visually\nchallenging to differentiate from true positives, compel the model to learn\ndepth-aware features, thereby improving its capacity to distinguish between\ntrue and false positives. Ray Denoising is designed as a plug-and-play module,\ncompatible with any DETR-style multi-view 3D detectors, and it only minimally\nincreases training computational costs without affecting inference speed. Our\ncomprehensive experiments, including detailed ablation studies, consistently\ndemonstrate that Ray Denoising outperforms strong baselines across multiple\ndatasets. It achieves a 1.9\\% improvement in mean Average Precision (mAP) over\nthe state-of-the-art StreamPETR method on the NuScenes dataset. It shows\nsignificant performance gains on the Argoverse 2 dataset, highlighting its\ngeneralization capability. The code will be available at\nhttps://github.com/LiewFeng/RayDN.\n"
    },
    {
        "title": "NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning\n  Disentangled Reasoning",
        "published_time": "2024-03-12T07:27:02Z",
        "abstract": "  Vision-and-Language Navigation (VLN), as a crucial research problem of\nEmbodied AI, requires an embodied agent to navigate through complex 3D\nenvironments following natural language instructions. Recent research has\nhighlighted the promising capacity of large language models (LLMs) in VLN by\nimproving navigational reasoning accuracy and interpretability. However, their\npredominant use in an offline manner usually suffers from substantial domain\ngap between the VLN task and the LLM training corpus. This paper introduces a\nnovel strategy called Navigational Chain-of-Thought (NavCoT), where we fulfill\nparameter-efficient in-domain training to enable self-guided navigational\ndecision, leading to a significant mitigation of the domain gap in a\ncost-effective manner. Specifically, at each timestep, the LLM is prompted to\nforecast the navigational chain-of-thought by: 1) acting as a world model to\nimagine the next observation according to the instruction, 2) selecting the\ncandidate observation that best aligns with the imagination, and 3) determining\nthe action based on the reasoning from the prior steps. Through constructing\nformalized labels for training, the LLM can learn to generate desired and\nreasonable chain-of-thought outputs for improving the action decision.\nExperimental results across various training settings and popular VLN\nbenchmarks (e.g., Room-to-Room (R2R), Room-across-Room (RxR), Room-for-Room\n(R4R)) show the significant superiority of NavCoT over the direct action\nprediction variants. Through simple parameter-efficient finetuning, our NavCoT\noutperforms a recent GPT4-based approach with ~7% relative improvement on the\nR2R dataset. We believe that NavCoT will help unlock more task-adaptive and\nscalable LLM-based embodied agents, which are helpful for developing real-world\nrobotics applications. Code is available at\nhttps://github.com/expectorlin/NavCoT.\n"
    },
    {
        "title": "Eliminating Cross-modal Conflicts in BEV Space for LiDAR-Camera 3D\n  Object Detection",
        "published_time": "2024-03-12T07:16:20Z",
        "abstract": "  Recent 3D object detectors typically utilize multi-sensor data and unify\nmulti-modal features in the shared bird's-eye view (BEV) representation space.\nHowever, our empirical findings indicate that previous methods have limitations\nin generating fusion BEV features free from cross-modal conflicts. These\nconflicts encompass extrinsic conflicts caused by BEV feature construction and\ninherent conflicts stemming from heterogeneous sensor signals. Therefore, we\npropose a novel Eliminating Conflicts Fusion (ECFusion) method to explicitly\neliminate the extrinsic/inherent conflicts in BEV space and produce improved\nmulti-modal BEV features. Specifically, we devise a Semantic-guided Flow-based\nAlignment (SFA) module to resolve extrinsic conflicts via unifying spatial\ndistribution in BEV space before fusion. Moreover, we design a Dissolved Query\nRecovering (DQR) mechanism to remedy inherent conflicts by preserving\nobjectness clues that are lost in the fusion BEV feature. In general, our\nmethod maximizes the effective information utilization of each modality and\nleverages inter-modal complementarity. Our method achieves state-of-the-art\nperformance in the highly competitive nuScenes 3D object detection dataset. The\ncode is released at https://github.com/fjhzhixi/ECFusion.\n"
    },
    {
        "title": "Time-Efficient and Identity-Consistent Virtual Try-On Using A Variant of\n  Altered Diffusion Models",
        "published_time": "2024-03-12T07:15:29Z",
        "abstract": "  This study discusses the critical issues of Virtual Try-On in contemporary\ne-commerce and the prospective metaverse, emphasizing the challenges of\npreserving intricate texture details and distinctive features of the target\nperson and the clothes in various scenarios, such as clothing texture and\nidentity characteristics like tattoos or accessories. In addition to the\nfidelity of the synthesized images, the efficiency of the synthesis process\npresents a significant hurdle. Various existing approaches are explored,\nhighlighting the limitations and unresolved aspects, e.g., identity information\nomission, uncontrollable artifacts, and low synthesis speed. It then proposes a\nnovel diffusion-based solution that addresses garment texture preservation and\nuser identity retention during virtual try-on. The proposed network comprises\ntwo primary modules - a warping module aligning clothing with individual\nfeatures and a try-on module refining the attire and generating missing parts\nintegrated with a mask-aware post-processing technique ensuring the integrity\nof the individual's identity. It demonstrates impressive results, surpassing\nthe state-of-the-art in speed by nearly 20 times during inference, with\nsuperior fidelity in qualitative assessments. Quantitative evaluations confirm\ncomparable performance with the recent SOTA method on the VITON-HD and\nDresscode datasets.\n"
    },
    {
        "title": "Textual Knowledge Matters: Cross-Modality Co-Teaching for Generalized\n  Visual Class Discovery",
        "published_time": "2024-03-12T07:06:50Z",
        "abstract": "  In this paper, we study the problem of Generalized Category Discovery (GCD),\nwhich aims to cluster unlabeled data from both known and unknown categories\nusing the knowledge of labeled data from known categories. Current GCD methods\nrely on only visual cues, which however neglect the multi-modality perceptive\nnature of human cognitive processes in discovering novel visual categories. To\naddress this, we propose a two-phase TextGCD framework to accomplish\nmulti-modality GCD by exploiting powerful Visual-Language Models. TextGCD\nmainly includes a retrieval-based text generation (RTG) phase and a\ncross-modality co-teaching (CCT) phase. First, RTG constructs a visual lexicon\nusing category tags from diverse datasets and attributes from Large Language\nModels, generating descriptive texts for images in a retrieval manner. Second,\nCCT leverages disparities between textual and visual modalities to foster\nmutual learning, thereby enhancing visual GCD. In addition, we design an\nadaptive class aligning strategy to ensure the alignment of category\nperceptions between modalities as well as a soft-voting mechanism to integrate\nmulti-modality cues. Experiments on eight datasets show the large superiority\nof our approach over state-of-the-art methods. Notably, our approach\noutperforms the best competitor, by 7.7% and 10.8% in All accuracy on\nImageNet-1k and CUB, respectively.\n"
    },
    {
        "title": "Entropy is not Enough for Test-Time Adaptation: From the Perspective of\n  Disentangled Factors",
        "published_time": "2024-03-12T07:01:57Z",
        "abstract": "  Test-time adaptation (TTA) fine-tunes pre-trained deep neural networks for\nunseen test data. The primary challenge of TTA is limited access to the entire\ntest dataset during online updates, causing error accumulation. To mitigate it,\nTTA methods have utilized the model output's entropy as a confidence metric\nthat aims to determine which samples have a lower likelihood of causing error.\nThrough experimental studies, however, we observed the unreliability of entropy\nas a confidence metric for TTA under biased scenarios and theoretically\nrevealed that it stems from the neglect of the influence of latent disentangled\nfactors of data on predictions. Building upon these findings, we introduce a\nnovel TTA method named Destroy Your Object (DeYO), which leverages a newly\nproposed confidence metric named Pseudo-Label Probability Difference (PLPD).\nPLPD quantifies the influence of the shape of an object on prediction by\nmeasuring the difference between predictions before and after applying an\nobject-destructive transformation. DeYO consists of sample selection and sample\nweighting, which employ entropy and PLPD concurrently. For robust adaptation,\nDeYO prioritizes samples that dominantly incorporate shape information when\nmaking predictions. Our extensive experiments demonstrate the consistent\nsuperiority of DeYO over baseline methods across various scenarios, including\nbiased and wild. Project page is publicly available at\nhttps://whitesnowdrop.github.io/DeYO/.\n"
    },
    {
        "title": "Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine\n  Unlearning",
        "published_time": "2024-03-12T06:50:32Z",
        "abstract": "  The trustworthy machine learning (ML) community is increasingly recognizing\nthe crucial need for models capable of selectively 'unlearning' data points\nafter training. This leads to the problem of machine unlearning (MU), aiming to\neliminate the influence of chosen data points on model performance, while still\nmaintaining the model's utility post-unlearning. Despite various MU methods for\ndata influence erasure, evaluations have largely focused on random data\nforgetting, ignoring the vital inquiry into which subset should be chosen to\ntruly gauge the authenticity of unlearning performance. To tackle this issue,\nwe introduce a new evaluative angle for MU from an adversarial viewpoint. We\npropose identifying the data subset that presents the most significant\nchallenge for influence erasure, i.e., pinpointing the worst-case forget set.\nUtilizing a bi-level optimization principle, we amplify unlearning challenges\nat the upper optimization level to emulate worst-case scenarios, while\nsimultaneously engaging in standard training and unlearning at the lower level,\nachieving a balance between data influence erasure and model utility. Our\nproposal offers a worst-case evaluation of MU's resilience and effectiveness.\nThrough extensive experiments across different datasets (including CIFAR-10,\n100, CelebA, Tiny ImageNet, and ImageNet) and models (including both image\nclassifiers and generative models), we expose critical pros and cons in\nexisting (approximate) unlearning strategies. Our results illuminate the\ncomplex challenges of MU in practice, guiding the future development of more\naccurate and robust unlearning algorithms. The code is available at\nhttps://github.com/OPTML-Group/Unlearn-WorstCase.\n"
    },
    {
        "title": "Premonition: Using Generative Models to Preempt Future Data Changes in\n  Continual Learning",
        "published_time": "2024-03-12T06:29:54Z",
        "abstract": "  Continual learning requires a model to adapt to ongoing changes in the data\ndistribution, and often to the set of tasks to be performed. It is rare,\nhowever, that the data and task changes are completely unpredictable. Given a\ndescription of an overarching goal or data theme, which we call a realm, humans\ncan often guess what concepts are associated with it. We show here that the\ncombination of a large language model and an image generation model can\nsimilarly provide useful premonitions as to how a continual learning challenge\nmight develop over time. We use the large language model to generate text\ndescriptions of semantically related classes that might potentially appear in\nthe data stream in future. These descriptions are then rendered using Stable\nDiffusion to generate new labelled image samples. The resulting synthetic\ndataset is employed for supervised pre-training, but is discarded prior to\ncommencing continual learning, along with the pre-training classification head.\nWe find that the backbone of our pre-trained networks can learn representations\nuseful for the downstream continual learning problem, thus becoming a valuable\ninput to any existing continual learning method. Although there are\ncomplexities arising from the domain gap between real and synthetic images, we\nshow that pre-training models in this manner improves multiple Class Incremenal\nLearning (CIL) methods on fine-grained image classification benchmarks.\nSupporting code can be found at https://github.com/cl-premonition/premonition.\n"
    },
    {
        "title": "BID: Boundary-Interior Decoding for Unsupervised Temporal Action\n  Localization Pre-Trainin",
        "published_time": "2024-03-12T06:23:45Z",
        "abstract": "  Skeleton-based motion representations are robust for action localization and\nunderstanding for their invariance to perspective, lighting, and occlusion,\ncompared with images. Yet, they are often ambiguous and incomplete when taken\nout of context, even for human annotators. As infants discern gestures before\nassociating them with words, actions can be conceptualized before being\ngrounded with labels. Therefore, we propose the first unsupervised pre-training\nframework, Boundary-Interior Decoding (BID), that partitions a skeleton-based\nmotion sequence into discovered semantically meaningful pre-action segments. By\nfine-tuning our pre-training network with a small number of annotated data, we\nshow results out-performing SOTA methods by a large margin.\n"
    },
    {
        "title": "KEBench: A Benchmark on Knowledge Editing for Large Vision-Language\n  Models",
        "published_time": "2024-03-12T06:16:33Z",
        "abstract": "  Currently, little research has been done on knowledge editing for Large\nVision-Language Models (LVLMs). Editing LVLMs faces the challenge of\neffectively integrating diverse modalities (image and text) while ensuring\ncoherent and contextually relevant modifications. An existing benchmark has\nthree metrics (Reliability, Locality and Generality) to measure knowledge\nediting for LVLMs. However, the benchmark falls short in the quality of\ngenerated images used in evaluation and cannot assess whether models\neffectively utilize edited knowledge in relation to the associated content. We\nadopt different data collection methods to construct a new benchmark,\n$\\textbf{KEBench}$, and extend new metric (Portability) for a comprehensive\nevaluation. Leveraging a multimodal knowledge graph, our image data exhibits\nclear directionality towards entities. This directional aspect can be further\nutilized to extract entity-related knowledge and form editing data. We\nconducted experiments of different editing methods on five LVLMs, and\nthoroughly analyze how these methods impact the models. The results reveal\nstrengths and deficiencies of these methods and, hopefully, provide insights\ninto potential avenues for future research.\n"
    },
    {
        "title": "Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic\n  Architecture",
        "published_time": "2024-03-12T06:07:29Z",
        "abstract": "  Video Motion Magnification (VMM) aims to reveal subtle and imperceptible\nmotion information of objects in the macroscopic world. Prior methods directly\nmodel the motion field from the Eulerian perspective by Representation Learning\nthat separates shape and texture or Multi-domain Learning from phase\nfluctuations. Inspired by the frequency spectrum, we observe that the\nlow-frequency components with stable energy always possess spatial structure\nand less noise, making them suitable for modeling the subtle motion field. To\nthis end, we present FD4MM, a new paradigm of Frequency Decoupling for Motion\nMagnification with a Multi-level Isomorphic Architecture to capture multi-level\nhigh-frequency details and a stable low-frequency structure (motion field) in\nvideo space. Since high-frequency details and subtle motions are susceptible to\ninformation degradation due to their inherent subtlety and unavoidable external\ninterference from noise, we carefully design Sparse High/Low-pass Filters to\nenhance the integrity of details and motion structures, and a Sparse Frequency\nMixer to promote seamless recoupling. Besides, we innovatively design a\ncontrastive regularization for this task to strengthen the model's ability to\ndiscriminate irrelevant features, reducing undesired motion magnification.\nExtensive experiments on both Real-world and Synthetic Datasets show that our\nFD4MM outperforms SOTA methods. Meanwhile, FD4MM reduces FLOPs by 1.63$\\times$\nand boosts inference speed by 1.68$\\times$ than the latest method. Our code is\navailable at https://github.com/Jiafei127/FD4MM.\n"
    },
    {
        "title": "Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction",
        "published_time": "2024-03-12T06:04:50Z",
        "abstract": "  Reliable hand mesh reconstruction (HMR) from commonly-used color and depth\nsensors is challenging especially under scenarios with varied illuminations and\nfast motions. Event camera is a highly promising alternative for its high\ndynamic range and dense temporal resolution properties, but it lacks key\ntexture appearance for hand mesh reconstruction. In this paper, we propose\nEvRGBHand -- the first approach for 3D hand mesh reconstruction with an event\ncamera and an RGB camera compensating for each other. By fusing two modalities\nof data across time, space, and information dimensions,EvRGBHand can tackle\noverexposure and motion blur issues in RGB-based HMR and foreground scarcity\nand background overflow issues in event-based HMR. We further propose\nEvRGBDegrader, which allows our model to generalize effectively in challenging\nscenes, even when trained solely on standard scenes, thus reducing data\nacquisition costs. Experiments on real-world data demonstrate that EvRGBHand\ncan effectively solve the challenging issues when using either type of camera\nalone via retaining the merits of both, and shows the potential of\ngeneralization to outdoor scenes and another type of event camera.\n"
    },
    {
        "title": "IM-Unpack: Training and Inference with Arbitrarily Low Precision\n  Integers",
        "published_time": "2024-03-12T05:44:27Z",
        "abstract": "  GEneral Matrix Multiply (GEMM) is a central operation in deep learning and\ncorresponds to the largest chunk of the compute footprint. Therefore, improving\nits efficiency is an active topic of ongoing research. A popular strategy is\nthe use of low bit-width integers to approximate the original entries in a\nmatrix. This allows efficiency gains, but often requires sophisticated\ntechniques to control the rounding error incurred. In this work, we first\nverify/check that when the low bit-width restriction is removed, for a variety\nof Transformer-based models, whether integers are sufficient for all GEMMs need\n-- for {\\em both} training and inference stages, and can achieve parity with\nfloating point counterparts. No sophisticated techniques are needed. We find\nthat while a large majority of entries in matrices (encountered in such models)\ncan be easily represented by {\\em low} bit-width integers, the existence of a\nfew heavy hitter entries make it difficult to achieve efficiency gains via the\nexclusive use of low bit-width GEMMs alone. To address this issue, we develop a\nsimple algorithm, Integer Matrix Unpacking (IM-Unpack), to {\\em unpack} a\nmatrix with large integer entries into a larger matrix whose entries all lie\nwithin the representable range of arbitrarily low bit-width integers. This\nallows {\\em equivalence} with the original GEMM, i.e., the exact result can be\nobtained using purely low bit-width integer GEMMs. This comes at the cost of\nadditional operations -- we show that for many popular models, this overhead is\nquite small.\n"
    },
    {
        "title": "Large Window-based Mamba UNet for Medical Image Segmentation: Beyond\n  Convolution and Self-attention",
        "published_time": "2024-03-12T05:34:51Z",
        "abstract": "  In clinical practice, medical image segmentation provides useful information\non the contours and dimensions of target organs or tissues, facilitating\nimproved diagnosis, analysis, and treatment. In the past few years,\nconvolutional neural networks (CNNs) and Transformers have dominated this area,\nbut they still suffer from either limited receptive fields or costly long-range\nmodeling. Mamba, a State Space Sequence Model (SSM), recently emerged as a\npromising paradigm for long-range dependency modeling with linear complexity.\nIn this paper, we introduce a Large Window-based Mamba U}-shape Network, or\nLMa-UNet, for 2D and 3D medical image segmentation. A distinguishing feature of\nour LMa-UNet is its utilization of large windows, excelling in locally spatial\nmodeling compared to small kernel-based CNNs and small window-based\nTransformers, while maintaining superior efficiency in global modeling compared\nto self-attention with quadratic complexity. Additionally, we design a novel\nhierarchical and bidirectional Mamba block to further enhance the global and\nneighborhood spatial modeling capability of Mamba. Comprehensive experiments\ndemonstrate the effectiveness and efficiency of our method and the feasibility\nof using large window size to achieve large receptive fields. Codes are\navailable at https://github.com/wjh892521292/LMa-UNet.\n"
    },
    {
        "title": "SGE: Structured Light System Based on Gray Code with an Event Camera",
        "published_time": "2024-03-12T05:20:44Z",
        "abstract": "  Fast and accurate depth sensing has long been a significant research\nchallenge. Event camera, as a device that quickly responds to intensity\nchanges, provides a new solution for structured light (SL) systems. In this\npaper, we introduce Gray code into event-based SL systems for the first time.\nOur setup includes an event camera and Digital Light Processing (DLP)\nprojector, enabling depth estimation through high-speed projection and decoding\nof Gray code patterns. By employing spatio-temporal encoding for point\nmatching, our method is immune to timestamp noise, realizing high-speed depth\nestimation without loss of accuracy. The binary nature of events and Gray code\nminimizes data redundancy, enabling us to fully utilize sensor bandwidth at\n100%. Experimental results show that our approach achieves accuracy comparable\nto state-of-the-art scanning methods while surpassing them in data acquisition\nspeed (up to 41 times improvement) without sacrificing accuracy. Our proposed\napproach offers a highly promising solution for ultra-fast, real-time, and\nhigh-precision dense depth estimation. Code and dataset will be publicly\navailable.\n"
    },
    {
        "title": "Efficient Diffusion Model for Image Restoration by Residual Shifting",
        "published_time": "2024-03-12T05:06:07Z",
        "abstract": "  While diffusion-based image restoration (IR) methods have achieved remarkable\nsuccess, they are still limited by the low inference speed attributed to the\nnecessity of executing hundreds or even thousands of sampling steps. Existing\nacceleration sampling techniques, though seeking to expedite the process,\ninevitably sacrifice performance to some extent, resulting in over-blurry\nrestored outcomes. To address this issue, this study proposes a novel and\nefficient diffusion model for IR that significantly reduces the required number\nof diffusion steps. Our method avoids the need for post-acceleration during\ninference, thereby avoiding the associated performance deterioration.\nSpecifically, our proposed method establishes a Markov chain that facilitates\nthe transitions between the high-quality and low-quality images by shifting\ntheir residuals, substantially improving the transition efficiency. A carefully\nformulated noise schedule is devised to flexibly control the shifting speed and\nthe noise strength during the diffusion process. Extensive experimental\nevaluations demonstrate that the proposed method achieves superior or\ncomparable performance to current state-of-the-art methods on three classical\nIR tasks, namely image super-resolution, image inpainting, and blind face\nrestoration, \\textit{\\textbf{even only with four sampling steps}}. Our code and\nmodel are publicly available at \\url{https://github.com/zsyOAOA/ResShift}.\n"
    },
    {
        "title": "Customizable Avatars with Dynamic Facial Action Coded Expressions\n  (CADyFACE) for Improved User Engagement",
        "published_time": "2024-03-12T05:00:38Z",
        "abstract": "  Customizable 3D avatar-based facial expression stimuli may improve user\nengagement in behavioral biomarker discovery and therapeutic intervention for\nautism, Alzheimer's disease, facial palsy, and more. However, there is a lack\nof customizable avatar-based stimuli with Facial Action Coding System (FACS)\naction unit (AU) labels. Therefore, this study focuses on (1) FACS-labeled,\ncustomizable avatar-based expression stimuli for maintaining subjects'\nengagement, (2) learning-based measurements that quantify subjects' facial\nresponses to such stimuli, and (3) validation of constructs represented by\nstimulus-measurement pairs. We propose Customizable Avatars with Dynamic Facial\nAction Coded Expressions (CADyFACE) labeled with AUs by a certified FACS\nexpert. To measure subjects' AUs in response to CADyFACE, we propose a novel\nBeta-guided Correlation and Multi-task Expression learning neural network\n(BeCoME-Net) for multi-label AU detection. The beta-guided correlation loss\nencourages feature correlation with AUs while discouraging correlation with\nsubject identities for improved generalization. We train BeCoME-Net for\nunilateral and bilateral AU detection and compare with state-of-the-art\napproaches. To assess construct validity of CADyFACE and BeCoME-Net, twenty\nhealthy adult volunteers complete expression recognition and mimicry tasks in\nan online feasibility study while webcam-based eye-tracking and video are\ncollected. We test validity of multiple constructs, including face preference\nduring recognition and AUs during mimicry.\n"
    },
    {
        "title": "Grey Level Texture Features for Segmentation of Chromogenic Dye RNAscope\n  From Breast Cancer Tissue",
        "published_time": "2024-01-29T04:43:07Z",
        "abstract": "  Chromogenic RNAscope dye and haematoxylin staining of cancer tissue\nfacilitates diagnosis of the cancer type and subsequent treatment, and fits\nwell into existing pathology workflows. However, manual quantification of the\nRNAscope transcripts (dots), which signify gene expression, is prohibitively\ntime consuming. In addition, there is a lack of verified supporting methods for\nquantification and analysis. This paper investigates the usefulness of grey\nlevel texture features for automatically segmenting and classifying the\npositions of RNAscope transcripts from breast cancer tissue. Feature analysis\nshowed that a small set of grey level features, including Grey Level Dependence\nMatrix and Neighbouring Grey Tone Difference Matrix features, were well suited\nfor the task. The automated method performed similarly to expert annotators at\nidentifying the positions of RNAscope transcripts, with an F1-score of 0.571\ncompared to the expert inter-rater F1-score of 0.596. These results demonstrate\nthe potential of grey level texture features for automated quantification of\nRNAscope in the pathology workflow.\n"
    },
    {
        "title": "Lumen: Unleashing Versatile Vision-Centric Capabilities of Large\n  Multimodal Models",
        "published_time": "2024-03-12T04:13:45Z",
        "abstract": "  Large Multimodal Model (LMM) is a hot research topic in the computer vision\narea and has also demonstrated remarkable potential across multiple\ndisciplinary fields. A recent trend is to further extend and enhance the\nperception capabilities of LMMs. The current methods follow the paradigm of\nadapting the visual task outputs to the format of the language model, which is\nthe main component of a LMM. This adaptation leads to convenient development of\nsuch LMMs with minimal modifications, however, it overlooks the intrinsic\ncharacteristics of diverse visual tasks and hinders the learning of perception\ncapabilities. To address this issue, we propose a novel LMM architecture named\nLumen, a Large multimodal model with versatile vision-centric capability\nenhancement. We decouple the LMM's learning of perception capabilities into\ntask-agnostic and task-specific stages. Lumen first promotes fine-grained\nvision-language concept alignment, which is the fundamental capability for\nvarious visual tasks. Thus the output of the task-agnostic stage is a shared\nrepresentation for all the tasks we address in this paper. Then the\ntask-specific decoding is carried out by flexibly routing the shared\nrepresentation to lightweight task decoders with negligible training efforts.\nBenefiting from such a decoupled design, our Lumen surpasses existing LMM-based\napproaches on the COCO detection benchmark with a clear margin and exhibits\nseamless scalability to additional visual tasks. Furthermore, we also conduct\ncomprehensive ablation studies and generalization evaluations for deeper\ninsights. The code will be released at https://github.com/SxJyJay/Lumen.\n"
    },
    {
        "title": "Dynamic U-Net: Adaptively Calibrate Features for Abdominal Multi-organ\n  Segmentation",
        "published_time": "2024-03-12T04:10:06Z",
        "abstract": "  U-Net has been widely used for segmenting abdominal organs, achieving\npromising performance. However, when it is used for multi-organ segmentation,\nfirst, it may be limited in exploiting global long-range contextual information\ndue to the implementation of standard convolutions. Second, the use of\nspatial-wise downsampling (e.g., max pooling or strided convolutions) in the\nencoding path may lead to the loss of deformable or discriminative details.\nThird, features upsampled from the higher level are concatenated with those\nthat persevered via skip connections. However, repeated downsampling and\nupsampling operations lead to misalignments between them and their\nconcatenation degrades segmentation performance. To address these limitations,\nwe propose Dynamically Calibrated Convolution (DCC), Dynamically Calibrated\nDownsampling (DCD), and Dynamically Calibrated Upsampling (DCU) modules,\nrespectively. The DCC module can utilize global inter-dependencies between\nspatial and channel features to calibrate these features adaptively. The DCD\nmodule enables networks to adaptively preserve deformable or discriminative\nfeatures during downsampling. The DCU module can dynamically align and\ncalibrate upsampled features to eliminate misalignments before concatenations.\nWe integrated the proposed modules into a standard U-Net, resulting in a new\narchitecture, termed Dynamic U-Net. This architectural design enables U-Net to\ndynamically adjust features for different organs. We evaluated Dynamic U-Net in\ntwo abdominal multi-organ segmentation benchmarks. Dynamic U-Net achieved\nstatistically improved segmentation accuracy compared with standard U-Net. Our\ncode is available at https://github.com/sotiraslab/DynamicUNet.\n"
    },
    {
        "title": "Fine-Grained Prototypes Distillation for Few-Shot Object Detection",
        "published_time": "2024-01-15T12:12:48Z",
        "abstract": "  Few-shot object detection (FSOD) aims at extending a generic detector for\nnovel object detection with only a few training examples. It attracts great\nconcerns recently due to the practical meanings. Meta-learning has been\ndemonstrated to be an effective paradigm for this task. In general, methods\nbased on meta-learning employ an additional support branch to encode novel\nexamples (a.k.a. support images) into class prototypes, which are then fused\nwith query branch to facilitate the model prediction. However, the class-level\nprototypes are difficult to precisely generate, and they also lack detailed\ninformation, leading to instability in performance.New methods are required to\ncapture the distinctive local context for more robust novel object detection.\nTo this end, we propose to distill the most representative support features\ninto fine-grained prototypes. These prototypes are then assigned into query\nfeature maps based on the matching results, modeling the detailed feature\nrelations between two branches. This process is realized by our Fine-Grained\nFeature Aggregation (FFA) module. Moreover, in terms of high-level feature\nfusion, we propose Balanced Class-Agnostic Sampling (B-CAS) strategy and\nNon-Linear Fusion (NLF) module from differenct perspectives. They are\ncomplementary to each other and depict the high-level feature relations more\neffectively. Extensive experiments on PASCAL VOC and MS COCO benchmarks show\nthat our method sets a new state-of-the-art performance in most settings. Our\ncode is available at https://github.com/wangchen1801/FPD.\n"
    },
    {
        "title": "Let Storytelling Tell Vivid Stories: An Expressive and Fluent Multimodal\n  Storyteller",
        "published_time": "2024-03-12T04:07:00Z",
        "abstract": "  Storytelling aims to generate reasonable and vivid narratives based on an\nordered image stream. The fidelity to the image story theme and the divergence\nof story plots attract readers to keep reading. Previous works iteratively\nimproved the alignment of multiple modalities but ultimately resulted in the\ngeneration of simplistic storylines for image streams. In this work, we propose\na new pipeline, termed LLaMS, to generate multimodal human-level stories that\nare embodied in expressiveness and consistency. Specifically, by fully\nexploiting the commonsense knowledge within the LLM, we first employ a sequence\ndata auto-enhancement strategy to enhance factual content expression and\nleverage a textual reasoning architecture for expressive story generation and\nprediction. Secondly, we propose SQ-Adatpter module for story illustration\ngeneration which can maintain sequence consistency. Numerical results are\nconducted through human evaluation to verify the superiority of proposed LLaMS.\nEvaluations show that LLaMS achieves state-of-the-art storytelling performance\nand 86% correlation and 100% consistency win rate as compared with previous\nSOTA methods. Furthermore, ablation experiments are conducted to verify the\neffectiveness of proposed sequence data enhancement and SQ-Adapter.\n"
    },
    {
        "title": "Advancements in Continuous Glucose Monitoring: Integrating Deep Learning\n  and ECG Signal",
        "published_time": "2024-03-12T03:57:25Z",
        "abstract": "  This paper presents a novel approach to noninvasive hyperglycemia monitoring\nutilizing electrocardiograms (ECG) from an extensive database comprising 1119\nsubjects. Previous research on hyperglycemia or glucose detection using ECG has\nbeen constrained by challenges related to generalization and scalability,\nprimarily due to using all subjects' ECG in training without considering unseen\nsubjects as a critical factor for developing methods with effective\ngeneralization. We designed a deep neural network model capable of identifying\nsignificant features across various spatial locations and examining the\ninterdependencies among different features within each convolutional layer. To\nexpedite processing speed, we segment the ECG of each user to isolate one\nheartbeat or one cycle of the ECG. Our model was trained using data from 727\nsubjects, while 168 were used for validation. The testing phase involved 224\nunseen subjects, with a dataset consisting of 9,000 segments. The result\nindicates that the proposed algorithm effectively detects hyperglycemia with a\n91.60% area under the curve (AUC), 81.05% sensitivity, and 85.54% specificity.\n"
    },
    {
        "title": "Continual All-in-One Adverse Weather Removal with Knowledge Replay on a\n  Unified Network Structure",
        "published_time": "2024-03-12T03:50:57Z",
        "abstract": "  In real-world applications, image degeneration caused by adverse weather is\nalways complex and changes with different weather conditions from days and\nseasons. Systems in real-world environments constantly encounter adverse\nweather conditions that are not previously observed. Therefore, it practically\nrequires adverse weather removal models to continually learn from incrementally\ncollected data reflecting various degeneration types. Existing adverse weather\nremoval approaches, for either single or multiple adverse weathers, are mainly\ndesigned for a static learning paradigm, which assumes that the data of all\ntypes of degenerations to handle can be finely collected at one time before a\nsingle-phase learning process. They thus cannot directly handle the incremental\nlearning requirements. To address this issue, we made the earliest effort to\ninvestigate the continual all-in-one adverse weather removal task, in a setting\ncloser to real-world applications. Specifically, we develop a novel continual\nlearning framework with effective knowledge replay (KR) on a unified network\nstructure. Equipped with a principal component projection and an effective\nknowledge distillation mechanism, the proposed KR techniques are tailored for\nthe all-in-one weather removal task. It considers the characteristics of the\nimage restoration task with multiple degenerations in continual learning, and\nthe knowledge for different degenerations can be shared and accumulated in the\nunified network structure. Extensive experimental results demonstrate the\neffectiveness of the proposed method to deal with this challenging task, which\nperforms competitively to existing dedicated or joint training image\nrestoration methods. Our code is available at\nhttps://github.com/xiaojihh/CL_all-in-one.\n"
    },
    {
        "title": "Learning Hierarchical Color Guidance for Depth Map Super-Resolution",
        "published_time": "2024-03-12T03:44:46Z",
        "abstract": "  Color information is the most commonly used prior knowledge for depth map\nsuper-resolution (DSR), which can provide high-frequency boundary guidance for\ndetail restoration. However, its role and functionality in DSR have not been\nfully developed. In this paper, we rethink the utilization of color information\nand propose a hierarchical color guidance network to achieve DSR. On the one\nhand, the low-level detail embedding module is designed to supplement\nhigh-frequency color information of depth features in a residual mask manner at\nthe low-level stages. On the other hand, the high-level abstract guidance\nmodule is proposed to maintain semantic consistency in the reconstruction\nprocess by using a semantic mask that encodes the global guidance information.\nThe color information of these two dimensions plays a role in the front and\nback ends of the attention-based feature projection (AFP) module in a more\ncomprehensive form. Simultaneously, the AFP module integrates the multi-scale\ncontent enhancement block and adaptive attention projection block to make full\nuse of multi-scale information and adaptively project critical restoration\ninformation in an attention manner for DSR. Compared with the state-of-the-art\nmethods on four benchmark datasets, our method achieves more competitive\nperformance both qualitatively and quantitatively.\n"
    },
    {
        "title": "Rediscovering BCE Loss for Uniform Classification",
        "published_time": "2024-03-12T03:44:40Z",
        "abstract": "  This paper introduces the concept of uniform classification, which employs a\nunified threshold to classify all samples rather than adaptive threshold\nclassifying each individual sample. We also propose the uniform classification\naccuracy as a metric to measure the model's performance in uniform\nclassification. Furthermore, begin with a naive loss, we mathematically derive\na loss function suitable for the uniform classification, which is the BCE\nfunction integrated with a unified bias. We demonstrate the unified threshold\ncould be learned via the bias. The extensive experiments on six classification\ndatasets and three feature extraction models show that, compared to the SoftMax\nloss, the models trained with the BCE loss not only exhibit higher uniform\nclassification accuracy but also higher sample-wise classification accuracy. In\naddition, the learned bias from BCE loss is very close to the unified threshold\nused in the uniform classification. The features extracted by the models\ntrained with BCE loss not only possess uniformity but also demonstrate better\nintra-class compactness and inter-class distinctiveness, yielding superior\nperformance on open-set tasks such as face recognition.\n"
    },
    {
        "title": "DEADiff: An Efficient Stylization Diffusion Model with Disentangled\n  Representations",
        "published_time": "2024-03-11T17:35:23Z",
        "abstract": "  The diffusion-based text-to-image model harbors immense potential in\ntransferring reference style. However, current encoder-based approaches\nsignificantly impair the text controllability of text-to-image models while\ntransferring styles. In this paper, we introduce DEADiff to address this issue\nusing the following two strategies: 1) a mechanism to decouple the style and\nsemantics of reference images. The decoupled feature representations are first\nextracted by Q-Formers which are instructed by different text descriptions.\nThen they are injected into mutually exclusive subsets of cross-attention\nlayers for better disentanglement. 2) A non-reconstructive learning method. The\nQ-Formers are trained using paired images rather than the identical target, in\nwhich the reference image and the ground-truth image are with the same style or\nsemantics. We show that DEADiff attains the best visual stylization results and\noptimal balance between the text controllability inherent in the text-to-image\nmodel and style similarity to the reference image, as demonstrated both\nquantitatively and qualitatively. Our project page is\nhttps://tianhao-qi.github.io/DEADiff/.\n"
    },
    {
        "title": "MENTOR: Multilingual tExt detectioN TOward leaRning by analogy",
        "published_time": "2024-03-12T03:35:17Z",
        "abstract": "  Text detection is frequently used in vision-based mobile robots when they\nneed to interpret texts in their surroundings to perform a given task. For\ninstance, delivery robots in multilingual cities need to be capable of doing\nmultilingual text detection so that the robots can read traffic signs and road\nmarkings. Moreover, the target languages change from region to region, implying\nthe need of efficiently re-training the models to recognize the novel/new\nlanguages. However, collecting and labeling training data for novel languages\nare cumbersome, and the efforts to re-train an existing/trained text detector\nare considerable. Even worse, such a routine would repeat whenever a novel\nlanguage appears. This motivates us to propose a new problem setting for\ntackling the aforementioned challenges in a more efficient way: \"We ask for a\ngeneralizable multilingual text detection framework to detect and identify both\nseen and unseen language regions inside scene images without the requirement of\ncollecting supervised training data for unseen languages as well as model\nre-training\". To this end, we propose \"MENTOR\", the first work to realize a\nlearning strategy between zero-shot learning and few-shot learning for\nmultilingual scene text detection.\n"
    },
    {
        "title": "SparseLIF: High-Performance Sparse LiDAR-Camera Fusion for 3D Object\n  Detection",
        "published_time": "2024-03-12T03:34:03Z",
        "abstract": "  Sparse 3D detectors have received significant attention since the query-based\nparadigm embraces low latency without explicit dense BEV feature construction.\nHowever, these detectors achieve worse performance than their dense\ncounterparts. In this paper, we find the key to bridging the performance gap is\nto enhance the awareness of rich representations in two modalities. Here, we\npresent a high-performance fully sparse detector for end-to-end multi-modality\n3D object detection. The detector, termed SparseLIF, contains three key\ndesigns, which are (1) Perspective-Aware Query Generation (PAQG) to generate\nhigh-quality 3D queries with perspective priors, (2) RoI-Aware Sampling (RIAS)\nto further refine prior queries by sampling RoI features from each modality,\n(3) Uncertainty-Aware Fusion (UAF) to precisely quantify the uncertainty of\neach sensor modality and adaptively conduct final multi-modality fusion, thus\nachieving great robustness against sensor noises. By the time of submission\n(2024/03/08), SparseLIF achieves state-of-the-art performance on the nuScenes\ndataset, ranking 1st on both validation set and test benchmark, outperforming\nall state-of-the-art 3D object detectors by a notable margin. The source code\nwill be released upon acceptance.\n"
    },
    {
        "title": "General surgery vision transformer: A video pre-trained foundation model\n  for general surgery",
        "published_time": "2024-03-09T16:02:46Z",
        "abstract": "  The absence of openly accessible data and specialized foundation models is a\nmajor barrier for computational research in surgery. Toward this, (i) we\nopen-source the largest dataset of general surgery videos to-date, consisting\nof 680 hours of surgical videos, including data from robotic and laparoscopic\ntechniques across 28 procedures; (ii) we propose a technique for video\npre-training a general surgery vision transformer (GSViT) on surgical videos\nbased on forward video prediction that can run in real-time for surgical\napplications, toward which we open-source the code and weights of GSViT; (iii)\nwe also release code and weights for procedure-specific fine-tuned versions of\nGSViT across 10 procedures; (iv) we demonstrate the performance of GSViT on the\nCholec80 phase annotation task, displaying improved performance over\nstate-of-the-art single frame predictors.\n"
    },
    {
        "title": "Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration",
        "published_time": "2024-03-10T00:47:05Z",
        "abstract": "  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n"
    },
    {
        "title": "A Bayesian Approach to OOD Robustness in Image Classification",
        "published_time": "2024-03-12T03:15:08Z",
        "abstract": "  An important and unsolved problem in computer vision is to ensure that the\nalgorithms are robust to changes in image domains. We address this problem in\nthe scenario where we have access to images from the target domains but no\nannotations. Motivated by the challenges of the OOD-CV benchmark where we\nencounter real world Out-of-Domain (OOD) nuisances and occlusion, we introduce\na novel Bayesian approach to OOD robustness for object classification. Our work\nextends Compositional Neural Networks (CompNets), which have been shown to be\nrobust to occlusion but degrade badly when tested on OOD data. We exploit the\nfact that CompNets contain a generative head defined over feature vectors\nrepresented by von Mises-Fisher (vMF) kernels, which correspond roughly to\nobject parts, and can be learned without supervision. We obverse that some vMF\nkernels are similar between different domains, while others are not. This\nenables us to learn a transitional dictionary of vMF kernels that are\nintermediate between the source and target domains and train the generative\nmodel on this dictionary using the annotations on the source domain, followed\nby iterative refinement. This approach, termed Unsupervised Generative\nTransition (UGT), performs very well in OOD scenarios even when occlusion is\npresent. UGT is evaluated on different OOD benchmarks including the OOD-CV\ndataset, several popular datasets (e.g., ImageNet-C [9]), artificial image\ncorruptions (including adding occluders), and synthetic-to-real domain\ntransfer, and does well in all scenarios outperforming SOTA alternatives (e.g.\nup to 10% top-1 accuracy on Occluded OOD-CV dataset).\n"
    },
    {
        "title": "Style2Talker: High-Resolution Talking Head Generation with Emotion Style\n  and Art Style",
        "published_time": "2024-03-11T01:32:29Z",
        "abstract": "  Although automatically animating audio-driven talking heads has recently\nreceived growing interest, previous efforts have mainly concentrated on\nachieving lip synchronization with the audio, neglecting two crucial elements\nfor generating expressive videos: emotion style and art style. In this paper,\nwe present an innovative audio-driven talking face generation method called\nStyle2Talker. It involves two stylized stages, namely Style-E and Style-A,\nwhich integrate text-controlled emotion style and picture-controlled art style\ninto the final output. In order to prepare the scarce emotional text\ndescriptions corresponding to the videos, we propose a labor-free paradigm that\nemploys large-scale pretrained models to automatically annotate emotional text\nlabels for existing audiovisual datasets. Incorporating the synthetic emotion\ntexts, the Style-E stage utilizes a large-scale CLIP model to extract emotion\nrepresentations, which are combined with the audio, serving as the condition\nfor an efficient latent diffusion model designed to produce emotional motion\ncoefficients of a 3DMM model. Moving on to the Style-A stage, we develop a\ncoefficient-driven motion generator and an art-specific style path embedded in\nthe well-known StyleGAN. This allows us to synthesize high-resolution\nartistically stylized talking head videos using the generated emotional motion\ncoefficients and an art style source picture. Moreover, to better preserve\nimage details and avoid artifacts, we provide StyleGAN with the multi-scale\ncontent features extracted from the identity image and refine its intermediate\nfeature maps by the designed content encoder and refinement network,\nrespectively. Extensive experimental results demonstrate our method outperforms\nexisting state-of-the-art methods in terms of audio-lip synchronization and\nperformance of both emotion style and art style.\n"
    },
    {
        "title": "CEAT: Continual Expansion and Absorption Transformer for Non-Exemplar\n  Class-Incremental Learning",
        "published_time": "2024-03-11T12:40:12Z",
        "abstract": "  In real-world applications, dynamic scenarios require the models to possess\nthe capability to learn new tasks continuously without forgetting the old\nknowledge. Experience-Replay methods store a subset of the old images for joint\ntraining. In the scenario of more strict privacy protection, storing the old\nimages becomes infeasible, which leads to a more severe plasticity-stability\ndilemma and classifier bias. To meet the above challenges, we propose a new\narchitecture, named continual expansion and absorption transformer~(CEAT). The\nmodel can learn the novel knowledge by extending the expanded-fusion layers in\nparallel with the frozen previous parameters. After the task ends, we\nlosslessly absorb the extended parameters into the backbone to ensure that the\nnumber of parameters remains constant. To improve the learning ability of the\nmodel, we designed a novel prototype contrastive loss to reduce the overlap\nbetween old and new classes in the feature space. Besides, to address the\nclassifier bias towards the new classes, we propose a novel approach to\ngenerate the pseudo-features to correct the classifier. We experiment with our\nmethods on three standard Non-Exemplar Class-Incremental Learning~(NECIL)\nbenchmarks. Extensive experiments demonstrate that our model gets a significant\nimprovement compared with the previous works and achieves 5.38%, 5.20%, and\n4.92% improvement on CIFAR-100, TinyImageNet, and ImageNet-Subset.\n"
    },
    {
        "title": "Adaptive Bounding Box Uncertainties via Two-Step Conformal Prediction",
        "published_time": "2024-03-12T02:45:24Z",
        "abstract": "  Quantifying a model's predictive uncertainty is essential for safety-critical\napplications such as autonomous driving. We consider quantifying such\nuncertainty for multi-object detection. In particular, we leverage conformal\nprediction to obtain uncertainty intervals with guaranteed coverage for object\nbounding boxes. One challenge in doing so is that bounding box predictions are\nconditioned on the object's class label. Thus, we develop a novel two-step\nconformal approach that propagates uncertainty in predicted class labels into\nthe uncertainty intervals for the bounding boxes. This broadens the validity of\nour conformal coverage guarantees to include incorrectly classified objects,\nensuring their usefulness when maximal safety assurances are required.\nMoreover, we investigate novel ensemble and quantile regression formulations to\nensure the bounding box intervals are adaptive to object size, leading to a\nmore balanced coverage across sizes. Validating our two-step approach on\nreal-world datasets for 2D bounding box localization, we find that desired\ncoverage levels are satisfied with actionably tight predictive uncertainty\nintervals.\n"
    },
    {
        "title": "GlanceVAD: Exploring Glance Supervision for Label-efficient Video\n  Anomaly Detection",
        "published_time": "2024-03-10T09:57:10Z",
        "abstract": "  In recent years, video anomaly detection has been extensively investigated in\nboth unsupervised and weakly supervised settings to alleviate costly temporal\nlabeling. Despite significant progress, these methods still suffer from\nunsatisfactory results such as numerous false alarms, primarily due to the\nabsence of precise temporal anomaly annotation. In this paper, we present a\nnovel labeling paradigm, termed \"glance annotation\", to achieve a better\nbalance between anomaly detection accuracy and annotation cost. Specifically,\nglance annotation is a random frame within each abnormal event, which can be\neasily accessed and is cost-effective. To assess its effectiveness, we manually\nannotate the glance annotations for two standard video anomaly detection\ndatasets: UCF-Crime and XD-Violence. Additionally, we propose a customized\nGlanceVAD method, that leverages gaussian kernels as the basic unit to compose\nthe temporal anomaly distribution, enabling the learning of diverse and robust\nanomaly representations from the glance annotations. Through comprehensive\nanalysis and experiments, we verify that the proposed labeling paradigm can\nachieve an excellent trade-off between annotation cost and model performance.\nExtensive experimental results also demonstrate the effectiveness of our\nGlanceVAD approach, which significantly outperforms existing advanced\nunsupervised and weakly supervised methods. Code and annotations will be\npublicly available at https://github.com/pipixin321/GlanceVAD.\n"
    },
    {
        "title": "Scissorhands: Scrub Data Influence via Connection Sensitivity in\n  Networks",
        "published_time": "2024-01-11T08:39:14Z",
        "abstract": "  Machine unlearning has become a pivotal task to erase the influence of data\nfrom a trained model. It adheres to recent data regulation standards and\nenhances the privacy and security of machine learning applications. In this\nwork, we present a new machine unlearning approach Scissorhands. Initially,\nScissorhands identifies the most pertinent parameters in the given model\nrelative to the forgetting data via connection sensitivity. By reinitializing\nthe most influential top-k percent of these parameters, a trimmed model for\nerasing the influence of the forgetting data is obtained. Subsequently,\nScissorhands fine-tunes the trimmed model with a gradient projection-based\napproach, seeking parameters that preserve information on the remaining data\nwhile discarding information related to the forgetting data. Our experimental\nresults, conducted across image classification and image generation tasks,\ndemonstrate that Scissorhands, showcases competitive performance when compared\nto existing methods.\n"
    },
    {
        "title": "FlowVQTalker: High-Quality Emotional Talking Face Generation through\n  Normalizing Flow and Quantization",
        "published_time": "2024-03-11T01:58:04Z",
        "abstract": "  Generating emotional talking faces is a practical yet challenging endeavor.\nTo create a lifelike avatar, we draw upon two critical insights from a human\nperspective: 1) The connection between audio and the non-deterministic facial\ndynamics, encompassing expressions, blinks, poses, should exhibit synchronous\nand one-to-many mapping. 2) Vibrant expressions are often accompanied by\nemotion-aware high-definition (HD) textures and finely detailed teeth. However,\nboth aspects are frequently overlooked by existing methods. To this end, this\npaper proposes using normalizing Flow and Vector-Quantization modeling to\nproduce emotional talking faces that satisfy both insights concurrently\n(FlowVQTalker). Specifically, we develop a flow-based coefficient generator\nthat encodes the dynamics of facial emotion into a multi-emotion-class latent\nspace represented as a mixture distribution. The generation process commences\nwith random sampling from the modeled distribution, guided by the accompanying\naudio, enabling both lip-synchronization and the uncertain nonverbal facial\ncues generation. Furthermore, our designed vector-quantization image generator\ntreats the creation of expressive facial images as a code query task, utilizing\na learned codebook to provide rich, high-quality textures that enhance the\nemotional perception of the results. Extensive experiments are conducted to\nshowcase the effectiveness of our approach.\n"
    },
    {
        "title": "AesopAgent: Agent-driven Evolutionary System on Story-to-Video\n  Production",
        "published_time": "2024-03-12T02:30:50Z",
        "abstract": "  The Agent and AIGC (Artificial Intelligence Generated Content) technologies\nhave recently made significant progress. We propose AesopAgent, an Agent-driven\nEvolutionary System on Story-to-Video Production. AesopAgent is a practical\napplication of agent technology for multimodal content generation. The system\nintegrates multiple generative capabilities within a unified framework, so that\nindividual users can leverage these modules easily. This innovative system\nwould convert user story proposals into scripts, images, and audio, and then\nintegrate these multimodal contents into videos. Additionally, the animating\nunits (e.g., Gen-2 and Sora) could make the videos more infectious. The\nAesopAgent system could orchestrate task workflow for video generation,\nensuring that the generated video is both rich in content and coherent. This\nsystem mainly contains two layers, i.e., the Horizontal Layer and the Utility\nLayer. In the Horizontal Layer, we introduce a novel RAG-based evolutionary\nsystem that optimizes the whole video generation workflow and the steps within\nthe workflow. It continuously evolves and iteratively optimizes workflow by\naccumulating expert experience and professional knowledge, including optimizing\nthe LLM prompts and utilities usage. The Utility Layer provides multiple\nutilities, leading to consistent image generation that is visually coherent in\nterms of composition, characters, and style. Meanwhile, it provides audio and\nspecial effects, integrating them into expressive and logically arranged\nvideos. Overall, our AesopAgent achieves state-of-the-art performance compared\nwith many previous works in visual storytelling. Our AesopAgent is designed for\nconvenient service for individual users, which is available on the following\npage: https://aesopai.github.io/.\n"
    },
    {
        "title": "SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic\n  Microscopy Segmentation",
        "published_time": "2024-03-12T02:28:29Z",
        "abstract": "  It has been shown that traditional deep learning methods for electronic\nmicroscopy segmentation usually suffer from low transferability when samples\nand annotations are limited, while large-scale vision foundation models are\nmore robust when transferring between different domains but facing sub-optimal\nimprovement under fine-tuning. In this work, we present a new few-shot domain\nadaptation framework SAMDA, which combines the Segment Anything Model(SAM) with\nnnUNet in the embedding space to achieve high transferability and accuracy.\nSpecifically, we choose the Unet-based network as the \"expert\" component to\nlearn segmentation features efficiently and design a SAM-based adaptation\nmodule as the \"generic\" component for domain transfer. By amalgamating the\n\"generic\" and \"expert\" components, we mitigate the modality imbalance in the\ncomplex pre-training knowledge inherent to large-scale Vision Foundation models\nand the challenge of transferability inherent to traditional neural networks.\nThe effectiveness of our model is evaluated on two electron microscopic image\ndatasets with different modalities for mitochondria segmentation, which\nimproves the dice coefficient on the target domain by 6.7%. Also, the SAM-based\nadaptor performs significantly better with only a single annotated image than\nthe 10-shot domain adaptation on nnUNet. We further verify our model on four\nMRI datasets from different sources to prove its generalization ability.\n"
    },
    {
        "title": "GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical\n  structure Generation",
        "published_time": "2024-03-12T02:09:39Z",
        "abstract": "  The annotation burden and extensive labor for gathering a large medical\ndataset with images and corresponding labels are rarely cost-effective and\nhighly intimidating. This results in a lack of abundant training data that\nundermines downstream tasks and partially contributes to the challenge image\nanalysis faces in the medical field. As a workaround, given the recent success\nof generative neural models, it is now possible to synthesize image datasets at\na high fidelity guided by external constraints. This paper explores this\npossibility and presents \\textbf{GuideGen}: a pipeline that jointly generates\nCT images and tissue masks for abdominal organs and colorectal cancer\nconditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to\nfit the discrete distribution of mask labels and generate low-resolution 3D\ntissue masks. Secondly, our Conditional Image Generator autoregressively\ngenerates CT slices conditioned on a corresponding mask slice to incorporate\nboth style information and anatomical guidance. This pipeline guarantees high\nfidelity and variability as well as exact alignment between generated CT\nvolumes and tissue masks. Both qualitative and quantitative experiments on 3D\nabdominal CTs demonstrate a high performance of our proposed pipeline, thereby\nproving our method can serve as a dataset generator and provide potential\nbenefits to downstream tasks. It is hoped that our work will offer a promising\nsolution on the multimodality generation of CT and its anatomical mask. Our\nsource code is publicly available at\nhttps://github.com/OvO1111/JointImageGeneration.\n"
    },
    {
        "title": "Towards Zero-shot Human-Object Interaction Detection via Vision-Language\n  Integration",
        "published_time": "2024-03-12T02:07:23Z",
        "abstract": "  Human-object interaction (HOI) detection aims to locate human-object pairs\nand identify their interaction categories in images. Most existing methods\nprimarily focus on supervised learning, which relies on extensive manual HOI\nannotations. In this paper, we propose a novel framework, termed Knowledge\nIntegration to HOI (KI2HOI), that effectively integrates the knowledge of\nvisual-language model to improve zero-shot HOI detection. Specifically, the\nverb feature learning module is designed based on visual semantics, by\nemploying the verb extraction decoder to convert corresponding verb queries\ninto interaction-specific category representations. We develop an effective\nadditive self-attention mechanism to generate more comprehensive visual\nrepresentations. Moreover, the innovative interaction representation decoder\neffectively extracts informative regions by integrating spatial and visual\nfeature information through a cross-attention mechanism. To deal with zero-shot\nlearning in low-data, we leverage a priori knowledge from the CLIP text encoder\nto initialize the linear classifier for enhanced interaction understanding.\nExtensive experiments conducted on the mainstream HICO-DET and V-COCO datasets\ndemonstrate that our model outperforms the previous methods in various\nzero-shot and full-supervised settings.\n"
    },
    {
        "title": "Time-Efficient Light-Field Acquisition Using Coded Aperture and Events",
        "published_time": "2024-03-12T02:04:17Z",
        "abstract": "  We propose a computational imaging method for time-efficient light-field\nacquisition that combines a coded aperture with an event-based camera.\nDifferent from the conventional coded-aperture imaging method, our method\napplies a sequence of coding patterns during a single exposure for an image\nframe. The parallax information, which is related to the differences in coding\npatterns, is recorded as events. The image frame and events, all of which are\nmeasured in a single exposure, are jointly used to computationally reconstruct\na light field. We also designed an algorithm pipeline for our method that is\nend-to-end trainable on the basis of deep optics and compatible with real\ncamera hardware. We experimentally showed that our method can achieve more\naccurate reconstruction than several other imaging methods with a single\nexposure. We also developed a hardware prototype with the potential to complete\nthe measurement on the camera within 22 msec and demonstrated that light fields\nfrom real 3-D scenes can be obtained with convincing visual quality. Our\nsoftware and supplementary video are available from our project website.\n"
    },
    {
        "title": "Gradient Alignment for Cross-Domain Face Anti-Spoofing",
        "published_time": "2024-02-29T02:57:44Z",
        "abstract": "  Recent advancements in domain generalization (DG) for face anti-spoofing\n(FAS) have garnered considerable attention. Traditional methods have focused on\ndesigning learning objectives and additional modules to isolate domain-specific\nfeatures while retaining domain-invariant characteristics in their\nrepresentations. However, such approaches often lack guarantees of consistent\nmaintenance of domain-invariant features or the complete removal of\ndomain-specific features. Furthermore, most prior works of DG for FAS do not\nensure convergence to a local flat minimum, which has been shown to be\nadvantageous for DG. In this paper, we introduce GAC-FAS, a novel learning\nobjective that encourages the model to converge towards an optimal flat minimum\nwithout necessitating additional learning modules. Unlike conventional\nsharpness-aware minimizers, GAC-FAS identifies ascending points for each domain\nand regulates the generalization gradient updates at these points to align\ncoherently with empirical risk minimization (ERM) gradient updates. This unique\napproach specifically guides the model to be robust against domain shifts. We\ndemonstrate the efficacy of GAC-FAS through rigorous testing on challenging\ncross-domain FAS datasets, where it establishes state-of-the-art performance.\nThe code is available at https://github.com/leminhbinh0209/CVPR24-FAS.\n"
    },
    {
        "title": "Calibrating Multi-modal Representations: A Pursuit of Group Robustness\n  without Annotations",
        "published_time": "2024-03-12T01:47:17Z",
        "abstract": "  Fine-tuning pre-trained vision-language models, like CLIP, has yielded\nsuccess on diverse downstream tasks. However, several pain points persist for\nthis paradigm: (i) directly tuning entire pre-trained models becomes both\ntime-intensive and computationally costly. Additionally, these tuned models\ntend to become highly specialized, limiting their practicality for real-world\ndeployment; (ii) recent studies indicate that pre-trained vision-language\nclassifiers may overly depend on spurious features -- patterns that correlate\nwith the target in training data, but are not related to the true labeling\nfunction; and (iii) existing studies on mitigating the reliance on spurious\nfeatures, largely based on the assumption that we can identify such features,\ndoes not provide definitive assurance for real-world applications. As a\npiloting study, this work focuses on exploring mitigating the reliance on\nspurious features for CLIP without using any group annotation. To this end, we\nsystematically study the existence of spurious correlation on CLIP and\nCILP+ERM. We first, following recent work on Deep Feature Reweighting (DFR),\nverify that last-layer retraining can greatly improve group robustness on\npretrained CLIP. In view of them, we advocate a lightweight representation\ncalibration method for fine-tuning CLIP, by first generating a calibration set\nusing the pretrained CLIP, and then calibrating representations of samples\nwithin this set through contrastive learning, all without the need for group\nlabels. Extensive experiments and in-depth visualizations on several benchmarks\nvalidate the effectiveness of our proposals, largely reducing reliance and\nsignificantly boosting the model generalization.\n"
    },
    {
        "title": "Frequency-Aware Deepfake Detection: Improving Generalizability through\n  Frequency Space Learning",
        "published_time": "2024-03-12T01:28:00Z",
        "abstract": "  This research addresses the challenge of developing a universal deepfake\ndetector that can effectively identify unseen deepfake images despite limited\ntraining data. Existing frequency-based paradigms have relied on\nfrequency-level artifacts introduced during the up-sampling in GAN pipelines to\ndetect forgeries. However, the rapid advancements in synthesis technology have\nled to specific artifacts for each generation model. Consequently, these\ndetectors have exhibited a lack of proficiency in learning the frequency domain\nand tend to overfit to the artifacts present in the training data, leading to\nsuboptimal performance on unseen sources. To address this issue, we introduce a\nnovel frequency-aware approach called FreqNet, centered around frequency domain\nlearning, specifically designed to enhance the generalizability of deepfake\ndetectors. Our method forces the detector to continuously focus on\nhigh-frequency information, exploiting high-frequency representation of\nfeatures across spatial and channel dimensions. Additionally, we incorporate a\nstraightforward frequency domain learning module to learn source-agnostic\nfeatures. It involves convolutional layers applied to both the phase spectrum\nand amplitude spectrum between the Fast Fourier Transform (FFT) and Inverse\nFast Fourier Transform (iFFT). Extensive experimentation involving 17 GANs\ndemonstrates the effectiveness of our proposed method, showcasing\nstate-of-the-art performance (+9.8\\%) while requiring fewer parameters. The\ncode is available at {\\cred\n\\url{https://github.com/chuangchuangtan/FreqNet-DeepfakeDetection}}.\n"
    },
    {
        "title": "Tree Counting by Bridging 3D Point Clouds with Imagery",
        "published_time": "2024-03-04T11:02:17Z",
        "abstract": "  Accurate and consistent methods for counting trees based on remote sensing\ndata are needed to support sustainable forest management, assess climate change\nmitigation strategies, and build trust in tree carbon credits. Two-dimensional\nremote sensing imagery primarily shows overstory canopy, and it does not\nfacilitate easy differentiation of individual trees in areas with a dense\ncanopy and does not allow for easy separation of trees when the canopy is\ndense. We leverage the fusion of three-dimensional LiDAR measurements and 2D\nimagery to facilitate the accurate counting of trees. We compare a deep\nlearning approach to counting trees in forests using 3D airborne LiDAR data and\n2D imagery. The approach is compared with state-of-the-art algorithms, like\noperating on 3D point cloud and 2D imagery. We empirically evaluate the\ndifferent methods on the NeonTreeCount data set, which we use to define a\ntree-counting benchmark. The experiments show that FuseCountNet yields more\naccurate tree counts.\n"
    },
    {
        "title": "It's All About Your Sketch: Democratising Sketch Control in Diffusion\n  Models",
        "published_time": "2024-03-12T01:05:25Z",
        "abstract": "  This paper unravels the potential of sketches for diffusion models,\naddressing the deceptive promise of direct sketch control in generative AI. We\nimportantly democratise the process, enabling amateur sketches to generate\nprecise images, living up to the commitment of \"what you sketch is what you\nget\". A pilot study underscores the necessity, revealing that deformities in\nexisting models stem from spatial-conditioning. To rectify this, we propose an\nabstraction-aware framework, utilising a sketch adapter, adaptive time-step\nsampling, and discriminative guidance from a pre-trained fine-grained\nsketch-based image retrieval model, working synergistically to reinforce\nfine-grained sketch-photo association. Our approach operates seamlessly during\ninference without the need for textual prompts; a simple, rough sketch akin to\nwhat you and I can create suffices! We welcome everyone to examine results\npresented in the paper and its supplementary. Contributions include\ndemocratising sketch control, introducing an abstraction-aware framework, and\nleveraging discriminative guidance, validated through extensive experiments.\n"
    },
    {
        "title": "Domain Adaptation Using Pseudo Labels",
        "published_time": "2024-02-09T22:15:11Z",
        "abstract": "  In the absence of labeled target data, unsupervised domain adaptation\napproaches seek to align the marginal distributions of the source and target\ndomains in order to train a classifier for the target. Unsupervised domain\nalignment procedures are category-agnostic and end up misaligning the\ncategories. We address this problem by deploying a pretrained network to\ndetermine accurate labels for the target domain using a multi-stage\npseudo-label refinement procedure. The filters are based on the confidence,\ndistance (conformity), and consistency of the pseudo labels. Our results on\nmultiple datasets demonstrate the effectiveness of our simple procedure in\ncomparison with complex state-of-the-art techniques.\n"
    },
    {
        "title": "Learn and Search: An Elegant Technique for Object Lookup using\n  Contrastive Learning",
        "published_time": "2024-03-12T00:58:19Z",
        "abstract": "  The rapid proliferation of digital content and the ever-growing need for\nprecise object recognition and segmentation have driven the advancement of\ncutting-edge techniques in the field of object classification and segmentation.\nThis paper introduces \"Learn and Search\", a novel approach for object lookup\nthat leverages the power of contrastive learning to enhance the efficiency and\neffectiveness of retrieval systems.\n  In this study, we present an elegant and innovative methodology that\nintegrates deep learning principles and contrastive learning to tackle the\nchallenges of object search. Our extensive experimentation reveals compelling\nresults, with \"Learn and Search\" achieving superior Similarity Grid Accuracy,\nshowcasing its efficacy in discerning regions of utmost similarity within an\nimage relative to a cropped image.\n  The seamless fusion of deep learning and contrastive learning to address the\nintricacies of object identification not only promises transformative\napplications in image recognition, recommendation systems, and content tagging\nbut also revolutionizes content-based search and retrieval. The amalgamation of\nthese techniques, as exemplified by \"Learn and Search,\" represents a\nsignificant stride in the ongoing evolution of methodologies in the dynamic\nrealm of object classification and segmentation.\n"
    },
    {
        "title": "You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image\n  Retrieval",
        "published_time": "2024-03-12T00:27:18Z",
        "abstract": "  Two primary input modalities prevail in image retrieval: sketch and text.\nWhile text is widely used for inter-category retrieval tasks, sketches have\nbeen established as the sole preferred modality for fine-grained image\nretrieval due to their ability to capture intricate visual details. In this\npaper, we question the reliance on sketches alone for fine-grained image\nretrieval by simultaneously exploring the fine-grained representation\ncapabilities of both sketch and text, orchestrating a duet between the two. The\nend result enables precise retrievals previously unattainable, allowing users\nto pose ever-finer queries and incorporate attributes like colour and\ncontextual cues from text. For this purpose, we introduce a novel\ncompositionality framework, effectively combining sketches and text using\npre-trained CLIP models, while eliminating the need for extensive fine-grained\ntextual descriptions. Last but not least, our system extends to novel\napplications in composite image retrieval, domain attribute transfer, and\nfine-grained generation, providing solutions for various real-world scenarios.\n"
    },
    {
        "title": "Monocular Microscope to CT Registration using Pose Estimation of the\n  Incus for Augmented Reality Cochlear Implant Surgery",
        "published_time": "2024-03-12T00:26:08Z",
        "abstract": "  For those experiencing severe-to-profound sensorineural hearing loss, the\ncochlear implant (CI) is the preferred treatment. Augmented reality (AR) aided\nsurgery can potentially improve CI procedures and hearing outcomes. Typically,\nAR solutions for image-guided surgery rely on optical tracking systems to\nregister pre-operative planning information to the display so that hidden\nanatomy or other important information can be overlayed and co-registered with\nthe view of the surgical scene. In this paper, our goal is to develop a method\nthat permits direct 2D-to-3D registration of the microscope video to the\npre-operative Computed Tomography (CT) scan without the need for external\ntracking equipment. Our proposed solution involves using surface mapping of a\nportion of the incus in surgical recordings and determining the pose of this\nstructure relative to the surgical microscope by performing pose estimation via\nthe perspective-n-point (PnP) algorithm. This registration can then be applied\nto pre-operative segmentations of other anatomy-of-interest, as well as the\nplanned electrode insertion trajectory to co-register this information for the\nAR display. Our results demonstrate the accuracy with an average rotation error\nof less than 25 degrees and a translation error of less than 2 mm, 3 mm, and\n0.55% for the x, y, and z axes, respectively. Our proposed method has the\npotential to be applicable and generalized to other surgical procedures while\nonly needing a monocular microscope during intra-operation.\n"
    },
    {
        "title": "Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers",
        "published_time": "2024-03-12T00:02:03Z",
        "abstract": "  This paper, for the first time, explores text-to-image diffusion models for\nZero-Shot Sketch-based Image Retrieval (ZS-SBIR). We highlight a pivotal\ndiscovery: the capacity of text-to-image diffusion models to seamlessly bridge\nthe gap between sketches and photos. This proficiency is underpinned by their\nrobust cross-modal capabilities and shape bias, findings that are substantiated\nthrough our pilot studies. In order to harness pre-trained diffusion models\neffectively, we introduce a straightforward yet powerful strategy focused on\ntwo key aspects: selecting optimal feature layers and utilising visual and\ntextual prompts. For the former, we identify which layers are most enriched\nwith information and are best suited for the specific retrieval requirements\n(category-level or fine-grained). Then we employ visual and textual prompts to\nguide the model's feature extraction process, enabling it to generate more\ndiscriminative and contextually relevant cross-modal representations. Extensive\nexperiments on several benchmark datasets validate significant performance\nimprovements.\n"
    },
    {
        "title": "Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like\n  Speed",
        "published_time": "2024-03-07T18:58:40Z",
        "abstract": "  We present a novel method for efficiently producing semi-dense matches across\nimages. Previous detector-free matcher LoFTR has shown remarkable matching\ncapability in handling large-viewpoint change and texture-poor scenarios but\nsuffers from low efficiency. We revisit its design choices and derive multiple\nimprovements for both efficiency and accuracy. One key observation is that\nperforming the transformer over the entire feature map is redundant due to\nshared local information, therefore we propose an aggregated attention\nmechanism with adaptive token selection for efficiency. Furthermore, we find\nspatial variance exists in LoFTR's fine correlation module, which is adverse to\nmatching accuracy. A novel two-stage correlation layer is proposed to achieve\naccurate subpixel correspondences for accuracy improvement. Our efficiency\noptimized model is $\\sim 2.5\\times$ faster than LoFTR which can even surpass\nstate-of-the-art efficient sparse matching pipeline SuperPoint + LightGlue.\nMoreover, extensive experiments show that our method can achieve higher\naccuracy compared with competitive semi-dense matchers, with considerable\nefficiency benefits. This opens up exciting prospects for large-scale or\nlatency-sensitive applications such as image retrieval and 3D reconstruction.\nProject page: https://zju3dv.github.io/efficientloftr.\n"
    },
    {
        "title": "TikTokActions: A TikTok-Derived Video Dataset for Human Action\n  Recognition",
        "published_time": "2024-02-14T00:41:10Z",
        "abstract": "  The increasing variety and quantity of tagged multimedia content on platforms\nsuch as TikTok provides an opportunity to advance computer vision modeling. We\nhave curated a distinctive dataset of 283,582 unique video clips categorized\nunder 386 hashtags relating to modern human actions. We release this dataset as\na valuable resource for building domain-specific foundation models for human\nmovement modeling tasks such as action recognition. To validate this dataset,\nwhich we name TikTokActions, we perform two sets of experiments. First, we\npretrain the state-of-the-art VideoMAEv2 with a ViT-base backbone on\nTikTokActions subset, and then fine-tune and evaluate on popular datasets such\nas UCF101 and the HMDB51. We find that the performance of the model pre-trained\nusing our Tik-Tok dataset is comparable to models trained on larger action\nrecognition datasets (95.3% on UCF101 and 53.24% on HMDB51). Furthermore, our\ninvestigation into the relationship between pre-training dataset size and\nfine-tuning performance reveals that beyond a certain threshold, the\nincremental benefit of larger training sets diminishes. This work introduces a\nuseful TikTok video dataset that is available for public use and provides\ninsights into the marginal benefit of increasing pre-training dataset sizes for\nvideo-based foundation models.\n"
    },
    {
        "title": "How to Handle Sketch-Abstraction in Sketch-Based Image Retrieval?",
        "published_time": "2024-03-11T23:08:29Z",
        "abstract": "  In this paper, we propose a novel abstraction-aware sketch-based image\nretrieval framework capable of handling sketch abstraction at varied levels.\nPrior works had mainly focused on tackling sub-factors such as drawing style\nand order, we instead attempt to model abstraction as a whole, and propose\nfeature-level and retrieval granularity-level designs so that the system builds\ninto its DNA the necessary means to interpret abstraction. On learning\nabstraction-aware features, we for the first-time harness the rich semantic\nembedding of pre-trained StyleGAN model, together with a novel\nabstraction-level mapper that deciphers the level of abstraction and\ndynamically selects appropriate dimensions in the feature matrix\ncorrespondingly, to construct a feature matrix embedding that can be freely\ntraversed to accommodate different levels of abstraction. For granularity-level\nabstraction understanding, we dictate that the retrieval model should not treat\nall abstraction-levels equally and introduce a differentiable surrogate Acc.@q\nloss to inject that understanding into the system. Different to the\ngold-standard triplet loss, our Acc.@q loss uniquely allows a sketch to\nnarrow/broaden its focus in terms of how stringent the evaluation should be -\nthe more abstract a sketch, the less stringent (higher $q$). Extensive\nexperiments depict our method to outperform existing state-of-the-arts in\nstandard SBIR tasks along with challenging scenarios like early retrieval,\nforensic sketch-photo matching, and style-invariant retrieval.\n"
    },
    {
        "title": "Action Reimagined: Text-to-Pose Video Editing for Dynamic Human Actions",
        "published_time": "2024-03-11T22:46:46Z",
        "abstract": "  We introduce a novel text-to-pose video editing method, ReimaginedAct. While\nexisting video editing tasks are limited to changes in attributes, backgrounds,\nand styles, our method aims to predict open-ended human action changes in\nvideo. Moreover, our method can accept not only direct instructional text\nprompts but also `what if' questions to predict possible action changes.\nReimaginedAct comprises video understanding, reasoning, and editing modules.\nFirst, an LLM is utilized initially to obtain a plausible answer for the\ninstruction or question, which is then used for (1) prompting Grounded-SAM to\nproduce bounding boxes of relevant individuals and (2) retrieving a set of pose\nvideos that we have collected for editing human actions. The retrieved pose\nvideos and the detected individuals are then utilized to alter the poses\nextracted from the original video. We also employ a timestep blending module to\nensure the edited video retains its original content except where necessary\nmodifications are needed. To facilitate research in text-to-pose video editing,\nwe introduce a new evaluation dataset, WhatifVideo-1.0. This dataset includes\nvideos of different scenarios spanning a range of difficulty levels, along with\nquestions and text prompts. Experimental results demonstrate that existing\nvideo editing methods struggle with human action editing, while our approach\ncan achieve effective action editing and even imaginary editing from\ncounterfactual questions.\n"
    },
    {
        "title": "3D Diffuser Actor: Policy Diffusion with 3D Scene Representations",
        "published_time": "2024-02-16T18:43:02Z",
        "abstract": "  We marry diffusion policies and 3D scene representations for robot\nmanipulation. Diffusion policies learn the action distribution conditioned on\nthe robot and environment state using conditional diffusion models. They have\nrecently shown to outperform both deterministic and alternative\nstate-conditioned action distribution learning methods. 3D robot policies use\n3D scene feature representations aggregated from a single or multiple camera\nviews using sensed depth. They have shown to generalize better than their 2D\ncounterparts across camera viewpoints. We unify these two lines of work and\npresent 3D Diffuser Actor, a neural policy architecture that, given a language\ninstruction, builds a 3D representation of the visual scene and conditions on\nit to iteratively denoise 3D rotations and translations for the robot's\nend-effector. At each denoising iteration, our model represents end-effector\npose estimates as 3D scene tokens and predicts the 3D translation and rotation\nerror for each of them, by featurizing them using 3D relative attention to\nother 3D visual and language tokens. 3D Diffuser Actor sets a new\nstate-of-the-art on RLBench with an absolute performance gain of 16.3% over the\ncurrent SOTA on a multi-view setup and an absolute gain of 13.1% on a\nsingle-view setup. On the CALVIN benchmark, it outperforms the current SOTA in\nthe setting of zero-shot unseen scene generalization by being able to\nsuccessfully run 0.2 more tasks, a 7% relative increase. It also works in the\nreal world from a handful of demonstrations. We ablate our model's\narchitectural design choices, such as 3D scene featurization and 3D relative\nattentions, and show they all help generalization. Our results suggest that 3D\nscene representations and powerful generative modeling are keys to efficient\nrobot learning from demonstrations.\n"
    },
    {
        "title": "2023 Low-Power Computer Vision Challenge (LPCVC) Summary",
        "published_time": "2024-03-11T20:51:18Z",
        "abstract": "  This article describes the 2023 IEEE Low-Power Computer Vision Challenge\n(LPCVC). Since 2015, LPCVC has been an international competition devoted to\ntackling the challenge of computer vision (CV) on edge devices. Most CV\nresearchers focus on improving accuracy, at the expense of ever-growing sizes\nof machine models. LPCVC balances accuracy with resource requirements. Winners\nmust achieve high accuracy with short execution time when their CV solutions\nrun on an embedded device, such as Raspberry PI or Nvidia Jetson Nano. The\nvision problem for 2023 LPCVC is segmentation of images acquired by Unmanned\nAerial Vehicles (UAVs, also called drones) after disasters. The 2023 LPCVC\nattracted 60 international teams that submitted 676 solutions during the\nsubmission window of one month. This article explains the setup of the\ncompetition and highlights the winners' methods that improve accuracy and\nshorten execution time.\n"
    },
    {
        "title": "One Category One Prompt: Dataset Distillation using Diffusion Models",
        "published_time": "2024-03-11T20:23:59Z",
        "abstract": "  The extensive amounts of data required for training deep neural networks pose\nsignificant challenges on storage and transmission fronts. Dataset distillation\nhas emerged as a promising technique to condense the information of massive\ndatasets into a much smaller yet representative set of synthetic samples.\nHowever, traditional dataset distillation approaches often struggle to scale\neffectively with high-resolution images and more complex architectures due to\nthe limitations in bi-level optimization. Recently, several works have proposed\nexploiting knowledge distillation with decoupled optimization schemes to scale\nup dataset distillation. Although these methods effectively address the\nscalability issue, they rely on extensive image augmentations requiring the\nstorage of soft labels for augmented images. In this paper, we introduce\nDataset Distillation using Diffusion Models (D3M) as a novel paradigm for\ndataset distillation, leveraging recent advancements in generative\ntext-to-image foundation models. Our approach utilizes textual inversion, a\ntechnique for fine-tuning text-to-image generative models, to create concise\nand informative representations for large datasets. By employing these learned\ntext prompts, we can efficiently store and infer new samples for introducing\ndata variability within a fixed memory budget. We show the effectiveness of our\nmethod through extensive experiments across various computer vision benchmark\ndatasets with different memory budgets.\n"
    },
    {
        "title": "Exploring Cluster Analysis in Nelore Cattle Visual Score Attribution",
        "published_time": "2024-03-11T20:07:05Z",
        "abstract": "  Assessing the biotype of cattle through human visual inspection is a very\ncommon and important practice in precision cattle breeding. This paper presents\nthe results of a correlation analysis between scores produced by humans for\nNelore cattle and a variety of measurements that can be derived from images or\nother instruments. It also presents a study using the k-means algorithm to\ngenerate new ways of clustering a batch of cattle using the measurements that\nmost correlate with the animal's body weight and visual scores.\n"
    },
    {
        "title": "COMQ: A Backpropagation-Free Algorithm for Post-Training Quantization",
        "published_time": "2024-03-11T20:04:03Z",
        "abstract": "  Post-training quantization (PTQ) has emerged as a practical approach to\ncompress large neural networks, making them highly efficient for deployment.\nHowever, effectively reducing these models to their low-bit counterparts\nwithout compromising the original accuracy remains a key challenge. In this\npaper, we propose an innovative PTQ algorithm termed COMQ, which sequentially\nconducts coordinate-wise minimization of the layer-wise reconstruction errors.\nWe consider the widely used integer quantization, where every quantized weight\ncan be decomposed into a shared floating-point scalar and an integer bit-code.\nWithin a fixed layer, COMQ treats all the scaling factor(s) and bit-codes as\nthe variables of the reconstruction error. Every iteration improves this error\nalong a single coordinate while keeping all other variables constant. COMQ is\neasy to use and requires no hyper-parameter tuning. It instead involves only\ndot products and rounding operations. We update these variables in a carefully\ndesigned greedy order, significantly enhancing the accuracy. COMQ achieves\nremarkable results in quantizing 4-bit Vision Transformers, with a negligible\nloss of less than 1% in Top-1 accuracy. In 4-bit INT quantization of\nconvolutional neural networks, COMQ maintains near-lossless accuracy with a\nminimal drop of merely 0.3% in Top-1 accuracy.\n"
    },
    {
        "title": "A New Machine Learning Dataset of Bulldog Nostril Images for Stenosis\n  Degree Classification",
        "published_time": "2024-03-11T20:02:17Z",
        "abstract": "  Brachycephaly, a conformation trait in some dog breeds, causes BOAS, a\nrespiratory disorder that affects the health and welfare of the dogs with\nvarious symptoms. In this paper, a new annotated dataset composed of 190 images\nof bulldogs' nostrils is presented. Three degrees of stenosis are approximately\nequally represented in the dataset: mild, moderate and severe stenosis. The\ndataset also comprises a small quantity of non stenotic nostril images. To the\nbest of our knowledge, this is the first image dataset addressing this problem.\nFurthermore, deep learning is investigated as an alternative to automatically\ninfer stenosis degree using nostril images. In this work, several neural\nnetworks were tested: ResNet50, MobileNetV3, DenseNet201, SwinV2 and MaxViT.\nFor this evaluation, the problem was modeled in two different ways: first, as a\nthree-class classification problem (mild or open, moderate, and severe);\nsecond, as a binary classification problem, with severe stenosis as target. For\nthe multiclass classification, a maximum median f-score of 53.77\\% was achieved\nby the MobileNetV3. For binary classification, a maximum median f-score of\n72.08\\% has been reached by ResNet50, indicating that the problem is\nchallenging but possibly tractable.\n"
    },
    {
        "title": "Heterogeneous Image-based Classification Using Distributional Data\n  Analysis",
        "published_time": "2024-03-11T19:41:40Z",
        "abstract": "  Diagnostic imaging has gained prominence as potential biomarkers for early\ndetection and diagnosis in a diverse array of disorders including cancer.\nHowever, existing methods routinely face challenges arising from various\nfactors such as image heterogeneity. We develop a novel imaging-based\ndistributional data analysis (DDA) approach that incorporates the probability\n(quantile) distribution of the pixel-level features as covariates. The proposed\napproach uses a smoothed quantile distribution (via a suitable basis\nrepresentation) as functional predictors in a scalar-on-functional quantile\nregression model. Some distinctive features of the proposed approach include\nthe ability to: (i) account for heterogeneity within the image; (ii)\nincorporate granular information spanning the entire distribution; and (iii)\ntackle variability in image sizes for unregistered images in cancer\napplications. Our primary goal is risk prediction in Hepatocellular carcinoma\nthat is achieved via predicting the change in tumor grades at post-diagnostic\nvisits using pre-diagnostic enhancement pattern mapping (EPM) images of the\nliver. Along the way, the proposed DDA approach is also used for case versus\ncontrol diagnosis and risk stratification objectives. Our analysis reveals that\nwhen coupled with global structural radiomics features derived from the\ncorresponding T1-MRI scans, the proposed smoothed quantile distributions\nderived from EPM images showed considerable improvements in sensitivity and\ncomparable specificity in contrast to classification based on routinely used\nsummary measures that do not account for image heterogeneity. Given that there\nare limited predictive modeling approaches based on heterogeneous images in\ncancer, the proposed method is expected to provide considerable advantages in\nimage-based early detection and risk prediction.\n"
    },
    {
        "title": "Simulation-Based Segmentation of Blood Vessels in Cerebral 3D OCTA\n  Images",
        "published_time": "2024-03-11T19:14:51Z",
        "abstract": "  Segmentation of blood vessels in murine cerebral 3D OCTA images is\nfoundational for in vivo quantitative analysis of the effects of neurovascular\ndisorders, such as stroke or Alzheimer's, on the vascular network. However, to\naccurately segment blood vessels with state-of-the-art deep learning methods, a\nvast amount of voxel-level annotations is required. Since cerebral 3D OCTA\nimages are typically plagued by artifacts and generally have a low\nsignal-to-noise ratio, acquiring manual annotations poses an especially\ncumbersome and time-consuming task. To alleviate the need for manual\nannotations, we propose utilizing synthetic data to supervise segmentation\nalgorithms. To this end, we extract patches from vessel graphs and transform\nthem into synthetic cerebral 3D OCTA images paired with their matching ground\ntruth labels by simulating the most dominant 3D OCTA artifacts. In extensive\nexperiments, we demonstrate that our approach achieves competitive results,\nenabling annotation-free blood vessel segmentation in cerebral 3D OCTA images.\n"
    },
    {
        "title": "Class Imbalance in Object Detection: An Experimental Diagnosis and Study\n  of Mitigation Strategies",
        "published_time": "2024-03-11T19:06:04Z",
        "abstract": "  Object detection, a pivotal task in computer vision, is frequently hindered\nby dataset imbalances, particularly the under-explored issue of\nforeground-foreground class imbalance. This lack of attention to\nforeground-foreground class imbalance becomes even more pronounced in the\ncontext of single-stage detectors. This study introduces a benchmarking\nframework utilizing the YOLOv5 single-stage detector to address the problem of\nforeground-foreground class imbalance. We crafted a novel 10-class long-tailed\ndataset from the COCO dataset, termed COCO-ZIPF, tailored to reflect common\nreal-world detection scenarios with a limited number of object classes. Against\nthis backdrop, we scrutinized three established techniques: sampling, loss\nweighing, and data augmentation. Our comparative analysis reveals that sampling\nand loss reweighing methods, while shown to be beneficial in two-stage detector\nsettings, do not translate as effectively in improving YOLOv5's performance on\nthe COCO-ZIPF dataset. On the other hand, data augmentation methods,\nspecifically mosaic and mixup, significantly enhance the model's mean Average\nPrecision (mAP), by introducing more variability and complexity into the\ntraining data. (Code available:\nhttps://github.com/craston/object_detection_cib)\n"
    },
    {
        "title": "A slice classification neural network for automated classification of\n  axial PET/CT slices from a multi-centric lymphoma dataset",
        "published_time": "2024-03-11T18:57:45Z",
        "abstract": "  Automated slice classification is clinically relevant since it can be\nincorporated into medical image segmentation workflows as a preprocessing step\nthat would flag slices with a higher probability of containing tumors, thereby\ndirecting physicians attention to the important slices. In this work, we train\na ResNet-18 network to classify axial slices of lymphoma PET/CT images\n(collected from two institutions) depending on whether the slice intercepted a\ntumor (positive slice) in the 3D image or if the slice did not (negative\nslice). Various instances of the network were trained on 2D axial datasets\ncreated in different ways: (i) slice-level split and (ii) patient-level split;\ninputs of different types were used: (i) only PET slices and (ii) concatenated\nPET and CT slices; and different training strategies were employed: (i)\ncenter-aware (CAW) and (ii) center-agnostic (CAG). Model performances were\ncompared using the area under the receiver operating characteristic curve\n(AUROC) and the area under the precision-recall curve (AUPRC), and various\nbinary classification metrics. We observe and describe a performance\noverestimation in the case of slice-level split as compared to the\npatient-level split training. The model trained using patient-level split data\nwith the network input containing only PET slices in the CAG training regime\nwas the best performing/generalizing model on a majority of metrics. Our models\nwere additionally more closely compared using the sensitivity metric on the\npositive slices from their respective test sets.\n"
    },
    {
        "title": "Finer: Investigating and Enhancing Fine-Grained Visual Concept\n  Recognition in Large Vision Language Models",
        "published_time": "2024-02-26T05:43:51Z",
        "abstract": "  Recent advances in instruction-tuned Large Vision-Language Models (LVLMs)\nhave imbued the models with the ability to generate high-level, image-grounded\nexplanations with ease. While such capability is largely attributed to the rich\nworld knowledge contained within the Large Language Models (LLMs), our work\nreveals their shortcomings in fine-grained visual categorization (FGVC) across\nsix different benchmark settings. Most recent state-of-the-art LVLMs like\nLLaVa-1.5, InstructBLIP and GPT-4V not only severely deteriorate in terms of\nclassification performance, e.g., average drop of 65.58 in EM for Stanford Dogs\nfor LLaVA-1.5, but also struggle to generate an accurate explanation with\ndetailed attributes based on the concept that appears within an input image\ndespite their capability to generate holistic image-level descriptions.\nIn-depth analyses show that instruction-tuned LVLMs exhibit modality gap,\nshowing discrepancy when given textual and visual inputs that correspond to the\nsame concept, preventing the image modality from leveraging the rich parametric\nknowledge within the LLMs. In an effort to further the community's endeavor in\nthis direction, we propose a multiple granularity attribute-centric evaluation\nbenchmark, Finer, which aims to establish a ground to evaluate LVLMs'\nfine-grained visual comprehension ability and provide significantly improved\nexplainability.\n"
    },
    {
        "title": "A cascaded deep network for automated tumor detection and segmentation\n  in clinical PET imaging of diffuse large B-cell lymphoma",
        "published_time": "2024-03-11T18:36:55Z",
        "abstract": "  Accurate detection and segmentation of diffuse large B-cell lymphoma (DLBCL)\nfrom PET images has important implications for estimation of total metabolic\ntumor volume, radiomics analysis, surgical intervention and radiotherapy.\nManual segmentation of tumors in whole-body PET images is time-consuming,\nlabor-intensive and operator-dependent. In this work, we develop and validate a\nfast and efficient three-step cascaded deep learning model for automated\ndetection and segmentation of DLBCL tumors from PET images. As compared to a\nsingle end-to-end network for segmentation of tumors in whole-body PET images,\nour three-step model is more effective (improves 3D Dice score from 58.9% to\n78.1%) since each of its specialized modules, namely the slice classifier, the\ntumor detector and the tumor segmentor, can be trained independently to a high\ndegree of skill to carry out a specific task, rather than a single network with\nsuboptimal performance on overall segmentation.\n"
    },
    {
        "title": "Improving deep learning with prior knowledge and cognitive models: A\n  survey on enhancing explainability, adversarial robustness and zero-shot\n  learning",
        "published_time": "2024-03-11T18:11:00Z",
        "abstract": "  We review current and emerging knowledge-informed and brain-inspired\ncognitive systems for realizing adversarial defenses, eXplainable Artificial\nIntelligence (XAI), and zero-shot or few-short learning. Data-driven deep\nlearning models have achieved remarkable performance and demonstrated\ncapabilities surpassing human experts in many applications. Yet, their\ninability to exploit domain knowledge leads to serious performance limitations\nin practical applications. In particular, deep learning systems are exposed to\nadversarial attacks, which can trick them into making glaringly incorrect\ndecisions. Moreover, complex data-driven models typically lack interpretability\nor explainability, i.e., their decisions cannot be understood by human\nsubjects. Furthermore, models are usually trained on standard datasets with a\nclosed-world assumption. Hence, they struggle to generalize to unseen cases\nduring inference in practical open-world environments, thus, raising the zero-\nor few-shot generalization problem. Although many conventional solutions exist,\nexplicit domain knowledge, brain-inspired neural network and cognitive\narchitectures offer powerful new dimensions towards alleviating these problems.\nPrior knowledge is represented in appropriate forms and incorporated in deep\nlearning frameworks to improve performance. Brain-inspired cognition methods\nuse computational models that mimic the human mind to enhance intelligent\nbehavior in artificial agents and autonomous robots. Ultimately, these models\nachieve better explainability, higher adversarial robustness and data-efficient\nlearning, and can, in turn, provide insights for cognitive science and\nneuroscience-that is, to deepen human understanding on how the brain works in\ngeneral, and how it handles these problems.\n"
    },
    {
        "title": "Mapping High-level Semantic Regions in Indoor Environments without\n  Object Recognition",
        "published_time": "2024-03-11T18:09:50Z",
        "abstract": "  Robots require a semantic understanding of their surroundings to operate in\nan efficient and explainable way in human environments. In the literature,\nthere has been an extensive focus on object labeling and exhaustive scene graph\ngeneration; less effort has been focused on the task of purely identifying and\nmapping large semantic regions. The present work proposes a method for semantic\nregion mapping via embodied navigation in indoor environments, generating a\nhigh-level representation of the knowledge of the agent. To enable region\nidentification, the method uses a vision-to-language model to provide scene\ninformation for mapping. By projecting egocentric scene understanding into the\nglobal frame, the proposed method generates a semantic map as a distribution\nover possible region labels at each location. This mapping procedure is paired\nwith a trained navigation policy to enable autonomous map generation. The\nproposed method significantly outperforms a variety of baselines, including an\nobject-based system and a pretrained scene classifier, in experiments in a\nphotorealistic simulator.\n"
    },
    {
        "title": "LISO: Lidar-only Self-Supervised 3D Object Detection",
        "published_time": "2024-03-11T18:02:52Z",
        "abstract": "  3D object detection is one of the most important components in any\nSelf-Driving stack, but current state-of-the-art (SOTA) lidar object detectors\nrequire costly & slow manual annotation of 3D bounding boxes to perform well.\nRecently, several methods emerged to generate pseudo ground truth without human\nsupervision, however, all of these methods have various drawbacks: Some methods\nrequire sensor rigs with full camera coverage and accurate calibration, partly\nsupplemented by an auxiliary optical flow engine. Others require expensive\nhigh-precision localization to find objects that disappeared over multiple\ndrives. We introduce a novel self-supervised method to train SOTA lidar object\ndetection networks which works on unlabeled sequences of lidar point clouds\nonly, which we call trajectory-regularized self-training. It utilizes a SOTA\nself-supervised lidar scene flow network under the hood to generate, track, and\niteratively refine pseudo ground truth. We demonstrate the effectiveness of our\napproach for multiple SOTA object detection networks across multiple real-world\ndatasets. Code will be released.\n"
    },
    {
        "title": "Attention Prompt Tuning: Parameter-efficient Adaptation of Pre-trained\n  Models for Spatiotemporal Modeling",
        "published_time": "2024-03-11T17:59:41Z",
        "abstract": "  In this paper, we introduce Attention Prompt Tuning (APT) - a computationally\nefficient variant of prompt tuning for video-based applications such as action\nrecognition. Prompt tuning approaches involve injecting a set of learnable\nprompts along with data tokens during fine-tuning while keeping the backbone\nfrozen. This approach greatly reduces the number of learnable parameters\ncompared to full tuning. For image-based downstream tasks, normally a couple of\nlearnable prompts achieve results close to those of full tuning. However,\nvideos, which contain more complex spatiotemporal information, require hundreds\nof tunable prompts to achieve reasonably good results. This reduces the\nparameter efficiency observed in images and significantly increases latency and\nthe number of floating-point operations (FLOPs) during inference. To tackle\nthese issues, we directly inject the prompts into the keys and values of the\nnon-local attention mechanism within the transformer block. Additionally, we\nintroduce a novel prompt reparameterization technique to make APT more robust\nagainst hyperparameter selection. The proposed APT approach greatly reduces the\nnumber of FLOPs and latency while achieving a significant performance boost\nover the existing parameter-efficient tuning methods on UCF101, HMDB51, and\nSSv2 datasets for action recognition. The code and pre-trained models are\navailable at https://github.com/wgcban/apt\n"
    },
    {
        "title": "BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed\n  Dual-Branch Diffusion",
        "published_time": "2024-03-11T17:59:31Z",
        "abstract": "  Image inpainting, the process of restoring corrupted images, has seen\nsignificant advancements with the advent of diffusion models (DMs). Despite\nthese advancements, current DM adaptations for inpainting, which involve\nmodifications to the sampling strategy or the development of\ninpainting-specific DMs, frequently suffer from semantic inconsistencies and\nreduced image quality. Addressing these challenges, our work introduces a novel\nparadigm: the division of masked image features and noisy latent into separate\nbranches. This division dramatically diminishes the model's learning load,\nfacilitating a nuanced incorporation of essential masked image information in a\nhierarchical fashion. Herein, we present BrushNet, a novel plug-and-play\ndual-branch model engineered to embed pixel-level masked image features into\nany pre-trained DM, guaranteeing coherent and enhanced image inpainting\noutcomes. Additionally, we introduce BrushData and BrushBench to facilitate\nsegmentation-based inpainting training and performance assessment. Our\nextensive experimental analysis demonstrates BrushNet's superior performance\nover existing models across seven key metrics, including image quality, mask\nregion preservation, and textual coherence.\n"
    },
    {
        "title": "Memory-based Adapters for Online 3D Scene Perception",
        "published_time": "2024-03-11T17:57:41Z",
        "abstract": "  In this paper, we propose a new framework for online 3D scene perception.\nConventional 3D scene perception methods are offline, i.e., take an already\nreconstructed 3D scene geometry as input, which is not applicable in robotic\napplications where the input data is streaming RGB-D videos rather than a\ncomplete 3D scene reconstructed from pre-collected RGB-D videos. To deal with\nonline 3D scene perception tasks where data collection and perception should be\nperformed simultaneously, the model should be able to process 3D scenes frame\nby frame and make use of the temporal information. To this end, we propose an\nadapter-based plug-and-play module for the backbone of 3D scene perception\nmodel, which constructs memory to cache and aggregate the extracted RGB-D\nfeatures to empower offline models with temporal learning ability.\nSpecifically, we propose a queued memory mechanism to cache the supporting\npoint cloud and image features. Then we devise aggregation modules which\ndirectly perform on the memory and pass temporal information to current frame.\nWe further propose 3D-to-2D adapter to enhance image features with strong\nglobal context. Our adapters can be easily inserted into mainstream offline\narchitectures of different tasks and significantly boost their performance on\nonline tasks. Extensive experiments on ScanNet and SceneNN datasets demonstrate\nour approach achieves leading performance on three 3D scene perception tasks\ncompared with state-of-the-art online methods by simply finetuning existing\noffline models, without any model and task-specific designs.\n\\href{https://xuxw98.github.io/Online3D/}{Project page}.\n"
    },
    {
        "title": "Bayesian Diffusion Models for 3D Shape Reconstruction",
        "published_time": "2024-03-11T17:55:53Z",
        "abstract": "  We present Bayesian Diffusion Models (BDM), a prediction algorithm that\nperforms effective Bayesian inference by tightly coupling the top-down (prior)\ninformation with the bottom-up (data-driven) procedure via joint diffusion\nprocesses. We show the effectiveness of BDM on the 3D shape reconstruction\ntask. Compared to prototypical deep learning data-driven approaches trained on\npaired (supervised) data-labels (e.g. image-point clouds) datasets, our BDM\nbrings in rich prior information from standalone labels (e.g. point clouds) to\nimprove the bottom-up 3D reconstruction. As opposed to the standard Bayesian\nframeworks where explicit prior and likelihood are required for the inference,\nBDM performs seamless information fusion via coupled diffusion processes with\nlearned gradient computation networks. The specialty of our BDM lies in its\ncapability to engage the active and effective information exchange and fusion\nof the top-down and bottom-up processes where each itself is a diffusion\nprocess. We demonstrate state-of-the-art results on both synthetic and\nreal-world benchmarks for 3D shape reconstruction.\n"
    },
    {
        "title": "Anatomically-Controllable Medical Image Generation with\n  Segmentation-Guided Diffusion Models",
        "published_time": "2024-02-07T19:35:09Z",
        "abstract": "  Diffusion models have enabled remarkably high-quality medical image\ngeneration, yet it is challenging to enforce anatomical constraints in\ngenerated images. This hampers many useful applications, including\npre-registered image generation, counterfactual scenarios, and others. To this\nend, we propose a diffusion model-based method that supports\nanatomically-controllable medical image generation, by following a multi-class\nanatomical segmentation mask at each sampling step. We additionally introduce a\nrandom mask ablation training algorithm to enable conditioning on a selected\ncombination of anatomical constraints while allowing flexibility in other\nanatomical areas. We compare our model (\"Seg-Diff\") to existing methods on\nbreast MRI and abdominal/neck-to-pelvis CT datasets with a wide range of\nanatomical objects. Results show that it reaches a new state-of-the-art in the\nfaithfulness of generated images to input anatomical masks on both datasets,\nand is on par for general anatomical realism. Finally, our model also enjoys\nthe extra benefit of being able to adjust the anatomical similarity of\ngenerated images to real images of choice through interpolation in its latent\nspace.\n"
    },
    {
        "title": "Explainable Transformer Prototypes for Medical Diagnoses",
        "published_time": "2024-03-11T17:46:21Z",
        "abstract": "  Deployments of artificial intelligence in medical diagnostics mandate not\njust accuracy and efficacy but also trust, emphasizing the need for\nexplainability in machine decisions. The recent trend in automated medical\nimage diagnostics leans towards the deployment of Transformer-based\narchitectures, credited to their impressive capabilities. Since the\nself-attention feature of transformers contributes towards identifying crucial\nregions during the classification process, they enhance the trustability of the\nmethods. However, the complex intricacies of these attention mechanisms may\nfall short of effectively pinpointing the regions of interest directly\ninfluencing AI decisions. Our research endeavors to innovate a unique attention\nblock that underscores the correlation between 'regions' rather than 'pixels'.\nTo address this challenge, we introduce an innovative system grounded in\nprototype learning, featuring an advanced self-attention mechanism that goes\nbeyond conventional ad-hoc visual explanation techniques by offering\ncomprehensible visual insights. A combined quantitative and qualitative\nmethodological approach was used to demonstrate the effectiveness of the\nproposed method on the large-scale NIH chest X-ray dataset. Experimental\nresults showed that our proposed method offers a promising direction for\nexplainability, which can lead to the development of more trustable systems,\nwhich can facilitate easier and rapid adoption of such technology into routine\nclinics. The code is available at www.github.com/NUBagcilab/r2r_proto.\n"
    },
    {
        "title": "Optimizing Latent Graph Representations of Surgical Scenes for Zero-Shot\n  Domain Transfer",
        "published_time": "2024-03-11T17:36:11Z",
        "abstract": "  Purpose: Advances in deep learning have resulted in effective models for\nsurgical video analysis; however, these models often fail to generalize across\nmedical centers due to domain shift caused by variations in surgical workflow,\ncamera setups, and patient demographics. Recently, object-centric learning has\nemerged as a promising approach for improved surgical scene understanding,\ncapturing and disentangling visual and semantic properties of surgical tools\nand anatomy to improve downstream task performance. In this work, we conduct a\nmulti-centric performance benchmark of object-centric approaches, focusing on\nCritical View of Safety assessment in laparoscopic cholecystectomy, then\npropose an improved approach for unseen domain generalization.\n  Methods: We evaluate four object-centric approaches for domain\ngeneralization, establishing baseline performance. Next, leveraging the\ndisentangled nature of object-centric representations, we dissect one of these\nmethods through a series of ablations (e.g. ignoring either visual or semantic\nfeatures for downstream classification). Finally, based on the results of these\nablations, we develop an optimized method specifically tailored for domain\ngeneralization, LG-DG, that includes a novel disentanglement loss function.\n  Results: Our optimized approach, LG-DG, achieves an improvement of 9.28% over\nthe best baseline approach. More broadly, we show that object-centric\napproaches are highly effective for domain generalization thanks to their\nmodular approach to representation learning.\n  Conclusion: We investigate the use of object-centric methods for unseen\ndomain generalization, identify method-agnostic factors critical for\nperformance, and present an optimized approach that substantially outperforms\nexisting methods.\n"
    },
    {
        "title": "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with\n  Auto-Generated Data",
        "published_time": "2024-03-11T17:35:33Z",
        "abstract": "  Recent text-to-image (T2I) generation models have demonstrated impressive\ncapabilities in creating images from text descriptions. However, these T2I\ngeneration models often fall short of generating images that precisely match\nthe details of the text inputs, such as incorrect spatial relationship or\nmissing objects. In this paper, we introduce SELMA: Skill-Specific Expert\nLearning and Merging with Auto-Generated Data, a novel paradigm to improve the\nfaithfulness of T2I models by fine-tuning models on automatically generated,\nmulti-skill image-text datasets, with skill-specific expert learning and\nmerging. First, SELMA leverages an LLM's in-context learning capability to\ngenerate multiple datasets of text prompts that can teach different skills, and\nthen generates the images with a T2I model based on the prompts. Next, SELMA\nadapts the T2I model to the new skills by learning multiple single-skill LoRA\n(low-rank adaptation) experts followed by expert merging. Our independent\nexpert fine-tuning specializes multiple models for different skills, and expert\nmerging helps build a joint multi-skill T2I model that can generate faithful\nimages given diverse text prompts, while mitigating the knowledge conflict from\ndifferent datasets. We empirically demonstrate that SELMA significantly\nimproves the semantic alignment and text faithfulness of state-of-the-art T2I\ndiffusion models on multiple benchmarks (+2.1% on TIFA and +6.9% on DSG), human\npreference metrics (PickScore, ImageReward, and HPS), as well as human\nevaluation. Moreover, fine-tuning with image-text pairs auto-collected via\nSELMA shows comparable performance to fine-tuning with ground truth data.\nLastly, we show that fine-tuning with images from a weaker T2I model can help\nimprove the generation quality of a stronger T2I model, suggesting promising\nweak-to-strong generalization in T2I models.\n"
    },
    {
        "title": "Applicability of oculomics for individual risk prediction: Repeatability\n  and robustness of retinal Fractal Dimension using DART and AutoMorph",
        "published_time": "2024-03-11T17:34:51Z",
        "abstract": "  Purpose: To investigate whether Fractal Dimension (FD)-based oculomics could\nbe used for individual risk prediction by evaluating repeatability and\nrobustness. Methods: We used two datasets: Caledonia, healthy adults imaged\nmultiple times in quick succession for research (26 subjects, 39 eyes, 377\ncolour fundus images), and GRAPE, glaucoma patients with baseline and follow-up\nvisits (106 subjects, 196 eyes, 392 images). Mean follow-up time was 18.3\nmonths in GRAPE, thus it provides a pessimistic lower-bound as vasculature\ncould change. FD was computed with DART and AutoMorph. Image quality was\nassessed with QuickQual, but no images were initially excluded. Pearson,\nSpearman, and Intraclass Correlation (ICC) were used for population-level\nrepeatability. For individual-level repeatability, we introduce measurement\nnoise parameter {\\lambda} which is within-eye Standard Deviation (SD) of FD\nmeasurements in units of between-eyes SD. Results: In Caledonia, ICC was 0.8153\nfor DART and 0.5779 for AutoMorph, Pearson/Spearman correlation (first and last\nimage) 0.7857/0.7824 for DART, and 0.3933/0.6253 for AutoMorph. In GRAPE,\nPearson/Spearman correlation (first and next visit) was 0.7479/0.7474 for DART,\nand 0.7109/0.7208 for AutoMorph (all p<0.0001). Median {\\lambda} in Caledonia\nwithout exclusions was 3.55\\% for DART and 12.65\\% for AutoMorph, and improved\nto up to 1.67\\% and 6.64\\% with quality-based exclusions, respectively. Quality\nexclusions primarily mitigated large outliers. Worst quality in an eye\ncorrelated strongly with {\\lambda} (Pearson 0.5350-0.7550, depending on dataset\nand method, all p<0.0001). Conclusions: Repeatability was sufficient for\nindividual-level predictions in heterogeneous populations. DART performed\nbetter on all metrics and might be able to detect small, longitudinal changes,\nhighlighting the potential of robust methods.\n"
    },
    {
        "title": "Advancing Generalizable Remote Physiological Measurement through the\n  Integration of Explicit and Implicit Prior Knowledge",
        "published_time": "2024-03-11T17:33:25Z",
        "abstract": "  Remote photoplethysmography (rPPG) is a promising technology that captures\nphysiological signals from face videos, with potential applications in medical\nhealth, emotional computing, and biosecurity recognition. The demand for rPPG\ntasks has expanded from demonstrating good performance on intra-dataset testing\nto cross-dataset testing (i.e., domain generalization). However, most existing\nmethods have overlooked the prior knowledge of rPPG, resulting in poor\ngeneralization ability. In this paper, we propose a novel framework that\nsimultaneously utilizes explicit and implicit prior knowledge in the rPPG task.\nSpecifically, we systematically analyze the causes of noise sources (e.g.,\ndifferent camera, lighting, skin types, and movement) across different domains\nand incorporate these prior knowledge into the network. Additionally, we\nleverage a two-branch network to disentangle the physiological feature\ndistribution from noises through implicit label correlation. Our extensive\nexperiments demonstrate that the proposed method not only outperforms\nstate-of-the-art methods on RGB cross-dataset evaluation but also generalizes\nwell from RGB datasets to NIR datasets. The code is available at\nhttps://github.com/keke-nice/Greip.\n"
    },
    {
        "title": "Split to Merge: Unifying Separated Modalities for Unsupervised Domain\n  Adaptation",
        "published_time": "2024-03-11T17:33:12Z",
        "abstract": "  Large vision-language models (VLMs) like CLIP have demonstrated good\nzero-shot learning performance in the unsupervised domain adaptation task. Yet,\nmost transfer approaches for VLMs focus on either the language or visual\nbranches, overlooking the nuanced interplay between both modalities. In this\nwork, we introduce a Unified Modality Separation (UniMoS) framework for\nunsupervised domain adaptation. Leveraging insights from modality gap studies,\nwe craft a nimble modality separation network that distinctly disentangles\nCLIP's features into language-associated and vision-associated components. Our\nproposed Modality-Ensemble Training (MET) method fosters the exchange of\nmodality-agnostic information while maintaining modality-specific nuances. We\nalign features across domains using a modality discriminator. Comprehensive\nevaluations on three benchmarks reveal our approach sets a new state-of-the-art\nwith minimal computational costs. Code: https://github.com/TL-UESTC/UniMoS\n"
    },
    {
        "title": "FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization",
        "published_time": "2024-03-11T17:00:27Z",
        "abstract": "  3D Gaussian splatting has achieved very impressive performance in real-time\nnovel view synthesis. However, it often suffers from over-reconstruction during\nGaussian densification where high-variance image regions are covered by a few\nlarge Gaussians only, leading to blur and artifacts in the rendered images. We\ndesign a progressive frequency regularization (FreGS) technique to tackle the\nover-reconstruction issue within the frequency space. Specifically, FreGS\nperforms coarse-to-fine Gaussian densification by exploiting low-to-high\nfrequency components that can be easily extracted with low-pass and high-pass\nfilters in the Fourier space. By minimizing the discrepancy between the\nfrequency spectrum of the rendered image and the corresponding ground truth, it\nachieves high-quality Gaussian densification and alleviates the\nover-reconstruction of Gaussian splatting effectively. Experiments over\nmultiple widely adopted benchmarks (e.g., Mip-NeRF360, Tanks-and-Temples and\nDeep Blending) show that FreGS achieves superior novel view synthesis and\noutperforms the state-of-the-art consistently.\n"
    },
    {
        "title": "FocusCLIP: Multimodal Subject-Level Guidance for Zero-Shot Transfer in\n  Human-Centric Tasks",
        "published_time": "2024-03-11T16:56:37Z",
        "abstract": "  We propose FocusCLIP, integrating subject-level guidance--a specialized\nmechanism for target-specific supervision--into the CLIP framework for improved\nzero-shot transfer on human-centric tasks. Our novel contributions enhance CLIP\non both the vision and text sides. On the vision side, we incorporate ROI\nheatmaps emulating human visual attention mechanisms to emphasize\nsubject-relevant image regions. On the text side, we introduce human pose\ndescriptions to provide rich contextual information. For human-centric tasks,\nFocusCLIP is trained with images from the MPII Human Pose dataset. The proposed\napproach surpassed CLIP by an average of 8.61% across five previously unseen\ndatasets covering three human-centric tasks. FocusCLIP achieved an average\naccuracy of 33.65% compared to 25.04% by CLIP. We observed a 3.98% improvement\nin activity recognition, a 14.78% improvement in age classification, and a\n7.06% improvement in emotion recognition. Moreover, using our proposed\nsingle-shot LLM prompting strategy, we release a high-quality MPII Pose\nDescriptions dataset to encourage further research in multimodal learning for\nhuman-centric tasks. Furthermore, we also demonstrate the effectiveness of our\nsubject-level supervision on non-human-centric tasks. FocusCLIP shows a 2.47%\nimprovement over CLIP in zero-shot bird classification using the CUB dataset.\nOur findings emphasize the potential of integrating subject-level guidance with\ngeneral pretraining methods for enhanced downstream performance.\n"
    },
    {
        "title": "Deep adaptative spectral zoom for improved remote heart rate estimation",
        "published_time": "2024-03-11T16:55:19Z",
        "abstract": "  Recent advances in remote heart rate measurement, motivated by data-driven\napproaches, have notably enhanced accuracy. However, these improvements\nprimarily focus on recovering the rPPG signal, overlooking the implicit\nchallenges of estimating the heart rate (HR) from the derived signal. While\nmany methods employ the Fast Fourier Transform (FFT) for HR estimation, the\nperformance of the FFT is inherently affected by a limited frequency\nresolution. In contrast, the Chirp-Z Transform (CZT), a generalization form of\nFFT, can refine the spectrum to the narrow-band range of interest for heart\nrate, providing improved frequential resolution and, consequently, more\naccurate estimation. This paper presents the advantages of employing the CZT\nfor remote HR estimation and introduces a novel data-driven adaptive CZT\nestimator. The objective of our proposed model is to tailor the CZT to match\nthe characteristics of each specific dataset sensor, facilitating a more\noptimal and accurate estimation of HR from the rPPG signal without compromising\ngeneralization across diverse datasets. This is achieved through a Sparse\nMatrix Optimization (SMO). We validate the effectiveness of our model through\nexhaustive evaluations on three publicly available datasets UCLA-rPPG, PURE,\nand UBFC-rPPG employing both intra- and cross-database performance metrics. The\nresults reveal outstanding heart rate estimation capabilities, establishing the\nproposed approach as a robust and versatile estimator for any rPPG method.\n"
    },
    {
        "title": "GRITv2: Efficient and Light-weight Social Relation Recognition",
        "published_time": "2024-03-11T16:49:59Z",
        "abstract": "  Our research focuses on the analysis and improvement of the Graph-based\nRelation Inference Transformer (GRIT), which serves as an important benchmark\nin the field. We conduct a comprehensive ablation study using the PISC-fine\ndataset, to find and explore improvement in efficiency and performance of\nGRITv2. Our research has provided a new state-of-the-art relation recognition\nmodel on the PISC relation dataset. We introduce several features in the GRIT\nmodel and analyse our new benchmarks in two versions: GRITv2-L (large) and\nGRITv2-S (small). Our proposed GRITv2-L surpasses existing methods on relation\nrecognition and the GRITv2-S is within 2% performance gap of GRITv2-L, which\nhas only 0.0625x the model size and parameters of GRITv2-L. Furthermore, we\nalso address the need for model compression, an area crucial for deploying\nefficient models on resource-constrained platforms. By applying quantization\ntechniques, we efficiently reduced the GRITv2-S size to 22MB and deployed it on\nthe flagship OnePlus 12 mobile which still surpasses the PISC-fine benchmarks\nin performance, highlighting the practical viability and improved efficiency of\nour model on mobile devices.\n"
    },
    {
        "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
        "published_time": "2024-03-11T16:48:25Z",
        "abstract": "  End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}\n"
    },
    {
        "title": "A Holistic Framework Towards Vision-based Traffic Signal Control with\n  Microscopic Simulation",
        "published_time": "2024-03-11T16:42:29Z",
        "abstract": "  Traffic signal control (TSC) is crucial for reducing traffic congestion that\nleads to smoother traffic flow, reduced idling time, and mitigated CO2\nemissions. In this study, we explore the computer vision approach for TSC that\nmodulates on-road traffic flows through visual observation. Unlike traditional\nfeature-based approaches, vision-based methods depend much less on heuristics\nand predefined features, bringing promising potentials for end-to-end learning\nand optimization of traffic signals. Thus, we introduce a holistic traffic\nsimulation framework called TrafficDojo towards vision-based TSC and its\nbenchmarking by integrating the microscopic traffic flow provided in SUMO into\nthe driving simulator MetaDrive. This proposed framework offers a versatile\ntraffic environment for in-depth analysis and comprehensive evaluation of\ntraffic signal controllers across diverse traffic conditions and scenarios. We\nestablish and compare baseline algorithms including both traditional and\nReinforecment Learning (RL) approaches. This work sheds insights into the\ndesign and development of vision-based TSC approaches and open up new research\nopportunities. All the code and baselines will be made publicly available.\n"
    },
    {
        "title": "BLO-SAM: Bi-level Optimization Based Overfitting-Preventing Finetuning\n  of SAM",
        "published_time": "2024-02-26T06:36:32Z",
        "abstract": "  The Segment Anything Model (SAM), a foundation model pretrained on millions\nof images and segmentation masks, has significantly advanced semantic\nsegmentation, a fundamental task in computer vision. Despite its strengths, SAM\nencounters two major challenges. Firstly, it struggles with segmenting specific\nobjects autonomously, as it relies on users to manually input prompts like\npoints or bounding boxes to identify targeted objects. Secondly, SAM faces\nchallenges in excelling at specific downstream tasks, like medical imaging, due\nto a disparity between the distribution of its pretraining data, which\npredominantly consists of general-domain images, and the data used in\ndownstream tasks. Current solutions to these problems, which involve finetuning\nSAM, often lead to overfitting, a notable issue in scenarios with very limited\ndata, like in medical imaging. To overcome these limitations, we introduce\nBLO-SAM, which finetunes SAM based on bi-level optimization (BLO). Our approach\nallows for automatic image segmentation without the need for manual prompts, by\noptimizing a learnable prompt embedding. Furthermore, it significantly reduces\nthe risk of overfitting by training the model's weight parameters and the\nprompt embedding on two separate subsets of the training dataset, each at a\ndifferent level of optimization. We apply BLO-SAM to diverse semantic\nsegmentation tasks in general and medical domains. The results demonstrate\nBLO-SAM's superior performance over various state-of-the-art image semantic\nsegmentation methods.\n"
    },
    {
        "title": "SiLVR: Scalable Lidar-Visual Reconstruction with Neural Radiance Fields\n  for Robotic Inspection",
        "published_time": "2024-03-11T16:31:25Z",
        "abstract": "  We present a neural-field-based large-scale reconstruction system that fuses\nlidar and vision data to generate high-quality reconstructions that are\ngeometrically accurate and capture photo-realistic textures. This system adapts\nthe state-of-the-art neural radiance field (NeRF) representation to also\nincorporate lidar data which adds strong geometric constraints on the depth and\nsurface normals. We exploit the trajectory from a real-time lidar SLAM system\nto bootstrap a Structure-from-Motion (SfM) procedure to both significantly\nreduce the computation time and to provide metric scale which is crucial for\nlidar depth loss. We use submapping to scale the system to large-scale\nenvironments captured over long trajectories. We demonstrate the reconstruction\nsystem with data from a multi-camera, lidar sensor suite onboard a legged\nrobot, hand-held while scanning building scenes for 600 metres, and onboard an\naerial robot surveying a multi-storey mock disaster site-building. Website:\nhttps://ori-drs.github.io/projects/silvr/\n"
    },
    {
        "title": "COOD: Combined out-of-distribution detection using multiple measures for\n  anomaly & novel class detection in large-scale hierarchical classification",
        "published_time": "2024-03-11T16:26:35Z",
        "abstract": "  High-performing out-of-distribution (OOD) detection, both anomaly and novel\nclass, is an important prerequisite for the practical use of classification\nmodels. In this paper, we focus on the species recognition task in images\nconcerned with large databases, a large number of fine-grained hierarchical\nclasses, severe class imbalance, and varying image quality. We propose a\nframework for combining individual OOD measures into one combined OOD (COOD)\nmeasure using a supervised model. The individual measures are several existing\nstate-of-the-art measures and several novel OOD measures developed with novel\nclass detection and hierarchical class structure in mind. COOD was extensively\nevaluated on three large-scale (500k+ images) biodiversity datasets in the\ncontext of anomaly and novel class detection. We show that COOD outperforms\nindividual, including state-of-the-art, OOD measures by a large margin in terms\nof TPR@1% FPR in the majority of experiments, e.g., improving detecting\nImageNet images (OOD) from 54.3% to 85.4% for the iNaturalist 2018 dataset.\nSHAP (feature contribution) analysis shows that different individual OOD\nmeasures are essential for various tasks, indicating that multiple OOD measures\nand combinations are needed to generalize. Additionally, we show that\nexplicitly considering ID images that are incorrectly classified for the\noriginal (species) recognition task is important for constructing\nhigh-performing OOD detection methods and for practical applicability. The\nframework can easily be extended or adapted to other tasks and media\nmodalities.\n"
    },
    {
        "title": "Learning with Noisy Foundation Models",
        "published_time": "2024-03-11T16:22:41Z",
        "abstract": "  Foundation models are usually pre-trained on large-scale datasets and then\nadapted to downstream tasks through tuning. However, the large-scale\npre-training datasets, often inaccessible or too expensive to handle, can\ncontain label noise that may adversely affect the generalization of the model\nand pose unexpected risks. This paper stands out as the first work to\ncomprehensively understand and analyze the nature of noise in pre-training\ndatasets and then effectively mitigate its impacts on downstream tasks.\nSpecifically, through extensive experiments of fully-supervised and image-text\ncontrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M\ndatasets, we demonstrate that, while slight noise in pre-training can benefit\nin-domain (ID) performance, where the training and testing data share a similar\ndistribution, it always deteriorates out-of-domain (OOD) performance, where\ntraining and testing distributions are significantly different. These\nobservations are agnostic to scales of pre-training datasets, pre-training\nnoise types, model architectures, pre-training objectives, downstream tuning\nmethods, and downstream applications. We empirically ascertain that the reason\nbehind this is that the pre-training noise shapes the feature space\ndifferently. We then propose a tuning method (NMTune) to affine the feature\nspace to mitigate the malignant effect of noise and improve generalization,\nwhich is applicable in both parameter-efficient and black-box tuning manners.\nWe additionally conduct extensive experiments on popular vision and language\nmodels, including APIs, which are supervised and self-supervised pre-trained on\nrealistic noisy data for evaluation. Our analysis and results demonstrate the\nimportance of this novel and fundamental research direction, which we term as\nNoisy Model Learning.\n"
    },
    {
        "title": "Real-Time Simulated Avatar from Head-Mounted Sensors",
        "published_time": "2024-03-11T16:15:51Z",
        "abstract": "  We present SimXR, a method for controlling a simulated avatar from\ninformation (headset pose and cameras) obtained from AR / VR headsets. Due to\nthe challenging viewpoint of head-mounted cameras, the human body is often\nclipped out of view, making traditional image-based egocentric pose estimation\nchallenging. On the other hand, headset poses provide valuable information\nabout overall body motion, but lack fine-grained details about the hands and\nfeet. To synergize headset poses with cameras, we control a humanoid to track\nheadset movement while analyzing input images to decide body movement. When\nbody parts are seen, the movements of hands and feet will be guided by the\nimages; when unseen, the laws of physics guide the controller to generate\nplausible motion. We design an end-to-end method that does not rely on any\nintermediate representations and learns to directly map from images and headset\nposes to humanoid control signals. To train our method, we also propose a\nlarge-scale synthetic dataset created using camera configurations compatible\nwith a commercially available VR headset (Quest 2) and show promising results\non real-world captures. To demonstrate the applicability of our framework, we\nalso test it on an AR headset with a forward-facing camera.\n"
    },
    {
        "title": "A Geospatial Approach to Predicting Desert Locust Breeding Grounds in\n  Africa",
        "published_time": "2024-03-11T16:13:58Z",
        "abstract": "  Desert locust swarms present a major threat to agriculture and food security.\nAddressing this challenge, our study develops an operationally-ready model for\npredicting locust breeding grounds, which has the potential to enhance early\nwarning systems and targeted control measures. We curated a dataset from the\nUnited Nations Food and Agriculture Organization's (UN-FAO) locust observation\nrecords and analyzed it using two types of spatio-temporal input features:\nremotely-sensed environmental and climate data as well as multi-spectral earth\nobservation images. Our approach employed custom deep learning models\n(three-dimensional and LSTM-based recurrent convolutional networks), along with\nthe geospatial foundational model Prithvi recently released by Jakubik et al.,\n2023. These models notably outperformed existing baselines, with the\nPrithvi-based model, fine-tuned on multi-spectral images from NASA's Harmonized\nLandsat and Sentinel-2 (HLS) dataset, achieving the highest accuracy, F1 and\nROC-AUC scores (83.03%, 81.53% and 87.69%, respectively). A significant finding\nfrom our research is that multi-spectral earth observation images alone are\nsufficient for effective locust breeding ground prediction without the need to\nexplicitly incorporate climatic or environmental features.\n"
    },
    {
        "title": "DiaLoc: An Iterative Approach to Embodied Dialog Localization",
        "published_time": "2024-03-11T16:03:43Z",
        "abstract": "  Multimodal learning has advanced the performance for many vision-language\ntasks. However, most existing works in embodied dialog research focus on\nnavigation and leave the localization task understudied. The few existing\ndialog-based localization approaches assume the availability of entire dialog\nprior to localizaiton, which is impractical for deployed dialog-based\nlocalization. In this paper, we propose DiaLoc, a new dialog-based localization\nframework which aligns with a real human operator behavior. Specifically, we\nproduce an iterative refinement of location predictions which can visualize\ncurrent pose believes after each dialog turn. DiaLoc effectively utilizes the\nmultimodal data for multi-shot localization, where a fusion encoder fuses\nvision and dialog information iteratively. We achieve state-of-the-art results\non embodied dialog-based localization task, in single-shot (+7.08% in\nAcc5@valUnseen) and multi-shot settings (+10.85% in Acc5@valUnseen). DiaLoc\nnarrows the gap between simulation and real-world applications, opening doors\nfor future research on collaborative localization and navigation.\n"
    },
    {
        "title": "DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video\n  Generation",
        "published_time": "2024-03-11T16:03:35Z",
        "abstract": "  World models have demonstrated superiority in autonomous driving,\nparticularly in the generation of multi-view driving videos. However,\nsignificant challenges still exist in generating customized driving videos. In\nthis paper, we propose DriveDreamer-2, which builds upon the framework of\nDriveDreamer and incorporates a Large Language Model (LLM) to generate\nuser-defined driving videos. Specifically, an LLM interface is initially\nincorporated to convert a user's query into agent trajectories. Subsequently, a\nHDMap, adhering to traffic regulations, is generated based on the trajectories.\nUltimately, we propose the Unified Multi-View Model to enhance temporal and\nspatial coherence in the generated driving videos. DriveDreamer-2 is the first\nworld model to generate customized driving videos, it can generate uncommon\ndriving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.\nBesides, experimental results demonstrate that the generated videos enhance the\ntraining of driving perception methods (e.g., 3D detection and tracking).\nFurthermore, video generation quality of DriveDreamer-2 surpasses other\nstate-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7,\nrepresenting relative improvements of 30% and 50%.\n"
    },
    {
        "title": "Stochastic Cortical Self-Reconstruction",
        "published_time": "2024-03-11T15:59:35Z",
        "abstract": "  Magnetic resonance imaging (MRI) is critical for diagnosing neurodegenerative\ndiseases, yet accurately assessing mild cortical atrophy remains a challenge\ndue to its subtlety. Automated cortex reconstruction, paired with healthy\nreference ranges, aids in pinpointing pathological atrophy, yet their\ngeneralization is limited by biases from image acquisition and processing. We\nintroduce the concept of stochastic cortical self-reconstruction (SCSR) that\ncreates a subject-specific healthy reference by taking MRI-derived thicknesses\nas input and, therefore, implicitly accounting for potential confounders. SCSR\nrandomly corrupts parts of the cortex and self-reconstructs them from the\nremaining information. Trained exclusively on healthy individuals, repeated\nself-reconstruction generates a stochastic reference cortex for assessing\ndeviations from the norm. We present three implementations of this concept:\nXGBoost applied on parcels, and two autoencoders on vertex level -- one based\non a multilayer perceptron and the other using a spherical U-Net. These models\nwere trained on healthy subjects from the UK Biobank and subsequently evaluated\nacross four public Alzheimer's datasets. Finally, we deploy the model on\nclinical in-house data, where deviation maps' high spatial resolution aids in\ndiscriminating between four types of dementia.\n"
    },
    {
        "title": "DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain\n  Generalization in Federated Learning",
        "published_time": "2024-03-11T15:58:15Z",
        "abstract": "  Federated learning (FL) has emerged as a powerful paradigm for learning from\ndecentralized data, and federated domain generalization further considers the\ntest dataset (target domain) is absent from the decentralized training data\n(source domains). However, most existing FL methods assume that domain labels\nare provided during training, and their evaluation imposes explicit constraints\non the number of domains, which must strictly match the number of clients.\nBecause of the underutilization of numerous edge devices and additional\ncross-client domain annotations in the real world, such restrictions may be\nimpractical and involve potential privacy leaks. In this paper, we propose an\nefficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a\nmethod that tackles the above restrictions by learning adaptive prompts for\ndomain generalization in a distributed manner. Specifically, we first design\ntwo types of prompts, i.e., global prompt to capture general knowledge across\nall clients and domain prompts to capture domain-specific knowledge. They\neliminate the restriction on the one-to-one mapping between source domains and\nlocal clients. Furthermore, a dynamic query metric is introduced to\nautomatically search the suitable domain label for each sample, which includes\ntwo-substep text-image alignments based on prompt tuning without\nlabor-intensive annotation. Extensive experiments on multiple datasets\ndemonstrate that our DiPrompT achieves superior domain generalization\nperformance over state-of-the-art FL methods when domain labels are not\nprovided, and even outperforms many centralized learning methods using domain\nlabels.\n"
    },
    {
        "title": "Medical Image Synthesis via Fine-Grained Image-Text Alignment and\n  Anatomy-Pathology Prompting",
        "published_time": "2024-03-11T15:56:17Z",
        "abstract": "  Data scarcity and privacy concerns limit the availability of high-quality\nmedical images for public use, which can be mitigated through medical image\nsynthesis. However, current medical image synthesis methods often struggle to\naccurately capture the complexity of detailed anatomical structures and\npathological conditions. To address these challenges, we propose a novel\nmedical image synthesis model that leverages fine-grained image-text alignment\nand anatomy-pathology prompts to generate highly detailed and accurate\nsynthetic medical images. Our method integrates advanced natural language\nprocessing techniques with image generative modeling, enabling precise\nalignment between descriptive text prompts and the synthesized images'\nanatomical and pathological details. The proposed approach consists of two key\ncomponents: an anatomy-pathology prompting module and a fine-grained\nalignment-based synthesis module. The anatomy-pathology prompting module\nautomatically generates descriptive prompts for high-quality medical images. To\nfurther synthesize high-quality medical images from the generated prompts, the\nfine-grained alignment-based synthesis module pre-defines a visual codebook for\nthe radiology dataset and performs fine-grained alignment between the codebook\nand generated prompts to obtain key patches as visual clues, facilitating\naccurate image synthesis. We validate the superiority of our method through\nexperiments on public chest X-ray datasets and demonstrate that our synthetic\nimages preserve accurate semantic information, making them valuable for various\nmedical applications.\n"
    },
    {
        "title": "HDRTransDC: High Dynamic Range Image Reconstruction with Transformer\n  Deformation Convolution",
        "published_time": "2024-03-11T15:48:17Z",
        "abstract": "  High Dynamic Range (HDR) imaging aims to generate an artifact-free HDR image\nwith realistic details by fusing multi-exposure Low Dynamic Range (LDR) images.\nCaused by large motion and severe under-/over-exposure among input LDR images,\nHDR imaging suffers from ghosting artifacts and fusion distortions. To address\nthese critical issues, we propose an HDR Transformer Deformation Convolution\n(HDRTransDC) network to generate high-quality HDR images, which consists of the\nTransformer Deformable Convolution Alignment Module (TDCAM) and the Dynamic\nWeight Fusion Block (DWFB). To solve the ghosting artifacts, the proposed TDCAM\nextracts long-distance content similar to the reference feature in the entire\nnon-reference features, which can accurately remove misalignment and fill the\ncontent occluded by moving objects. For the purpose of eliminating fusion\ndistortions, we propose DWFB to spatially adaptively select useful information\nacross frames to effectively fuse multi-exposed features. Extensive experiments\nshow that our method quantitatively and qualitatively achieves state-of-the-art\nperformance.\n"
    },
    {
        "title": "LeOCLR: Leveraging Original Images for Contrastive Learning of Visual\n  Representations",
        "published_time": "2024-03-11T15:33:32Z",
        "abstract": "  Contrastive instance discrimination outperforms supervised learning in\ndownstream tasks like image classification and object detection. However, this\napproach heavily relies on data augmentation during representation learning,\nwhich may result in inferior results if not properly implemented. Random\ncropping followed by resizing is a common form of data augmentation used in\ncontrastive learning, but it can lead to degraded representation learning if\nthe two random crops contain distinct semantic content. To address this issue,\nthis paper introduces LeOCLR (Leveraging Original Images for Contrastive\nLearning of Visual Representations), a framework that employs a new instance\ndiscrimination approach and an adapted loss function that ensures the shared\nregion between positive pairs is semantically correct. The experimental results\nshow that our approach consistently improves representation learning across\ndifferent datasets compared to baseline models. For example, our approach\noutperforms MoCo-v2 by 5.1% on ImageNet-1K in linear evaluation and several\nother methods on transfer learning tasks.\n"
    },
    {
        "title": "Deep Learning Approaches for Human Action Recognition in Video Data",
        "published_time": "2024-03-11T15:31:25Z",
        "abstract": "  Human action recognition in videos is a critical task with significant\nimplications for numerous applications, including surveillance, sports\nanalytics, and healthcare. The challenge lies in creating models that are both\nprecise in their recognition capabilities and efficient enough for practical\nuse. This study conducts an in-depth analysis of various deep learning models\nto address this challenge. Utilizing a subset of the UCF101 Videos dataset, we\nfocus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks\n(RNNs), and Two-Stream ConvNets. The research reveals that while CNNs\neffectively capture spatial features and RNNs encode temporal sequences,\nTwo-Stream ConvNets exhibit superior performance by integrating spatial and\ntemporal dimensions. These insights are distilled from the evaluation metrics\nof accuracy, precision, recall, and F1-score. The results of this study\nunderscore the potential of composite models in achieving robust human action\nrecognition and suggest avenues for future research in optimizing these models\nfor real-world deployment.\n"
    },
    {
        "title": "Multistep Consistency Models",
        "published_time": "2024-03-11T15:26:34Z",
        "abstract": "  Diffusion models are relatively easy to train but require many steps to\ngenerate samples. Consistency models are far more difficult to train, but\ngenerate samples in a single step.\n  In this paper we propose Multistep Consistency Models: A unification between\nConsistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that\ncan interpolate between a consistency model and a diffusion model: a trade-off\nbetween sampling speed and sampling quality. Specifically, a 1-step consistency\nmodel is a conventional consistency model whereas we show that a $\\infty$-step\nconsistency model is a diffusion model.\n  Multistep Consistency Models work really well in practice. By increasing the\nsample budget from a single step to 2-8 steps, we can train models more easily\nthat generate higher quality samples, while retaining much of the sampling\nspeed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1\nFID on Imagenet128 in 8 steps with consistency distillation. We also show that\nour method scales to a text-to-image diffusion model, generating samples that\nare very close to the quality of the original model.\n"
    },
    {
        "title": "Shape Non-rigid Kinematics (SNK): A Zero-Shot Method for Non-Rigid Shape\n  Matching via Unsupervised Functional Map Regularized Reconstruction",
        "published_time": "2024-03-11T15:23:11Z",
        "abstract": "  We present Shape Non-rigid Kinematics (SNK), a novel zero-shot method for\nnon-rigid shape matching that eliminates the need for extensive training or\nground truth data. SNK operates on a single pair of shapes, and employs a\nreconstruction-based strategy using an encoder-decoder architecture, which\ndeforms the source shape to closely match the target shape. During the process,\nan unsupervised functional map is predicted and converted into a point-to-point\nmap, serving as a supervisory mechanism for the reconstruction. To aid in\ntraining, we have designed a new decoder architecture that generates smooth,\nrealistic deformations. SNK demonstrates competitive results on traditional\nbenchmarks, simplifying the shape-matching process without compromising\naccuracy. Our code can be found online: https://github.com/pvnieo/SNK\n"
    },
    {
        "title": "Data-Independent Operator: A Training-Free Artifact Representation\n  Extractor for Generalizable Deepfake Detection",
        "published_time": "2024-03-11T15:22:28Z",
        "abstract": "  Recently, the proliferation of increasingly realistic synthetic images\ngenerated by various generative adversarial networks has increased the risk of\nmisuse. Consequently, there is a pressing need to develop a generalizable\ndetector for accurately recognizing fake images. The conventional methods rely\non generating diverse training sources or large pretrained models. In this\nwork, we show that, on the contrary, the small and training-free filter is\nsufficient to capture more general artifact representations. Due to its unbias\ntowards both the training and test sources, we define it as Data-Independent\nOperator (DIO) to achieve appealing improvements on unseen sources. In our\nframework, handcrafted filters and the randomly-initialized convolutional layer\ncan be used as the training-free artifact representations extractor with\nexcellent results. With the data-independent operator of a popular classifier,\nsuch as Resnet50, one could already reach a new state-of-the-art without bells\nand whistles. We evaluate the effectiveness of the DIO on 33 generation models,\neven DALLE and Midjourney. Our detector achieves a remarkable improvement of\n$13.3\\%$, establishing a new state-of-the-art performance. The DIO and its\nextension can serve as strong baselines for future methods. The code is\navailable at\n\\url{https://github.com/chuangchuangtan/Data-Independent-Operator}.\n"
    },
    {
        "title": "CT2Rep: Automated Radiology Report Generation for 3D Medical Imaging",
        "published_time": "2024-03-11T15:17:45Z",
        "abstract": "  Medical imaging plays a crucial role in diagnosis, with radiology reports\nserving as vital documentation. Automating report generation has emerged as a\ncritical need to alleviate the workload of radiologists. While machine learning\nhas facilitated report generation for 2D medical imaging, extending this to 3D\nhas been unexplored due to computational complexity and data scarcity. We\nintroduce the first method to generate radiology reports for 3D medical\nimaging, specifically targeting chest CT volumes. Given the absence of\ncomparable methods, we establish a baseline using an advanced 3D vision encoder\nin medical imaging to demonstrate our method's effectiveness, which leverages a\nnovel auto-regressive causal transformer. Furthermore, recognizing the benefits\nof leveraging information from previous visits, we augment CT2Rep with a\ncross-attention-based multi-modal fusion module and hierarchical memory,\nenabling the incorporation of longitudinal multimodal data. Access our code at:\nhttps://github.com/ibrahimethemhamamci/CT2Rep\n"
    },
    {
        "title": "MambaMIL: Enhancing Long Sequence Modeling with Sequence Reordering in\n  Computational Pathology",
        "published_time": "2024-03-11T15:17:25Z",
        "abstract": "  Multiple Instance Learning (MIL) has emerged as a dominant paradigm to\nextract discriminative feature representations within Whole Slide Images (WSIs)\nin computational pathology. Despite driving notable progress, existing MIL\napproaches suffer from limitations in facilitating comprehensive and efficient\ninteractions among instances, as well as challenges related to time-consuming\ncomputations and overfitting. In this paper, we incorporate the Selective Scan\nSpace State Sequential Model (Mamba) in Multiple Instance Learning (MIL) for\nlong sequence modeling with linear complexity, termed as MambaMIL. By\ninheriting the capability of vanilla Mamba, MambaMIL demonstrates the ability\nto comprehensively understand and perceive long sequences of instances.\nFurthermore, we propose the Sequence Reordering Mamba (SR-Mamba) aware of the\norder and distribution of instances, which exploits the inherent valuable\ninformation embedded within the long sequences. With the SR-Mamba as the core\ncomponent, MambaMIL can effectively capture more discriminative features and\nmitigate the challenges associated with overfitting and high computational\noverhead. Extensive experiments on two public challenging tasks across nine\ndiverse datasets demonstrate that our proposed framework performs favorably\nagainst state-of-the-art MIL methods. The code is released at\nhttps://github.com/isyangshu/MambaMIL.\n"
    },
    {
        "title": "Dynamic Perturbation-Adaptive Adversarial Training on Medical Image\n  Classification",
        "published_time": "2024-03-11T15:16:20Z",
        "abstract": "  Remarkable successes were made in Medical Image Classification (MIC)\nrecently, mainly due to wide applications of convolutional neural networks\n(CNNs). However, adversarial examples (AEs) exhibited imperceptible similarity\nwith raw data, raising serious concerns on network robustness. Although\nadversarial training (AT), in responding to malevolent AEs, was recognized as\nan effective approach to improve robustness, it was challenging to overcome\ngeneralization decline of networks caused by the AT. In this paper, in order to\nreserve high generalization while improving robustness, we proposed a dynamic\nperturbation-adaptive adversarial training (DPAAT) method, which placed AT in a\ndynamic learning environment to generate adaptive data-level perturbations and\nprovided a dynamically updated criterion by loss information collections to\nhandle the disadvantage of fixed perturbation sizes in conventional AT methods\nand the dependence on external transference. Comprehensive testing on\ndermatology HAM10000 dataset showed that the DPAAT not only achieved better\nrobustness improvement and generalization preservation but also significantly\nenhanced mean average precision and interpretability on various CNNs,\nindicating its great potential as a generic adversarial training method on the\nMIC.\n"
    },
    {
        "title": "Leveraging Internal Representations of Model for Magnetic Image\n  Classification",
        "published_time": "2024-03-11T15:15:50Z",
        "abstract": "  Data generated by edge devices has the potential to train intelligent\nautonomous systems across various domains. Despite the emergence of diverse\nmachine learning approaches addressing privacy concerns and utilizing\ndistributed data, security issues persist due to the sensitive storage of data\nshards in disparate locations. This paper introduces a potentially\ngroundbreaking paradigm for machine learning model training, specifically\ndesigned for scenarios with only a single magnetic image and its corresponding\nlabel image available. We harness the capabilities of Deep Learning to generate\nconcise yet informative samples, aiming to overcome data scarcity. Through the\nutilization of deep learning's internal representations, our objective is to\nefficiently address data scarcity issues and produce meaningful results. This\nmethodology presents a promising avenue for training machine learning models\nwith minimal data.\n"
    },
    {
        "title": "Boosting Image Restoration via Priors from Pre-trained Models",
        "published_time": "2024-03-11T15:11:57Z",
        "abstract": "  Pre-trained models with large-scale training data, such as CLIP and Stable\nDiffusion, have demonstrated remarkable performance in various high-level\ncomputer vision tasks such as image understanding and generation from language\ndescriptions. Yet, their potential for low-level tasks such as image\nrestoration remains relatively unexplored. In this paper, we explore such\nmodels to enhance image restoration. As off-the-shelf features (OSF) from\npre-trained models do not directly serve image restoration, we propose to learn\nan additional lightweight module called Pre-Train-Guided Refinement Module\n(PTG-RM) to refine restoration results of a target restoration network with\nOSF. PTG-RM consists of two components, Pre-Train-Guided Spatial-Varying\nEnhancement (PTG-SVE), and Pre-Train-Guided Channel-Spatial Attention\n(PTG-CSA). PTG-SVE enables optimal short- and long-range neural operations,\nwhile PTG-CSA enhances spatial-channel attention for restoration-related\nlearning. Extensive experiments demonstrate that PTG-RM, with its compact size\n($<$1M parameters), effectively enhances restoration performance of various\nmodels across different tasks, including low-light enhancement, deraining,\ndeblurring, and denoising.\n"
    },
    {
        "title": "MiKASA: Multi-Key-Anchor & Scene-Aware Transformer for 3D Visual\n  Grounding",
        "published_time": "2024-03-05T16:01:55Z",
        "abstract": "  3D visual grounding involves matching natural language descriptions with\ntheir corresponding objects in 3D spaces. Existing methods often face\nchallenges with accuracy in object recognition and struggle in interpreting\ncomplex linguistic queries, particularly with descriptions that involve\nmultiple anchors or are view-dependent. In response, we present the MiKASA\n(Multi-Key-Anchor Scene-Aware) Transformer. Our novel end-to-end trained model\nintegrates a self-attention-based scene-aware object encoder and an original\nmulti-key-anchor technique, enhancing object recognition accuracy and the\nunderstanding of spatial relationships. Furthermore, MiKASA improves the\nexplainability of decision-making, facilitating error diagnosis. Our model\nachieves the highest overall accuracy in the Referit3D challenge for both the\nSr3D and Nr3D datasets, particularly excelling by a large margin in categories\nthat require viewpoint-dependent descriptions.\n  The source code and additional resources for this project are available on\nGitHub: https://github.com/birdy666/MiKASA-3DVG\n"
    },
    {
        "title": "Genetic Learning for Designing Sim-to-Real Data Augmentations",
        "published_time": "2024-03-11T15:00:56Z",
        "abstract": "  Data augmentations are useful in closing the sim-to-real domain gap when\ntraining on synthetic data. This is because they widen the training data\ndistribution, thus encouraging the model to generalize better to other domains.\nMany image augmentation techniques exist, parametrized by different settings,\nsuch as strength and probability. This leads to a large space of different\npossible augmentation policies. Some policies work better than others for\novercoming the sim-to-real gap for specific datasets, and it is unclear why.\nThis paper presents two different interpretable metrics that can be combined to\npredict how well a certain augmentation policy will work for a specific\nsim-to-real setting, focusing on object detection. We validate our metrics by\ntraining many models with different augmentation policies and showing a strong\ncorrelation with performance on real data. Additionally, we introduce\nGeneticAugment, a genetic programming method that can leverage these metrics to\nautomatically design an augmentation policy for a specific dataset without\nneeding to train a model.\n"
    },
    {
        "title": "FaceChain-SuDe: Building Derived Class to Inherit Category Attributes\n  for One-shot Subject-Driven Generation",
        "published_time": "2024-03-11T14:43:40Z",
        "abstract": "  Subject-driven generation has garnered significant interest recently due to\nits ability to personalize text-to-image generation. Typical works focus on\nlearning the new subject's private attributes. However, an important fact has\nnot been taken seriously that a subject is not an isolated new concept but\nshould be a specialization of a certain category in the pre-trained model. This\nresults in the subject failing to comprehensively inherit the attributes in its\ncategory, causing poor attribute-related generations. In this paper, motivated\nby object-oriented programming, we model the subject as a derived class whose\nbase class is its semantic category. This modeling enables the subject to\ninherit public attributes from its category while learning its private\nattributes from the user-provided example. Specifically, we propose a\nplug-and-play method, Subject-Derived regularization (SuDe). It constructs the\nbase-derived class modeling by constraining the subject-driven generated images\nto semantically belong to the subject's category. Extensive experiments under\nthree baselines and two backbones on various subjects show that our SuDe\nenables imaginative attribute-related generations while maintaining subject\nfidelity. Codes will be open sourced soon at FaceChain\n(https://github.com/modelscope/facechain).\n"
    },
    {
        "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models",
        "published_time": "2024-03-11T14:35:32Z",
        "abstract": "  In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV.\n"
    },
    {
        "title": "Average Calibration Error: A Differentiable Loss for Improved\n  Reliability in Image Segmentation",
        "published_time": "2024-03-11T14:31:03Z",
        "abstract": "  Deep neural networks for medical image segmentation often produce\noverconfident results misaligned with empirical observations. Such\nmiscalibration, challenges their clinical translation. We propose to use\nmarginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss\nfunction to improve pixel-wise calibration without compromising segmentation\nquality. We show that this loss, despite using hard binning, is directly\ndifferentiable, bypassing the need for approximate but differentiable surrogate\nor soft binning approaches. Our work also introduces the concept of dataset\nreliability histograms which generalises standard reliability diagrams for\nrefined visual assessment of calibration in semantic segmentation aggregated at\nthe dataset level. Using mL1-ACE, we reduce average and maximum calibration\nerror by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS\n2021 dataset. We share our code here: https://github.com/cai4cai/ACE-DLIRIS\n"
    },
    {
        "title": "EarthLoc: Astronaut Photography Localization by Indexing Earth from\n  Space",
        "published_time": "2024-03-11T14:30:51Z",
        "abstract": "  Astronaut photography, spanning six decades of human spaceflight, presents a\nunique Earth observations dataset with immense value for both scientific\nresearch and disaster response. Despite its significance, accurately localizing\nthe geographical extent of these images, crucial for effective utilization,\nposes substantial challenges. Current manual localization efforts are\ntime-consuming, motivating the need for automated solutions. We propose a novel\napproach - leveraging image retrieval - to address this challenge efficiently.\nWe introduce innovative training techniques, including Year-Wise Data\nAugmentation and a Neutral-Aware Multi-Similarity Loss, which contribute to the\ndevelopment of a high-performance model, EarthLoc. We develop six evaluation\ndatasets and perform a comprehensive benchmark comparing EarthLoc to existing\nmethods, showcasing its superior efficiency and accuracy. Our approach marks a\nsignificant advancement in automating the localization of astronaut\nphotography, which will help bridge a critical gap in Earth observations data.\nCode and datasets are available at https://github.com/gmberton/EarthLoc\n"
    },
    {
        "title": "Shortcut Learning in Medical Image Segmentation",
        "published_time": "2024-03-11T14:14:52Z",
        "abstract": "  Shortcut learning is a phenomenon where machine learning models prioritize\nlearning simple, potentially misleading cues from data that do not generalize\nwell beyond the training set. While existing research primarily investigates\nthis in the realm of image classification, this study extends the exploration\nof shortcut learning into medical image segmentation. We demonstrate that\nclinical annotations such as calipers, and the combination of zero-padded\nconvolutions and center-cropped training sets in the dataset can inadvertently\nserve as shortcuts, impacting segmentation accuracy. We identify and evaluate\nthe shortcut learning on two different but common medical image segmentation\ntasks. In addition, we suggest strategies to mitigate the influence of shortcut\nlearning and improve the generalizability of the segmentation models. By\nuncovering the presence and implications of shortcuts in medical image\nsegmentation, we provide insights and methodologies for evaluating and\novercoming this pervasive challenge and call for attention in the community for\nshortcuts in segmentation.\n"
    },
    {
        "title": "Distribution-Aware Data Expansion with Diffusion Models",
        "published_time": "2024-03-11T14:07:53Z",
        "abstract": "  The scale and quality of a dataset significantly impact the performance of\ndeep models. However, acquiring large-scale annotated datasets is both a costly\nand time-consuming endeavor. To address this challenge, dataset expansion\ntechnologies aim to automatically augment datasets, unlocking the full\npotential of deep models. Current data expansion methods encompass image\ntransformation-based and synthesis-based methods. The transformation-based\nmethods introduce only local variations, resulting in poor diversity. While\nimage synthesis-based methods can create entirely new content, significantly\nenhancing informativeness. However, existing synthesis methods carry the risk\nof distribution deviations, potentially degrading model performance with\nout-of-distribution samples. In this paper, we propose DistDiff, an effective\ndata expansion framework based on the distribution-aware diffusion model.\nDistDiff constructs hierarchical prototypes to approximate the real data\ndistribution, optimizing latent data points within diffusion models with\nhierarchical energy guidance. We demonstrate its ability to generate\ndistribution-consistent samples, achieving substantial improvements in data\nexpansion tasks. Specifically, without additional training, DistDiff achieves a\n30.7% improvement in accuracy across six image datasets compared to the model\ntrained on original datasets and a 9.8% improvement compared to the\nstate-of-the-art diffusion-based method. Our code is available at\nhttps://github.com/haoweiz23/DistDiff\n"
    },
    {
        "title": "V3D: Video Diffusion Models are Effective 3D Generators",
        "published_time": "2024-03-11T14:03:36Z",
        "abstract": "  Automatic 3D generation has recently attracted widespread attention. Recent\nmethods have greatly accelerated the generation speed, but usually produce\nless-detailed objects due to limited model capacity or 3D data. Motivated by\nrecent advancements in video diffusion models, we introduce V3D, which\nleverages the world simulation capacity of pre-trained video diffusion models\nto facilitate 3D generation. To fully unleash the potential of video diffusion\nto perceive the 3D world, we further introduce geometrical consistency prior\nand extend the video diffusion model to a multi-view consistent 3D generator.\nBenefiting from this, the state-of-the-art video diffusion model could be\nfine-tuned to generate 360degree orbit frames surrounding an object given a\nsingle image. With our tailored reconstruction pipelines, we can generate\nhigh-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method\ncan be extended to scene-level novel view synthesis, achieving precise control\nover the camera path with sparse input views. Extensive experiments demonstrate\nthe superior performance of the proposed approach, especially in terms of\ngeneration quality and multi-view consistency. Our code is available at\nhttps://github.com/heheyas/V3D\n"
    },
    {
        "title": "Enhancing Image Caption Generation Using Reinforcement Learning with\n  Human Feedback",
        "published_time": "2024-03-11T13:57:05Z",
        "abstract": "  Research on generative models to produce human-aligned / human-preferred\noutputs has seen significant recent contributions. Between text and\nimage-generative models, we narrowed our focus to text-based generative models,\nparticularly to produce captions for images that align with human preferences.\nIn this research, we explored a potential method to amplify the performance of\nthe Deep Neural Network Model to generate captions that are preferred by\nhumans. This was achieved by integrating Supervised Learning and Reinforcement\nLearning with Human Feedback (RLHF) using the Flickr8k dataset. Also, a novel\nloss function that is capable of optimizing the model based on human feedback\nis introduced. In this paper, we provide a concise sketch of our approach and\nresults, hoping to contribute to the ongoing advances in the field of\nhuman-aligned generative AI models.\n"
    },
    {
        "title": "Real-Time Multimodal Cognitive Assistant for Emergency Medical Services",
        "published_time": "2024-03-11T13:56:57Z",
        "abstract": "  Emergency Medical Services (EMS) responders often operate under\ntime-sensitive conditions, facing cognitive overload and inherent risks,\nrequiring essential skills in critical thinking and rapid decision-making. This\npaper presents CognitiveEMS, an end-to-end wearable cognitive assistant system\nthat can act as a collaborative virtual partner engaging in the real-time\nacquisition and analysis of multimodal data from an emergency scene and\ninteracting with EMS responders through Augmented Reality (AR) smart glasses.\nCognitiveEMS processes the continuous streams of data in real-time and\nleverages edge computing to provide assistance in EMS protocol selection and\nintervention recognition. We address key technical challenges in real-time\ncognitive assistance by introducing three novel components: (i) a Speech\nRecognition model that is fine-tuned for real-world medical emergency\nconversations using simulated EMS audio recordings, augmented with synthetic\ndata generated by large language models (LLMs); (ii) an EMS Protocol Prediction\nmodel that combines state-of-the-art (SOTA) tiny language models with EMS\ndomain knowledge using graph-based attention mechanisms; (iii) an EMS Action\nRecognition module which leverages multimodal audio and video data and protocol\npredictions to infer the intervention/treatment actions taken by the responders\nat the incident scene. Our results show that for speech recognition we achieve\nsuperior performance compared to SOTA (WER of 0.290 vs. 0.618) on\nconversational data. Our protocol prediction component also significantly\noutperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition\nachieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s\nfor protocol prediction on the edge and 0.31s on the server.\n"
    },
    {
        "title": "Large Model driven Radiology Report Generation with Clinical Quality\n  Reinforcement Learning",
        "published_time": "2024-03-11T13:47:11Z",
        "abstract": "  Radiology report generation (RRG) has attracted significant attention due to\nits potential to reduce the workload of radiologists. Current RRG approaches\nare still unsatisfactory against clinical standards. This paper introduces a\nnovel RRG method, \\textbf{LM-RRG}, that integrates large models (LMs) with\nclinical quality reinforcement learning to generate accurate and comprehensive\nchest X-ray radiology reports. Our method first designs a large language model\ndriven feature extractor to analyze and interpret different regions of the\nchest X-ray image, emphasizing specific regions with medical significance.\nNext, based on the large model's decoder, we develop a multimodal report\ngenerator that leverages multimodal prompts from visual features and textual\ninstruction to produce the radiology report in an auto-regressive way. Finally,\nto better reflect the clinical significant and insignificant errors that\nradiologists would normally assign in the report, we introduce a novel clinical\nquality reinforcement learning strategy. It utilizes the radiology report\nclinical quality (RadCliQ) metric as a reward function in the learning process.\nExtensive experiments on the MIMIC-CXR and IU-Xray datasets demonstrate the\nsuperiority of our method over the state of the art.\n"
    },
    {
        "title": "Editable Scene Simulation for Autonomous Driving via Collaborative\n  LLM-Agents",
        "published_time": "2024-02-08T15:26:28Z",
        "abstract": "  Scene simulation in autonomous driving has gained significant attention\nbecause of its huge potential for generating customized data. However, existing\neditable scene simulation approaches face limitations in terms of user\ninteraction efficiency, multi-camera photo-realistic rendering and external\ndigital assets integration. To address these challenges, this paper introduces\nChatSim, the first system that enables editable photo-realistic 3D driving\nscene simulations via natural language commands with external digital assets.\nTo enable editing with high command flexibility,~ChatSim leverages a large\nlanguage model (LLM) agent collaboration framework. To generate photo-realistic\noutcomes, ChatSim employs a novel multi-camera neural radiance field method.\nFurthermore, to unleash the potential of extensive high-quality digital assets,\nChatSim employs a novel multi-camera lighting estimation method to achieve\nscene-consistent assets' rendering. Our experiments on Waymo Open Dataset\ndemonstrate that ChatSim can handle complex language commands and generate\ncorresponding photo-realistic scene videos.\n"
    },
    {
        "title": "Elastic Feature Consolidation for Cold Start Exemplar-free Incremental\n  Learning",
        "published_time": "2024-02-06T11:35:02Z",
        "abstract": "  Exemplar-Free Class Incremental Learning (EFCIL) aims to learn from a\nsequence of tasks without having access to previous task data. In this paper,\nwe consider the challenging Cold Start scenario in which insufficient data is\navailable in the first task to learn a high-quality backbone. This is\nespecially challenging for EFCIL since it requires high plasticity, which\nresults in feature drift which is difficult to compensate for in the\nexemplar-free setting. To address this problem, we propose a simple and\neffective approach that consolidates feature representations by regularizing\ndrift in directions highly relevant to previous tasks and employs prototypes to\nreduce task-recency bias. Our method, called Elastic Feature Consolidation\n(EFC), exploits a tractable second-order approximation of feature drift based\non an Empirical Feature Matrix (EFM). The EFM induces a pseudo-metric in\nfeature space which we use to regularize feature drift in important directions\nand to update Gaussian prototypes used in a novel asymmetric cross entropy loss\nwhich effectively balances prototype rehearsal with data from new tasks.\nExperimental results on CIFAR-100, Tiny-ImageNet, ImageNet-Subset and\nImageNet-1K demonstrate that Elastic Feature Consolidation is better able to\nlearn new tasks by maintaining model plasticity and significantly outperform\nthe state-of-the-art.\n"
    },
    {
        "title": "Fast Text-to-3D-Aware Face Generation and Manipulation via Direct\n  Cross-modal Mapping and Geometric Regularization",
        "published_time": "2024-03-11T13:17:55Z",
        "abstract": "  Text-to-3D-aware face (T3D Face) generation and manipulation is an emerging\nresearch hot spot in machine learning, which still suffers from low efficiency\nand poor quality. In this paper, we propose an End-to-End Efficient and\nEffective network for fast and accurate T3D face generation and manipulation,\ntermed $E^3$-FaceNet. Different from existing complex generation paradigms,\n$E^3$-FaceNet resorts to a direct mapping from text instructions to 3D-aware\nvisual space. We introduce a novel Style Code Enhancer to enhance cross-modal\nsemantic alignment, alongside an innovative Geometric Regularization objective\nto maintain consistency across multi-view generations. Extensive experiments on\nthree benchmark datasets demonstrate that $E^3$-FaceNet can not only achieve\npicture-like 3D face generation and manipulation, but also improve inference\nspeed by orders of magnitudes. For instance, compared with Latent3D,\n$E^3$-FaceNet speeds up the five-view generations by almost 470 times, while\nstill exceeding in generation quality. Our code are released at\nhttps://github.com/Aria-Zhangjl/E3-FaceNet.\n"
    },
    {
        "title": "PCLD: Point Cloud Layerwise Diffusion for Adversarial Purification",
        "published_time": "2024-03-11T13:13:10Z",
        "abstract": "  Point clouds are extensively employed in a variety of real-world applications\nsuch as robotics, autonomous driving and augmented reality. Despite the recent\nsuccess of point cloud neural networks, especially for safety-critical tasks,\nit is essential to also ensure the robustness of the model. A typical way to\nassess a model's robustness is through adversarial attacks, where test-time\nexamples are generated based on gradients to deceive the model. While many\ndifferent defense mechanisms are studied in 2D, studies on 3D point clouds have\nbeen relatively limited in the academic field. Inspired from PointDP, which\ndenoises the network inputs by diffusion, we propose Point Cloud Layerwise\nDiffusion (PCLD), a layerwise diffusion based 3D point cloud defense strategy.\nUnlike PointDP, we propagated the diffusion denoising after each layer to\nincrementally enhance the results. We apply our defense method to different\ntypes of commonly used point cloud models and adversarial attacks to evaluate\nits robustness. Our experiments demonstrate that the proposed defense method\nachieved results that are comparable to or surpass those of existing\nmethodologies, establishing robustness through a novel technique. Code is\navailable at https://github.com/batuceng/diffusion-layer-robustness-pc.\n"
    },
    {
        "title": "Advancing Graph Neural Networks with HL-HGAT: A Hodge-Laplacian and\n  Attention Mechanism Approach for Heterogeneous Graph-Structured Data",
        "published_time": "2024-03-11T13:04:21Z",
        "abstract": "  Graph neural networks (GNNs) have proven effective in capturing relationships\namong nodes in a graph. This study introduces a novel perspective by\nconsidering a graph as a simplicial complex, encompassing nodes, edges,\ntriangles, and $k$-simplices, enabling the definition of graph-structured data\non any $k$-simplices. Our contribution is the Hodge-Laplacian heterogeneous\ngraph attention network (HL-HGAT), designed to learn heterogeneous signal\nrepresentations across $k$-simplices. The HL-HGAT incorporates three key\ncomponents: HL convolutional filters (HL-filters), simplicial projection (SP),\nand simplicial attention pooling (SAP) operators, applied to $k$-simplices.\nHL-filters leverage the unique topology of $k$-simplices encoded by the\nHodge-Laplacian (HL) operator, operating within the spectral domain of the\n$k$-th HL operator. To address computation challenges, we introduce a\npolynomial approximation for HL-filters, exhibiting spatial localization\nproperties. Additionally, we propose a pooling operator to coarsen\n$k$-simplices, combining features through simplicial attention mechanisms of\nself-attention and cross-attention via transformers and SP operators, capturing\ntopological interconnections across multiple dimensions of simplices. The\nHL-HGAT is comprehensively evaluated across diverse graph applications,\nincluding NP-hard problems, graph multi-label and classification challenges,\nand graph regression tasks in logistics, computer vision, biology, chemistry,\nand neuroscience. The results demonstrate the model's efficacy and versatility\nin handling a wide range of graph-based scenarios.\n"
    },
    {
        "title": "Transferring Relative Monocular Depth to Surgical Vision with Temporal\n  Consistency",
        "published_time": "2024-03-11T12:57:51Z",
        "abstract": "  Relative monocular depth, inferring depth up to shift and scale from a single\nimage, is an active research topic. Recent deep learning models, trained on\nlarge and varied meta-datasets, now provide excellent performance in the domain\nof natural images. However, few datasets exist which provide ground truth depth\nfor endoscopic images, making training such models from scratch unfeasible.\nThis work investigates the transfer of these models into the surgical domain,\nand presents an effective and simple way to improve on standard supervision\nthrough the use of temporal consistency self-supervision. We show temporal\nconsistency significantly improves supervised training alone when transferring\nto the low-data regime of endoscopy, and outperforms the prevalent\nself-supervision technique for this task. In addition we show our method\ndrastically outperforms the state-of-the-art method from within the domain of\nendoscopy. We also release our code, model and ensembled meta-dataset,\nMeta-MED, establishing a strong benchmark for future work.\n"
    },
    {
        "title": "Restoring Ancient Ideograph: A Multimodal Multitask Neural Network\n  Approach",
        "published_time": "2024-03-11T12:57:28Z",
        "abstract": "  Cultural heritage serves as the enduring record of human thought and history.\nDespite significant efforts dedicated to the preservation of cultural relics,\nmany ancient artefacts have been ravaged irreversibly by natural deterioration\nand human actions. Deep learning technology has emerged as a valuable tool for\nrestoring various kinds of cultural heritages, including ancient text\nrestoration. Previous research has approached ancient text restoration from\neither visual or textual perspectives, often overlooking the potential of\nsynergizing multimodal information. This paper proposes a novel Multimodal\nMultitask Restoring Model (MMRM) to restore ancient texts, particularly\nemphasising the ideograph. This model combines context understanding with\nresidual visual information from damaged ancient artefacts, enabling it to\npredict damaged characters and generate restored images simultaneously. We\ntested the MMRM model through experiments conducted on both simulated datasets\nand authentic ancient inscriptions. The results show that the proposed method\ngives insightful restoration suggestions in both simulation experiments and\nreal-world scenarios. To the best of our knowledge, this work represents the\npioneering application of multimodal deep learning in ancient text restoration,\nwhich will contribute to the understanding of ancient society and culture in\ndigital humanities fields.\n"
    },
    {
        "title": "Trustworthy Partial Label Learning with Out-of-distribution Detection",
        "published_time": "2024-03-11T12:56:36Z",
        "abstract": "  Partial Label Learning (PLL) grapples with learning from ambiguously labelled\ndata, and it has been successfully applied in fields such as image recognition.\nNevertheless, traditional PLL methods rely on the closed-world assumption,\nwhich can be limiting in open-world scenarios and negatively impact model\nperformance and generalization. To tackle these challenges, our study\nintroduces a novel method called PLL-OOD, which is the first to incorporate\nOut-of-Distribution (OOD) detection into the PLL framework. PLL-OOD\nsignificantly enhances model adaptability and accuracy by merging\nself-supervised learning with partial label loss and pioneering the\nPartial-Energy (PE) score for OOD detection. This approach improves data\nfeature representation and effectively disambiguates candidate labels, using a\ndynamic label confidence matrix to refine predictions. The PE score, adjusted\nby label confidence, precisely identifies OOD instances, optimizing model\ntraining towards in-distribution data. This innovative method markedly boosts\nPLL model robustness and performance in open-world settings. To validate our\napproach, we conducted a comprehensive comparative experiment combining the\nexisting state-of-the-art PLL model with multiple OOD scores on the CIFAR-10\nand CIFAR-100 datasets with various OOD datasets. The results demonstrate that\nthe proposed PLL-OOD framework is highly effective and effectiveness\noutperforms existing models, showcasing its superiority and effectiveness.\n"
    },
    {
        "title": "Answering Diverse Questions via Text Attached with Key Audio-Visual\n  Clues",
        "published_time": "2024-03-11T12:51:37Z",
        "abstract": "  Audio-visual question answering (AVQA) requires reference to video content\nand auditory information, followed by correlating the question to predict the\nmost precise answer. Although mining deeper layers of audio-visual information\nto interact with questions facilitates the multimodal fusion process, the\nredundancy of audio-visual parameters tends to reduce the generalization of the\ninference engine to multiple question-answer pairs in a single video. Indeed,\nthe natural heterogeneous relationship between audiovisuals and text makes the\nperfect fusion challenging, to prevent high-level audio-visual semantics from\nweakening the network's adaptability to diverse question types, we propose a\nframework for performing mutual correlation distillation (MCD) to aid question\ninference. MCD is divided into three main steps: 1) firstly, the residual\nstructure is utilized to enhance the audio-visual soft associations based on\nself-attention, then key local audio-visual features relevant to the question\ncontext are captured hierarchically by shared aggregators and coupled in the\nform of clues with specific question vectors. 2) Secondly, knowledge\ndistillation is enforced to align audio-visual-text pairs in a shared latent\nspace to narrow the cross-modal semantic gap. 3) And finally, the audio-visual\ndependencies are decoupled by discarding the decision-level integrations. We\nevaluate the proposed method on two publicly available datasets containing\nmultiple question-and-answer pairs, i.e., Music-AVQA and AVQA. Experiments show\nthat our method outperforms other state-of-the-art methods, and one interesting\nfinding behind is that removing deep audio-visual features during inference can\neffectively mitigate overfitting. The source code is released at\nhttp://github.com/rikeilong/MCD-forAVQA.\n"
    },
    {
        "title": "CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object\n  Localization Perspective",
        "published_time": "2024-03-11T12:48:22Z",
        "abstract": "  Recently, convolutional neural networks (CNNs) with large size kernels have\nattracted much attention in the computer vision field, following the success of\nthe Vision Transformers. Large kernel CNNs have been reported to perform well\nin downstream vision tasks as well as in classification performance. The reason\nfor the high-performance of large kernel CNNs in downstream tasks has been\nattributed to the large effective receptive field (ERF) produced by large size\nkernels, but this view has not been fully tested. We therefore revisit the\nperformance of large kernel CNNs in downstream task, focusing on the weakly\nsupervised object localization (WSOL) task. WSOL, a difficult downstream task\nthat is not fully supervised, provides a new angle to explore the capabilities\nof the large kernel CNNs. Our study compares the modern large kernel CNNs\nConvNeXt, RepLKNet, and SLaK to test the validity of the naive expectation that\nERF size is important for improving downstream task performance. Our analysis\nof the factors contributing to high performance provides a different\nperspective, in which the main factor is feature map improvement. Furthermore,\nwe find that modern CNNs are robust to the CAM problems of local regions of\nobjects being activated, which has long been discussed in WSOL. CAM is the most\nclassic WSOL method, but because of the above-mentioned problems, it is often\nused as a baseline method for comparison. However, experiments on the\nCUB-200-2011 dataset show that simply combining a large kernel CNN, CAM, and\nsimple data augmentation methods can achieve performance (90.99% MaxBoxAcc)\ncomparable to the latest WSOL method, which is CNN-based and requires special\ntraining or complex post-processing. The code is available at\nhttps://github.com/snskysk/CAM-Back-Again.\n"
    },
    {
        "title": "Car Damage Detection and Patch-to-Patch Self-supervised Image Alignment",
        "published_time": "2024-03-11T12:46:53Z",
        "abstract": "  Most computer vision applications aim to identify pixels in a scene and use\nthem for diverse purposes. One intriguing application is car damage detection\nfor insurance carriers which tends to detect all car damages by comparing both\npre-trip and post-trip images, even requiring two components: (i) car damage\ndetection; (ii) image alignment. Firstly, we implemented a Mask R-CNN model to\ndetect car damages on custom images. Whereas for the image alignment section,\nwe especially propose a novel self-supervised Patch-to-Patch SimCLR inspired\nalignment approach to find perspective transformations between custom pre/post\ncar rental images except for traditional computer vision methods.\n"
    },
    {
        "title": "LVC-LGMC: Joint Local and Global Motion Compensation for Learned Video\n  Compression",
        "published_time": "2024-02-01T15:43:43Z",
        "abstract": "  Existing learned video compression models employ flow net or deformable\nconvolutional networks (DCN) to estimate motion information. However, the\nlimited receptive fields of flow net and DCN inherently direct their\nattentiveness towards the local contexts. Global contexts, such as large-scale\nmotions and global correlations among frames are ignored, presenting a\nsignificant bottleneck for capturing accurate motions. To address this issue,\nwe propose a joint local and global motion compensation module (LGMC) for\nleaned video coding. More specifically, we adopt flow net for local motion\ncompensation. To capture global context, we employ the cross attention in\nfeature domain for motion compensation. In addition, to avoid the quadratic\ncomplexity of vanilla cross attention, we divide the softmax operations in\nattention into two independent softmax operations, leading to linear\ncomplexity. To validate the effectiveness of our proposed LGMC, we integrate it\nwith DCVC-TCM and obtain learned video compression with joint local and global\nmotion compensation (LVC-LGMC). Extensive experiments demonstrate that our\nLVC-LGMC has significant rate-distortion performance improvements over baseline\nDCVC-TCM.\n"
    },
    {
        "title": "Dual-path Frequency Discriminators for Few-shot Anomaly Detection",
        "published_time": "2024-03-07T02:17:59Z",
        "abstract": "  Few-shot anomaly detection (FSAD) is essential in industrial manufacturing.\nHowever, existing FSAD methods struggle to effectively leverage a limited\nnumber of normal samples, and they may fail to detect and locate inconspicuous\nanomalies in the spatial domain. We further discover that these subtle\nanomalies would be more noticeable in the frequency domain. In this paper, we\npropose a Dual-Path Frequency Discriminators (DFD) network from a frequency\nperspective to tackle these issues. Specifically, we generate anomalies at both\nimage-level and feature-level. Differential frequency components are extracted\nby the multi-frequency information construction module and supplied into the\nfine-grained feature construction module to provide adapted features. We\nconsider anomaly detection as a discriminative classification problem,\nwherefore the dual-path feature discrimination module is employed to detect and\nlocate the image-level and feature-level anomalies in the feature space. The\ndiscriminators aim to learn a joint representation of anomalous features and\nnormal features in the latent space. Extensive experiments conducted on MVTec\nAD and VisA benchmarks demonstrate that our DFD surpasses current\nstate-of-the-art methods. Source code will be available.\n"
    },
    {
        "title": "PeerAiD: Improving Adversarial Distillation from a Specialized Peer\n  Tutor",
        "published_time": "2024-03-11T12:36:14Z",
        "abstract": "  Adversarial robustness of the neural network is a significant concern when it\nis applied to security-critical domains. In this situation, adversarial\ndistillation is a promising option which aims to distill the robustness of the\nteacher network to improve the robustness of a small student network. Previous\nworks pretrain the teacher network to make it robust to the adversarial\nexamples aimed at itself. However, the adversarial examples are dependent on\nthe parameters of the target network. The fixed teacher network inevitably\ndegrades its robustness against the unseen transferred adversarial examples\nwhich targets the parameters of the student network in the adversarial\ndistillation process. We propose PeerAiD to make a peer network learn the\nadversarial examples of the student network instead of adversarial examples\naimed at itself. PeerAiD is an adversarial distillation that trains the peer\nnetwork and the student network simultaneously in order to make the peer\nnetwork specialized for defending the student network. We observe that such\npeer networks surpass the robustness of pretrained robust teacher network\nagainst student-attacked adversarial samples. With this peer network and\nadversarial distillation, PeerAiD achieves significantly higher robustness of\nthe student network with AutoAttack (AA) accuracy up to 1.66%p and improves the\nnatural accuracy of the student network up to 4.72%p with ResNet-18 and\nTinyImageNet dataset.\n"
    },
    {
        "title": "epsilon-Mesh Attack: A Surface-based Adversarial Point Cloud Attack for\n  Facial Expression Recognition",
        "published_time": "2024-03-11T12:29:55Z",
        "abstract": "  Point clouds and meshes are widely used 3D data structures for many computer\nvision applications. While the meshes represent the surfaces of an object,\npoint cloud represents sampled points from the surface which is also the output\nof modern sensors such as LiDAR and RGB-D cameras. Due to the wide application\narea of point clouds and the recent advancements in deep neural networks,\nstudies focusing on robust classification of the 3D point cloud data emerged.\nTo evaluate the robustness of deep classifier networks, a common method is to\nuse adversarial attacks where the gradient direction is followed to change the\ninput slightly. The previous studies on adversarial attacks are generally\nevaluated on point clouds of daily objects. However, considering 3D faces,\nthese adversarial attacks tend to affect the person's facial structure more\nthan the desired amount and cause malformation. Specifically for facial\nexpressions, even a small adversarial attack can have a significant effect on\nthe face structure. In this paper, we suggest an adversarial attack called\n$\\epsilon$-Mesh Attack, which operates on point cloud data via limiting\nperturbations to be on the mesh surface. We also parameterize our attack by\n$\\epsilon$ to scale the perturbation mesh. Our surface-based attack has tighter\nperturbation bounds compared to $L_2$ and $L_\\infty$ norm bounded attacks that\noperate on unit-ball. Even though our method has additional constraints, our\nexperiments on CoMA, Bosphorus and FaceWarehouse datasets show that\n$\\epsilon$-Mesh Attack (Perpendicular) successfully confuses trained DGCNN and\nPointNet models $99.72\\%$ and $97.06\\%$ of the time, with indistinguishable\nfacial deformations. The code is available at\nhttps://github.com/batuceng/e-mesh-attack.\n"
    },
    {
        "title": "Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration\n  Framework",
        "published_time": "2024-03-11T12:27:20Z",
        "abstract": "  Large vision models based in deep learning architectures have been\nconsistently advancing the state-of-the-art in biometric recognition. However,\nthree weaknesses are commonly reported for such kind of approaches: 1) their\nextreme demands in terms of learning data; 2) the difficulties in generalising\nbetween different domains; and 3) the lack of interpretability/explainability,\nwith biometrics being of particular interest, as it is important to provide\nevidence able to be used for forensics/legal purposes (e.g., in courts). To the\nbest of our knowledge, this paper describes the first recognition\nframework/strategy that aims at addressing the three weaknesses simultaneously.\nAt first, it relies exclusively in synthetic samples for learning purposes.\nInstead of requiring a large amount and variety of samples for each subject,\nthe idea is to exclusively enroll a 3D point cloud per identity. Then, using\ngenerative strategies, we synthesize a very large (potentially infinite) number\nof samples, containing all the desired covariates (poses, clothing, distances,\nperspectives, lighting, occlusions,...). Upon the synthesizing method used, it\nis possible to adapt precisely to different kind of domains, which accounts for\ngeneralization purposes. Such data are then used to learn a model that performs\nlocal registration between image pairs, establishing positive correspondences\nbetween body parts that are the key, not only to recognition (according to\ncardinality and distribution), but also to provide an interpretable description\nof the response (e.g.: \"both samples are from the same person, as they have\nsimilar facial shape, hair color and legs thickness\").\n"
    },
    {
        "title": "Ricci flow-based brain surface covariance descriptors for Alzheimer\n  disease",
        "published_time": "2024-03-11T12:07:33Z",
        "abstract": "  Automated feature extraction from MRI brain scans and diagnosis of\nAlzheimer's disease are ongoing challenges. With advances in 3D imaging\ntechnology, 3D data acquisition is becoming more viable and efficient than its\n2D counterpart. Rather than using feature-based vectors, in this paper, for the\nfirst time, we suggest a pipeline to extract novel covariance-based descriptors\nfrom the cortical surface using the Ricci energy optimization. The covariance\ndescriptors are components of the nonlinear manifold of symmetric\npositive-definite matrices, thus we focus on using the Gaussian radial basis\nfunction to apply manifold-based classification to the 3D shape problem.\nApplying this novel signature to the analysis of abnormal cortical brain\nmorphometry allows for diagnosing Alzheimer's disease. Experimental studies\nperformed on about two hundred 3D MRI brain models, gathered from Alzheimer's\nDisease Neuroimaging Initiative (ADNI) dataset demonstrate the effectiveness of\nour descriptors in achieving remarkable classification accuracy.\n"
    },
    {
        "title": "Evaluating the Energy Efficiency of Few-Shot Learning for Object\n  Detection in Industrial Settings",
        "published_time": "2024-03-11T11:41:30Z",
        "abstract": "  In the ever-evolving era of Artificial Intelligence (AI), model performance\nhas constituted a key metric driving innovation, leading to an exponential\ngrowth in model size and complexity. However, sustainability and energy\nefficiency have been critical requirements during deployment in contemporary\nindustrial settings, necessitating the use of data-efficient approaches such as\nfew-shot learning. In this paper, to alleviate the burden of lengthy model\ntraining and minimize energy consumption, a finetuning approach to adapt\nstandard object detection models to downstream tasks is examined. Subsequently,\na thorough case study and evaluation of the energy demands of the developed\nmodels, applied in object detection benchmark datasets from volatile industrial\nenvironments is presented. Specifically, different finetuning strategies as\nwell as utilization of ancillary evaluation data during training are examined,\nand the trade-off between performance and efficiency is highlighted in this\nlow-data regime. Finally, this paper introduces a novel way to quantify this\ntrade-off through a customized Efficiency Factor metric.\n"
    },
    {
        "title": "Forest Inspection Dataset for Aerial Semantic Segmentation and Depth\n  Estimation",
        "published_time": "2024-03-11T11:26:44Z",
        "abstract": "  Humans use UAVs to monitor changes in forest environments since they are\nlightweight and provide a large variety of surveillance data. However, their\ninformation does not present enough details for understanding the scene which\nis needed to assess the degree of deforestation. Deep learning algorithms must\nbe trained on large amounts of data to output accurate interpretations, but\nground truth recordings of annotated forest imagery are not available. To solve\nthis problem, we introduce a new large aerial dataset for forest inspection\nwhich contains both real-world and virtual recordings of natural environments,\nwith densely annotated semantic segmentation labels and depth maps, taken in\ndifferent illumination conditions, at various altitudes and recording angles.\nWe test the performance of two multi-scale neural networks for solving the\nsemantic segmentation task (HRNet and PointFlow network), studying the impact\nof the various acquisition conditions and the capabilities of transfer learning\nfrom virtual to real data. Our results showcase that the best results are\nobtained when the training is done on a dataset containing a large variety of\nscenarios, rather than separating the data into specific categories. We also\ndevelop a framework to assess the deforestation degree of an area.\n"
    },
    {
        "title": "Density-Guided Label Smoothing for Temporal Localization of Driving\n  Actions",
        "published_time": "2024-03-11T11:06:41Z",
        "abstract": "  Temporal localization of driving actions plays a crucial role in advanced\ndriver-assistance systems and naturalistic driving studies. However, this is a\nchallenging task due to strict requirements for robustness, reliability and\naccurate localization. In this work, we focus on improving the overall\nperformance by efficiently utilizing video action recognition networks and\nadapting these to the problem of action localization. To this end, we first\ndevelop a density-guided label smoothing technique based on label probability\ndistributions to facilitate better learning from boundary video-segments that\ntypically include multiple labels. Second, we design a post-processing step to\nefficiently fuse information from video-segments and multiple camera views into\nscene-level predictions, which facilitates elimination of false positives. Our\nmethodology yields a competitive performance on the A2 test set of the\nnaturalistic driving action recognition track of the 2022 NVIDIA AI City\nChallenge with an F1 score of 0.271.\n"
    },
    {
        "title": "Distributionally Generative Augmentation for Fair Facial Attribute\n  Classification",
        "published_time": "2024-03-11T10:50:53Z",
        "abstract": "  Facial Attribute Classification (FAC) holds substantial promise in widespread\napplications. However, FAC models trained by traditional methodologies can be\nunfair by exhibiting accuracy inconsistencies across varied data\nsubpopulations. This unfairness is largely attributed to bias in data, where\nsome spurious attributes (e.g., Male) statistically correlate with the target\nattribute (e.g., Smiling). Most of existing fairness-aware methods rely on the\nlabels of spurious attributes, which may be unavailable in practice. This work\nproposes a novel, generation-based two-stage framework to train a fair FAC\nmodel on biased data without additional annotation. Initially, we identify the\npotential spurious attributes based on generative models. Notably, it enhances\ninterpretability by explicitly showing the spurious attributes in image space.\nFollowing this, for each image, we first edit the spurious attributes with a\nrandom degree sampled from a uniform distribution, while keeping target\nattribute unchanged. Then we train a fair FAC model by fostering model\ninvariance to these augmentation. Extensive experiments on three common\ndatasets demonstrate the effectiveness of our method in promoting fairness in\nFAC without compromising accuracy. Codes are in\nhttps://github.com/heqianpei/DiGA.\n"
    },
    {
        "title": "Cross-domain and Cross-dimension Learning for Image-to-Graph\n  Transformers",
        "published_time": "2024-03-11T10:48:56Z",
        "abstract": "  Direct image-to-graph transformation is a challenging task that solves object\ndetection and relationship prediction in a single model. Due to the complexity\nof this task, large training datasets are rare in many domains, which makes the\ntraining of large networks challenging. This data sparsity necessitates the\nestablishment of pre-training strategies akin to the state-of-the-art in\ncomputer vision. In this work, we introduce a set of methods enabling\ncross-domain and cross-dimension transfer learning for image-to-graph\ntransformers. We propose (1) a regularized edge sampling loss for sampling the\noptimal number of object relationships (edges) across domains, (2) a domain\nadaptation framework for image-to-graph transformers that aligns features from\ndifferent domains, and (3) a simple projection function that allows us to\npretrain 3D transformers on 2D input data. We demonstrate our method's utility\nin cross-domain and cross-dimension experiments, where we pretrain our models\non 2D satellite images before applying them to vastly different target domains\nin 2D and 3D. Our method consistently outperforms a series of baselines on\nchallenging benchmarks, such as retinal or whole-brain vessel graph extraction.\n"
    },
    {
        "title": "BEV2PR: BEV-Enhanced Visual Place Recognition with Structural Cues",
        "published_time": "2024-03-11T10:46:43Z",
        "abstract": "  In this paper, we propose a new image-based visual place recognition (VPR)\nframework by exploiting the structural cues in bird's-eye view (BEV) from a\nsingle monocular camera. The motivation arises from two key observations about\nVPR: 1) For the methods based on both camera and LiDAR sensors, the integration\nof LiDAR in robotic systems has led to increased expenses, while the alignment\nof data between different sensors is also a major challenge. 2) Other\nimage-/camera-based methods, involving integrating RGB images and their derived\nvariants (e.g., pseudo depth images, pseudo 3D point clouds), exhibit several\nlimitations, such as the failure to effectively exploit the explicit spatial\nrelationships between different objects. To tackle the above issues, we design\na new BEV-enhanced VPR framework, nemely BEV2PR, which can generate a composite\ndescriptor with both visual cues and spatial awareness solely based on a single\ncamera. For the visual cues, any popular aggregation module for RGB global\nfeatures can be integrated into our framework. The key points lie in: 1) We use\nBEV segmentation features as an explicit source of structural knowledge in\nconstructing global features. 2) The lower layers of the pre-trained backbone\nfrom BEV map generation are shared for visual and structural streams in VPR,\nfacilitating the learning of fine-grained local features in the visual stream.\n3) The complementary visual features and structural features can jointly\nenhance VPR performance. Our BEV2PR framework enables consistent performance\nimprovements over several popular camera-based VPR aggregation modules when\nintegrating them. The experiments on our collected VPR-NuScenes dataset\ndemonstrate an absolute gain of 2.47% on Recall@1 for the strong Conv-AP\nbaseline to achieve the best performance in our setting, and notably, a 18.06%\ngain on the hard set.\n"
    },
    {
        "title": "Exploiting Style Latent Flows for Generalizing Deepfake Detection Video\n  Detection",
        "published_time": "2024-03-11T10:35:58Z",
        "abstract": "  This paper presents a new approach for the detection of fake videos, based on\nthe analysis of style latent vectors and their abnormal behavior in temporal\nchanges in the generated videos. We discovered that the generated facial videos\nsuffer from the temporal distinctiveness in the temporal changes of style\nlatent vectors, which are inevitable during the generation of temporally stable\nvideos with various facial expressions and geometric transformations. Our\nframework utilizes the StyleGRU module, trained by contrastive learning, to\nrepresent the dynamic properties of style latent vectors. Additionally, we\nintroduce a style attention module that integrates StyleGRU-generated features\nwith content-based features, enabling the detection of visual and temporal\nartifacts. We demonstrate our approach across various benchmark scenarios in\ndeepfake detection, showing its superiority in cross-dataset and\ncross-manipulation scenarios. Through further analysis, we also validate the\nimportance of using temporal changes of style latent vectors to improve the\ngenerality of deepfake video detection.\n"
    },
    {
        "title": "Transformer-based Fusion of 2D-pose and Spatio-temporal Embeddings for\n  Distracted Driver Action Recognition",
        "published_time": "2024-03-11T10:26:38Z",
        "abstract": "  Classification and localization of driving actions over time is important for\nadvanced driver-assistance systems and naturalistic driving studies. Temporal\nlocalization is challenging because it requires robustness, reliability, and\naccuracy. In this study, we aim to improve the temporal localization and\nclassification accuracy performance by adapting video action recognition and 2D\nhuman-pose estimation networks to one model. Therefore, we design a\ntransformer-based fusion architecture to effectively combine 2D-pose features\nand spatio-temporal features. The model uses 2D-pose features as the positional\nembedding of the transformer architecture and spatio-temporal features as the\nmain input to the encoder of the transformer. The proposed solution is generic\nand independent of the camera numbers and positions, giving frame-based class\nprobabilities as output. Finally, the post-processing step combines information\nfrom different camera views to obtain final predictions and eliminate false\npositives. The model performs well on the A2 test set of the 2023 NVIDIA AI\nCity Challenge for naturalistic driving action recognition, achieving the\noverlap score of the organizer-defined distracted driver behaviour metric of\n0.5079.\n"
    },
    {
        "title": "Leveraging Foundation Models for Content-Based Medical Image Retrieval\n  in Radiology",
        "published_time": "2024-03-11T10:06:45Z",
        "abstract": "  Content-based image retrieval (CBIR) has the potential to significantly\nimprove diagnostic aid and medical research in radiology. Current CBIR systems\nface limitations due to their specialization to certain pathologies, limiting\ntheir utility. In response, we propose using vision foundation models as\npowerful and versatile off-the-shelf feature extractors for content-based\nmedical image retrieval. By benchmarking these models on a comprehensive\ndataset of 1.6 million 2D radiological images spanning four modalities and 161\npathologies, we identify weakly-supervised models as superior, achieving a P@1\nof up to 0.594. This performance not only competes with a specialized model but\ndoes so without the need for fine-tuning. Our analysis further explores the\nchallenges in retrieving pathological versus anatomical structures, indicating\nthat accurate retrieval of pathological features presents greater difficulty.\nDespite these challenges, our research underscores the vast potential of\nfoundation models for CBIR in radiology, proposing a shift towards versatile,\ngeneral-purpose medical image retrieval systems that do not require specific\ntuning.\n"
    },
    {
        "title": "Detection of Object Throwing Behavior in Surveillance Videos",
        "published_time": "2024-03-11T09:53:19Z",
        "abstract": "  Anomalous behavior detection is a challenging research area within computer\nvision. Progress in this area enables automated detection of dangerous behavior\nusing surveillance camera feeds. A dangerous behavior that is often overlooked\nin other research is the throwing action in traffic flow, which is one of the\nunique requirements of our Smart City project to enhance public safety. This\npaper proposes a solution for throwing action detection in surveillance videos\nusing deep learning. At present, datasets for throwing actions are not publicly\navailable. To address the use-case of our Smart City project, we first generate\nthe novel public 'Throwing Action' dataset, consisting of 271 videos of\nthrowing actions performed by traffic participants, such as pedestrians,\nbicyclists, and car drivers, and 130 normal videos without throwing actions.\nSecond, we compare the performance of different feature extractors for our\nanomaly detection method on the UCF-Crime and Throwing-Action datasets. The\nexplored feature extractors are the Convolutional 3D (C3D) network, the\nInflated 3D ConvNet (I3D) network, and the Multi-Fiber Network (MFNet).\nFinally, the performance of the anomaly detection algorithm is improved by\napplying the Adam optimizer instead of Adadelta, and proposing a mean normal\nloss function that covers the multitude of normal situations in traffic. Both\naspects yield better anomaly detection performance. Besides this, the proposed\nmean normal loss function lowers the false alarm rate on the combined dataset.\nThe experimental results reach an area under the ROC curve of 86.10 for the\nThrowing-Action dataset, and 80.13 on the combined dataset, respectively.\n"
    },
    {
        "title": "OMH: Structured Sparsity via Optimally Matched Hierarchy for\n  Unsupervised Semantic Segmentation",
        "published_time": "2024-03-11T09:46:41Z",
        "abstract": "  Unsupervised Semantic Segmentation (USS) involves segmenting images without\nrelying on predefined labels, aiming to alleviate the burden of extensive human\nlabeling. Existing methods utilize features generated by self-supervised models\nand specific priors for clustering. However, their clustering objectives are\nnot involved in the optimization of the features during training. Additionally,\ndue to the lack of clear class definitions in USS, the resulting segments may\nnot align well with the clustering objective. In this paper, we introduce a\nnovel approach called Optimally Matched Hierarchy (OMH) to simultaneously\naddress the above issues. The core of our method lies in imposing structured\nsparsity on the feature space, which allows the features to encode information\nwith different levels of granularity. The structure of this sparsity stems from\nour hierarchy (OMH). To achieve this, we learn a soft but sparse hierarchy\namong parallel clusters through Optimal Transport. Our OMH yields better\nunsupervised segmentation performance compared to existing USS methods. Our\nextensive experiments demonstrate the benefits of OMH when utilizing our\ndifferentiable paradigm. We will make our code publicly available.\n"
    },
    {
        "title": "ReStainGAN: Leveraging IHC to IF Stain Domain Translation for in-silico\n  Data Generation",
        "published_time": "2024-03-11T09:45:34Z",
        "abstract": "  The creation of in-silico datasets can expand the utility of existing\nannotations to new domains with different staining patterns in computational\npathology. As such, it has the potential to significantly lower the cost\nassociated with building large and pixel precise datasets needed to train\nsupervised deep learning models. We propose a novel approach for the generation\nof in-silico immunohistochemistry (IHC) images by disentangling morphology\nspecific IHC stains into separate image channels in immunofluorescence (IF)\nimages. The proposed approach qualitatively and quantitatively outperforms\nbaseline methods as proven by training nucleus segmentation models on the\ncreated in-silico datasets.\n"
    },
    {
        "title": "Fooling Neural Networks for Motion Forecasting via Adversarial Attacks",
        "published_time": "2024-03-07T23:44:10Z",
        "abstract": "  Human motion prediction is still an open problem, which is extremely\nimportant for autonomous driving and safety applications. Although there are\ngreat advances in this area, the widely studied topic of adversarial attacks\nhas not been applied to multi-regression models such as GCNs and MLP-based\narchitectures in human motion prediction. This work intends to reduce this gap\nusing extensive quantitative and qualitative experiments in state-of-the-art\narchitectures similar to the initial stages of adversarial attacks in image\nclassification. The results suggest that models are susceptible to attacks even\non low levels of perturbation. We also show experiments with 3D transformations\nthat affect the model performance, in particular, we show that most models are\nsensitive to simple rotations and translations which do not alter joint\ndistances. We conclude that similar to earlier CNN models, motion forecasting\ntasks are susceptible to small perturbations and simple 3D transformations.\n"
    },
    {
        "title": "3DRef: 3D Dataset and Benchmark for Reflection Detection in RGB and\n  Lidar Data",
        "published_time": "2024-03-11T09:29:44Z",
        "abstract": "  Reflective surfaces present a persistent challenge for reliable 3D mapping\nand perception in robotics and autonomous systems. However, existing reflection\ndatasets and benchmarks remain limited to sparse 2D data. This paper introduces\nthe first large-scale 3D reflection detection dataset containing more than\n50,000 aligned samples of multi-return Lidar, RGB images, and 2D/3D semantic\nlabels across diverse indoor environments with various reflections. Textured 3D\nground truth meshes enable automatic point cloud labeling to provide precise\nground truth annotations. Detailed benchmarks evaluate three Lidar point cloud\nsegmentation methods, as well as current state-of-the-art image segmentation\nnetworks for glass and mirror detection. The proposed dataset advances\nreflection detection by providing a comprehensive testbed with precise global\nalignment, multi-modal data, and diverse reflective objects and materials. It\nwill drive future research towards reliable reflection detection. The dataset\nis publicly available at http://3dref.github.io\n"
    },
    {
        "title": "Multi-Scale Implicit Transformer with Re-parameterize for\n  Arbitrary-Scale Super-Resolution",
        "published_time": "2024-03-11T09:23:20Z",
        "abstract": "  Recently, the methods based on implicit neural representations have shown\nexcellent capabilities for arbitrary-scale super-resolution (ASSR). Although\nthese methods represent the features of an image by generating latent codes,\nthese latent codes are difficult to adapt for different magnification factors\nof super-resolution, which seriously affects their performance. Addressing\nthis, we design Multi-Scale Implicit Transformer (MSIT), consisting of an\nMulti-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among\nthem, MSNO obtains multi-scale latent codes through feature enhancement,\nmulti-scale characteristics extraction, and multi-scale characteristics\nmerging. MSSA further enhances the multi-scale characteristics of latent codes,\nresulting in better performance. Furthermore, to improve the performance of\nnetwork, we propose the Re-Interaction Module (RIM) combined with the\ncumulative training strategy to improve the diversity of learned information\nfor the network. We have systematically introduced multi-scale characteristics\nfor the first time in ASSR, extensive experiments are performed to validate the\neffectiveness of MSIT, and our method achieves state-of-the-art performance in\narbitrary super-resolution tasks.\n"
    },
    {
        "title": "SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale\n  SAR Object Detection",
        "published_time": "2024-03-11T09:20:40Z",
        "abstract": "  Synthetic Aperture Radar (SAR) object detection has gained significant\nattention recently due to its irreplaceable all-weather imaging capabilities.\nHowever, this research field suffers from both limited public datasets (mostly\ncomprising <2K images with only mono-category objects) and inaccessible source\ncode. To tackle these challenges, we establish a new benchmark dataset and an\nopen-source method for large-scale SAR object detection. Our dataset,\nSARDet-100K, is a result of intense surveying, collecting, and standardizing 10\nexisting SAR detection datasets, providing a large-scale and diverse dataset\nfor research purposes. To the best of our knowledge, SARDet-100K is the first\nCOCO-level large-scale multi-class SAR object detection dataset ever created.\nWith this high-quality dataset, we conducted comprehensive experiments and\nuncovered a crucial challenge in SAR object detection: the substantial\ndisparities between the pretraining on RGB datasets and finetuning on SAR\ndatasets in terms of both data domain and model structure. To bridge these\ngaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA)\npretraining framework that tackles the problems from the perspective of data\ninput, domain transition, and model migration. The proposed MSFA method\nsignificantly enhances the performance of SAR object detection models while\ndemonstrating exceptional generalizability and flexibility across diverse\nmodels. This work aims to pave the way for further advancements in SAR object\ndetection. The dataset and code is available at\nhttps://github.com/zcablii/SARDet_100K.\n"
    },
    {
        "title": "Reconstructing Visual Stimulus Images from EEG Signals Based on Deep\n  Visual Representation Model",
        "published_time": "2024-03-11T09:19:09Z",
        "abstract": "  Reconstructing visual stimulus images is a significant task in neural\ndecoding, and up to now, most studies consider the functional magnetic\nresonance imaging (fMRI) as the signal source. However, the fMRI-based image\nreconstruction methods are difficult to widely applied because of the\ncomplexity and high cost of the acquisition equipments. Considering the\nadvantages of low cost and easy portability of the electroencephalogram (EEG)\nacquisition equipments, we propose a novel image reconstruction method based on\nEEG signals in this paper. Firstly, to satisfy the high recognizability of\nvisual stimulus images in fast switching manner, we build a visual stimuli\nimage dataset, and obtain the EEG dataset by a corresponding EEG signals\ncollection experiment. Secondly, the deep visual representation model(DVRM)\nconsisting of a primary encoder and a subordinate decoder is proposed to\nreconstruct visual stimuli. The encoder is designed based on the\nresidual-in-residual dense blocks to learn the distribution characteristics\nbetween EEG signals and visual stimulus images, while the decoder is designed\nbased on the deep neural network to reconstruct the visual stimulus image from\nthe learned deep visual representation. The DVRM can fit the deep and multiview\nvisual features of human natural state and make the reconstructed images more\nprecise. Finally, we evaluate the DVRM in the quality of the generated images\non our EEG dataset. The results show that the DVRM have good performance in the\ntask of learning deep visual representation from EEG signals and generating\nreconstructed images that are realistic and highly resemble the original\nimages.\n"
    },
    {
        "title": "Confidence-Aware RGB-D Face Recognition via Virtual Depth Synthesis",
        "published_time": "2024-03-11T09:12:24Z",
        "abstract": "  2D face recognition encounters challenges in unconstrained environments due\nto varying illumination, occlusion, and pose. Recent studies focus on RGB-D\nface recognition to improve robustness by incorporating depth information.\nHowever, collecting sufficient paired RGB-D training data is expensive and\ntime-consuming, hindering wide deployment. In this work, we first construct a\ndiverse depth dataset generated by 3D Morphable Models for depth model\npre-training. Then, we propose a domain-independent pre-training framework that\nutilizes readily available pre-trained RGB and depth models to separately\nperform face recognition without needing additional paired data for retraining.\nTo seamlessly integrate the two distinct networks and harness the complementary\nbenefits of RGB and depth information for improved accuracy, we propose an\ninnovative Adaptive Confidence Weighting (ACW). This mechanism is designed to\nlearn confidence estimates for each modality to achieve modality fusion at the\nscore level. Our method is simple and lightweight, only requiring ACW training\nbeyond the backbone models. Experiments on multiple public RGB-D face\nrecognition benchmarks demonstrate state-of-the-art performance surpassing\nprevious methods based on depth estimation and feature fusion, validating the\nefficacy of our approach.\n"
    },
    {
        "title": "Active Generation for Image Classification",
        "published_time": "2024-03-11T08:45:31Z",
        "abstract": "  Recently, the growing capabilities of deep generative models have underscored\ntheir potential in enhancing image classification accuracy. However, existing\nmethods often demand the generation of a disproportionately large number of\nimages compared to the original dataset, while having only marginal\nimprovements in accuracy. This computationally expensive and time-consuming\nprocess hampers the practicality of such approaches. In this paper, we propose\nto address the efficiency of image generation by focusing on the specific needs\nand characteristics of the model. With a central tenet of active learning, our\nmethod, named ActGen, takes a training-aware approach to image generation. It\naims to create images akin to the challenging or misclassified samples\nencountered by the current model and incorporates these generated images into\nthe training set to augment model performance. ActGen introduces an attentive\nimage guidance technique, using real images as guides during the denoising\nprocess of a diffusion model. The model's attention on class prompt is\nleveraged to ensure the preservation of similar foreground object while\ndiversifying the background. Furthermore, we introduce a gradient-based\ngeneration guidance method, which employs two losses to generate more\nchallenging samples and prevent the generated images from being too similar to\npreviously generated ones. Experimental results on the CIFAR and ImageNet\ndatasets demonstrate that our method achieves better performance with a\nsignificantly reduced number of generated images.\n"
    },
    {
        "title": "Advancing Text-Driven Chest X-Ray Generation with Policy-Based\n  Reinforcement Learning",
        "published_time": "2024-03-11T08:43:57Z",
        "abstract": "  Recent advances in text-conditioned image generation diffusion models have\nbegun paving the way for new opportunities in modern medical domain, in\nparticular, generating Chest X-rays (CXRs) from diagnostic reports.\nNonetheless, to further drive the diffusion models to generate CXRs that\nfaithfully reflect the complexity and diversity of real data, it has become\nevident that a nontrivial learning approach is needed. In light of this, we\npropose CXRL, a framework motivated by the potential of reinforcement learning\n(RL). Specifically, we integrate a policy gradient RL approach with\nwell-designed multiple distinctive CXR-domain specific reward models. This\napproach guides the diffusion denoising trajectory, achieving precise CXR\nposture and pathological details. Here, considering the complex medical image\nenvironment, we present \"RL with Comparative Feedback\" (RLCF) for the reward\nmechanism, a human-like comparative evaluation that is known to be more\neffective and reliable in complex scenarios compared to direct evaluation. Our\nCXRL framework includes jointly optimizing learnable adaptive condition\nembeddings (ACE) and the image generator, enabling the model to produce more\naccurate and higher perceptual CXR quality. Our extensive evaluation of the\nMIMIC-CXR-JPG dataset demonstrates the effectiveness of our RL-based tuning\napproach. Consequently, our CXRL generates pathologically realistic CXRs,\nestablishing a new standard for generating CXRs with high fidelity to\nreal-world clinical scenarios.\n"
    },
    {
        "title": "Structure Your Data: Towards Semantic Graph Counterfactuals",
        "published_time": "2024-03-11T08:40:37Z",
        "abstract": "  Counterfactual explanations (CEs) based on concepts are explanations that\nconsider alternative scenarios to understand which high-level semantic features\ncontributed to particular model predictions. In this work, we propose CEs based\non the semantic graphs accompanying input data to achieve more descriptive,\naccurate, and human-aligned explanations. Building upon state-of-the-art (SoTA)\nconceptual attempts, we adopt a model-agnostic edit-based approach and\nintroduce leveraging GNNs for efficient Graph Edit Distance (GED) computation.\nWith a focus on the visual domain, we represent images as scene graphs and\nobtain their GNN embeddings to bypass solving the NP-hard graph similarity\nproblem for all input pairs, an integral part of the CE computation process. We\napply our method to benchmark and real-world datasets with varying difficulty\nand availability of semantic annotations. Testing on diverse classifiers, we\nfind that our CEs outperform previous SoTA explanation models based on\nsemantics, including both white and black-box as well as conceptual and\npixel-level approaches. Their superiority is proven quantitatively and\nqualitatively, as validated by human subjects, highlighting the significance of\nleveraging semantic edges in the presence of intricate relationships. Our\nmodel-agnostic graph-based approach is widely applicable and easily extensible,\nproducing actionable explanations across different contexts.\n"
    },
    {
        "title": "Skeleton Supervised Airway Segmentation",
        "published_time": "2024-03-11T08:37:03Z",
        "abstract": "  Fully-supervised airway segmentation has accomplished significant triumphs\nover the years in aiding pre-operative diagnosis and intra-operative\nnavigation. However, full voxel-level annotation constitutes a labor-intensive\nand time-consuming task, often plagued by issues such as missing branches,\nbranch annotation discontinuity, or erroneous edge delineation. label-efficient\nsolutions for airway extraction are rarely explored yet primarily demanding in\nmedical practice. To this end, we introduce a novel skeleton-level annotation\n(SkA) tailored to the airway, which simplifies the annotation workflow while\nenhancing annotation consistency and accuracy, preserving the complete\ntopology. Furthermore, we propose a skeleton-supervised learning framework to\nachieve accurate airway segmentation. Firstly, a dual-stream buffer inference\nis introduced to realize initial label propagation from SkA, avoiding the\ncollapse of direct learning from SkA. Then, we construct a geometry-aware\ndual-path propagation framework (GDP) to further promote complementary\npropagation learning, composed of hard geometry-aware propagation learning and\nsoft geometry-aware propagation guidance. Experiments reveal that our proposed\nframework outperforms the competing methods with SKA, which amounts to only\n1.96% airways, and achieves comparable performance with the baseline model that\nis fully supervised with 100% airways, demonstrating its significant potential\nin achieving label-efficient segmentation for other tubular structures, such as\nvessels.\n"
    },
    {
        "title": "Vosh: Voxel-Mesh Hybrid Representation for Real-Time View Synthesis",
        "published_time": "2024-03-11T08:28:51Z",
        "abstract": "  The neural radiance field (NeRF) has emerged as a prominent methodology for\nsynthesizing realistic images of novel views. While neural radiance\nrepresentations based on voxels or mesh individually offer distinct advantages,\nexcelling in either rendering quality or speed, each has limitations in the\nother aspect. In response, we propose a pioneering hybrid representation named\nVosh, seamlessly combining both voxel and mesh components in hybrid rendering\nfor view synthesis. Vosh is meticulously crafted by optimizing the voxel grid\nof NeRF, strategically with selected voxels replaced by mesh. Therefore, it\nexcels in fast rendering scenes with simple geometry and textures through its\nmesh component, while simultaneously enabling high-quality rendering in\nintricate regions by leveraging voxel component. The flexibility of Vosh is\nshowcased through the ability to adjust hybrid ratios, providing users the\nability to control the balance between rendering quality and speed based on\nflexible usage. Experimental results demonstrates that our method achieves\ncommendable trade-off between rendering quality and speed, and notably has\nreal-time performance on mobile devices.\n"
    },
    {
        "title": "3D Semantic Segmentation-Driven Representations for 3D Object Detection",
        "published_time": "2024-03-11T08:17:56Z",
        "abstract": "  In autonomous driving, 3D detection provides more precise information to\ndownstream tasks, including path planning and motion estimation, compared to 2D\ndetection. Therefore, the need for 3D detection research has emerged. However,\nalthough single and multi-view images and depth maps obtained from the camera\nwere used, detection accuracy was relatively low compared to other\nmodality-based detectors due to the lack of geometric information. The proposed\nmulti-modal 3D object detection combines semantic features obtained from images\nand geometric features obtained from point clouds, but there are difficulties\nin defining unified representation to fuse data existing in different domains\nand synchronization between them. In this paper, we propose SeSame : point-wise\nsemantic feature as a new presentation to ensure sufficient semantic\ninformation of the existing LiDAR-only based 3D detection. Experiments show\nthat our approach outperforms previous state-of-the-art at different levels of\ndifficulty in car and performance improvement on the KITTI object detection\nbenchmark. Our code is available at https://github.com/HAMA-DL-dev/SeSame\n"
    },
    {
        "title": "A Converting Autoencoder Toward Low-latency and Energy-efficient DNN\n  Inference at the Edge",
        "published_time": "2024-03-11T08:13:42Z",
        "abstract": "  Reducing inference time and energy usage while maintaining prediction\naccuracy has become a significant concern for deep neural networks (DNN)\ninference on resource-constrained edge devices. To address this problem, we\npropose a novel approach based on \"converting\" autoencoder and lightweight\nDNNs. This improves upon recent work such as early-exiting framework and DNN\npartitioning. Early-exiting frameworks spend different amounts of computation\npower for different input data depending upon their complexity. However, they\ncan be inefficient in real-world scenarios that deal with many hard image\nsamples. On the other hand, DNN partitioning algorithms that utilize the\ncomputation power of both the cloud and edge devices can be affected by network\ndelays and intermittent connections between the cloud and the edge. We present\nCBNet, a low-latency and energy-efficient DNN inference framework tailored for\nedge devices. It utilizes a \"converting\" autoencoder to efficiently transform\nhard images into easy ones, which are subsequently processed by a lightweight\nDNN for inference. To the best of our knowledge, such autoencoder has not been\nproposed earlier. Our experimental results using three popular\nimage-classification datasets on a Raspberry Pi 4, a Google Cloud instance, and\nan instance with Nvidia Tesla K80 GPU show that CBNet achieves up to 4.8x\nspeedup in inference latency and 79% reduction in energy usage compared to\ncompeting techniques while maintaining similar or higher accuracy.\n"
    },
    {
        "title": "Incorporating Improved Sinusoidal Threshold-based Semi-supervised Method\n  and Diffusion Models for Osteoporosis Diagnosis",
        "published_time": "2024-03-11T08:11:46Z",
        "abstract": "  Osteoporosis is a common skeletal disease that seriously affects patients'\nquality of life. Traditional osteoporosis diagnosis methods are expensive and\ncomplex. The semi-supervised model based on diffusion model and class threshold\nsinusoidal decay proposed in this paper can automatically diagnose osteoporosis\nbased on patient's imaging data, which has the advantages of convenience,\naccuracy, and low cost. Unlike previous semi-supervised models, all the\nunlabeled data used in this paper are generated by the diffusion model.\nCompared with real unlabeled data, synthetic data generated by the diffusion\nmodel show better performance. In addition, this paper proposes a novel\npseudo-label threshold adjustment mechanism, Sinusoidal Threshold Decay, which\ncan make the semi-supervised model converge more quickly and improve its\nperformance. Specifically, the method is tested on a dataset including 749\ndental panoramic images, and its achieved leading detect performance and\nproduces a 80.10% accuracy.\n"
    },
    {
        "title": "QuantTune: Optimizing Model Quantization with Adaptive Outlier-Driven\n  Fine Tuning",
        "published_time": "2024-03-11T08:09:30Z",
        "abstract": "  Transformer-based models have gained widespread popularity in both the\ncomputer vision (CV) and natural language processing (NLP) fields. However,\nsignificant challenges arise during post-training linear quantization, leading\nto noticeable reductions in inference accuracy. Our study focuses on uncovering\nthe underlying causes of these accuracy drops and proposing a\nquantization-friendly fine-tuning method, \\textbf{QuantTune}. Firstly, our\nanalysis revealed that, on average, 65\\% of quantization errors result from the\nprecision loss incurred by the dynamic range amplification effect of outliers\nacross the target Transformer-based models. Secondly, \\textbf{QuantTune}\nadjusts weights based on the deviation of outlier activations and effectively\nconstrains the dynamic ranges of the problematic activations. As a result, it\nsuccessfully mitigates the negative impact of outliers on the inference\naccuracy of quantized models. Lastly, \\textbf{QuantTune} can be seamlessly\nintegrated into the back-propagation pass in the fine-tuning process without\nrequiring extra complexity in inference software and hardware design. Our\napproach showcases significant improvements in post-training quantization\nacross a range of Transformer-based models, including ViT, Bert-base, and OPT.\nQuantTune reduces accuracy drops by 12.09\\% at 8-bit quantization and 33.8\\% at\n7-bit compared to top calibration methods, outperforming state-of-the-art\nsolutions by over 18.84\\% across ViT models.\n"
    },
    {
        "title": "Query-guided Prototype Evolution Network for Few-Shot Segmentation",
        "published_time": "2024-03-11T07:50:40Z",
        "abstract": "  Previous Few-Shot Segmentation (FSS) approaches exclusively utilize support\nfeatures for prototype generation, neglecting the specific requirements of the\nquery. To address this, we present the Query-guided Prototype Evolution Network\n(QPENet), a new method that integrates query features into the generation\nprocess of foreground and background prototypes, thereby yielding customized\nprototypes attuned to specific queries. The evolution of the foreground\nprototype is accomplished through a \\textit{support-query-support} iterative\nprocess involving two new modules: Pseudo-prototype Generation (PPG) and Dual\nPrototype Evolution (DPE). The PPG module employs support features to create an\ninitial prototype for the preliminary segmentation of the query image,\nresulting in a pseudo-prototype reflecting the unique needs of the current\nquery. Subsequently, the DPE module performs reverse segmentation on support\nimages using this pseudo-prototype, leading to the generation of evolved\nprototypes, which can be considered as custom solutions. As for the background\nprototype, the evolution begins with a global background prototype that\nrepresents the generalized features of all training images. We also design a\nGlobal Background Cleansing (GBC) module to eliminate potential adverse\ncomponents mirroring the characteristics of the current foreground class.\nExperimental results on the PASCAL-$5^i$ and COCO-$20^i$ datasets attest to the\nsubstantial enhancements achieved by QPENet over prevailing state-of-the-art\ntechniques, underscoring the validity of our ideas.\n"
    },
    {
        "title": "Ada-Tracker: Soft Tissue Tracking via Inter-Frame and Adaptive-Template\n  Matching",
        "published_time": "2024-03-11T07:42:40Z",
        "abstract": "  Soft tissue tracking is crucial for computer-assisted interventions. Existing\napproaches mainly rely on extracting discriminative features from the template\nand videos to recover corresponding matches. However, it is difficult to adopt\nthese techniques in surgical scenes, where tissues are changing in shape and\nappearance throughout the surgery. To address this problem, we exploit optical\nflow to naturally capture the pixel-wise tissue deformations and adaptively\ncorrect the tracked template. Specifically, we first implement an inter-frame\nmatching mechanism to extract a coarse region of interest based on optical flow\nfrom consecutive frames. To accommodate appearance change and alleviate drift,\nwe then propose an adaptive-template matching method, which updates the tracked\ntemplate based on the reliability of the estimates. Our approach, Ada-Tracker,\nenjoys both short-term dynamics modeling by capturing local deformations and\nlong-term dynamics modeling by introducing global temporal compensation. We\nevaluate our approach on the public SurgT benchmark, which is generated from\nHamlyn, SCARED, and Kidney boundary datasets. The experimental results show\nthat Ada-Tracker achieves superior accuracy and performs more robustly against\nprior works. Code is available at https://github.com/wrld/Ada-Tracker.\n"
    },
    {
        "title": "Toward Robust Canine Cardiac Diagnosis: Deep Prototype Alignment\n  Network-Based Few-Shot Segmentation in Veterinary Medicine",
        "published_time": "2024-03-11T07:19:29Z",
        "abstract": "  In the cutting-edge domain of medical artificial intelligence (AI),\nremarkable advances have been achieved in areas such as diagnosis, prediction,\nand therapeutic interventions. Despite these advances, the technology for image\nsegmentation faces the significant barrier of having to produce extensively\nannotated datasets. To address this challenge, few-shot segmentation (FSS) has\nbeen recognized as one of the innovative solutions. Although most of the FSS\nresearch has focused on human health care, its application in veterinary\nmedicine, particularly for pet care, remains largely limited. This study has\nfocused on accurate segmentation of the heart and left atrial enlargement on\ncanine chest radiographs using the proposed deep prototype alignment network\n(DPANet). The PANet architecture is adopted as the backbone model, and\nexperiments are conducted using various encoders based on VGG-19, ResNet-18,\nand ResNet-50 to extract features. Experimental results demonstrate that the\nproposed DPANet achieves the highest performance. In the 2way-1shot scenario,\nit achieves the highest intersection over union (IoU) value of 0.6966, and in\nthe 2way-5shot scenario, it achieves the highest IoU value of 0.797. The DPANet\nnot only signifies a performance improvement, but also shows an improved\ntraining speed in the 2way-5shot scenario. These results highlight our model's\nexceptional capability as a trailblazing solution for segmenting the heart and\nleft atrial enlargement in veterinary applications through FSS, setting a new\nbenchmark in veterinary AI research, and demonstrating its superior potential\nto veterinary medicine advances.\n"
    },
    {
        "title": "LightM-UNet: Mamba Assists in Lightweight UNet for Medical Image\n  Segmentation",
        "published_time": "2024-03-08T12:07:42Z",
        "abstract": "  UNet and its variants have been widely used in medical image segmentation.\nHowever, these models, especially those based on Transformer architectures,\npose challenges due to their large number of parameters and computational\nloads, making them unsuitable for mobile health applications. Recently, State\nSpace Models (SSMs), exemplified by Mamba, have emerged as competitive\nalternatives to CNN and Transformer architectures. Building upon this, we\nemploy Mamba as a lightweight substitute for CNN and Transformer within UNet,\naiming at tackling challenges stemming from computational resource limitations\nin real medical settings. To this end, we introduce the Lightweight Mamba UNet\n(LightM-UNet) that integrates Mamba and UNet in a lightweight framework.\nSpecifically, LightM-UNet leverages the Residual Vision Mamba Layer in a pure\nMamba fashion to extract deep semantic features and model long-range spatial\ndependencies, with linear computational complexity. Extensive experiments\nconducted on two real-world 2D/3D datasets demonstrate that LightM-UNet\nsurpasses existing state-of-the-art literature. Notably, when compared to the\nrenowned nnU-Net, LightM-UNet achieves superior segmentation performance while\ndrastically reducing parameter and computation costs by 116x and 21x,\nrespectively. This highlights the potential of Mamba in facilitating model\nlightweighting. Our code implementation is publicly available at\nhttps://github.com/MrBlankness/LightM-UNet.\n"
    },
    {
        "title": "3D-aware Image Generation and Editing with Multi-modal Conditions",
        "published_time": "2024-03-11T07:10:37Z",
        "abstract": "  3D-consistent image generation from a single 2D semantic label is an\nimportant and challenging research topic in computer graphics and computer\nvision. Although some related works have made great progress in this field,\nmost of the existing methods suffer from poor disentanglement performance of\nshape and appearance, and lack multi-modal control. In this paper, we propose a\nnovel end-to-end 3D-aware image generation and editing model incorporating\nmultiple types of conditional inputs, including pure noise, text and reference\nimage. On the one hand, we dive into the latent space of 3D Generative\nAdversarial Networks (GANs) and propose a novel disentanglement strategy to\nseparate appearance features from shape features during the generation process.\nOn the other hand, we propose a unified framework for flexible image generation\nand editing tasks with multi-modal conditions. Our method can generate diverse\nimages with distinct noises, edit the attribute through a text description and\nconduct style transfer by giving a reference RGB image. Extensive experiments\ndemonstrate that the proposed method outperforms alternative approaches both\nqualitatively and quantitatively on image generation and editing.\n"
    },
    {
        "title": "Point Mamba: A Novel Point Cloud Backbone Based on State Space Model\n  with Octree-Based Ordering Strategy",
        "published_time": "2024-03-11T07:07:39Z",
        "abstract": "  Recently, state space model (SSM) has gained great attention due to its\npromising performance, linear complexity, and long sequence modeling ability in\nboth language and image domains. However, it is non-trivial to extend SSM to\nthe point cloud field, because of the causality requirement of SSM and the\ndisorder and irregularity nature of point clouds. In this paper, we propose a\nnovel SSM-based point cloud processing backbone, named Point Mamba, with a\ncausality-aware ordering mechanism. To construct the causal dependency\nrelationship, we design an octree-based ordering strategy on raw irregular\npoints, globally sorting points in a z-order sequence and also retaining their\nspatial proximity. Our method achieves state-of-the-art performance compared\nwith transformer-based counterparts, with 93.4% accuracy and 75.7 mIOU\nrespectively on the ModelNet40 classification dataset and ScanNet semantic\nsegmentation dataset. Furthermore, our Point Mamba has linear complexity, which\nis more efficient than transformer-based methods. Our method demonstrates the\ngreat potential that SSM can serve as a generic backbone in point cloud\nunderstanding. Codes are released at https://github.com/IRMVLab/Point-Mamba.\n"
    },
    {
        "title": "Group Distributionally Robust Dataset Distillation with Risk\n  Minimization",
        "published_time": "2024-02-07T09:03:04Z",
        "abstract": "  Dataset distillation (DD) has emerged as a widely adopted technique for\ncrafting a synthetic dataset that captures the essential information of a\ntraining dataset, facilitating the training of accurate neural models. Its\napplications span various domains, including transfer learning, federated\nlearning, and neural architecture search. The most popular methods for\nconstructing the synthetic data rely on matching the convergence properties of\ntraining the model with the synthetic dataset and the training dataset.\nHowever, targeting the training dataset must be thought of as auxiliary in the\nsame sense that the training set is an approximate substitute for the\npopulation distribution, and the latter is the data of interest. Yet despite\nits popularity, an aspect that remains unexplored is the relationship of DD to\nits generalization, particularly across uncommon subgroups. That is, how can we\nensure that a model trained on the synthetic dataset performs well when faced\nwith samples from regions with low population density? Here, the\nrepresentativeness and coverage of the dataset become salient over the\nguaranteed training error at inference. Drawing inspiration from\ndistributionally robust optimization, we introduce an algorithm that combines\nclustering with the minimization of a risk measure on the loss to conduct DD.\nWe provide a theoretical rationale for our approach and demonstrate its\neffective generalization and robustness across subgroups through numerical\nexperiments. The source code is available in\nhttps://github.com/Mming11/RobustDatasetDistillation.\n"
    },
    {
        "title": "Reliable Spatial-Temporal Voxels For Multi-Modal Test-Time Adaptation",
        "published_time": "2024-03-11T06:56:08Z",
        "abstract": "  Multi-modal test-time adaptation (MM-TTA) is proposed to adapt models to an\nunlabeled target domain by leveraging the complementary multi-modal inputs in\nan online manner. Previous MM-TTA methods rely on predictions of cross-modal\ninformation in each input frame, while they ignore the fact that predictions of\ngeometric neighborhoods within consecutive frames are highly correlated,\nleading to unstable predictions across time. To fulfill this gap, we propose\nReLiable Spatial-temporal Voxels (Latte), an MM-TTA method that leverages\nreliable cross-modal spatial-temporal correspondences for multi-modal 3D\nsegmentation. Motivated by the fact that reliable predictions should be\nconsistent with their spatial-temporal correspondences, Latte aggregates\nconsecutive frames in a slide window manner and constructs ST voxel to capture\ntemporally local prediction consistency for each modality. After filtering out\nST voxels with high ST entropy, Latte conducts cross-modal learning for each\npoint and pixel by attending to those with reliable and consistent predictions\namong both spatial and temporal neighborhoods. Experimental results show that\nLatte achieves state-of-the-art performance on three different MM-TTA\nbenchmarks compared to previous MM-TTA or TTA methods.\n"
    },
    {
        "title": "From Pixel to Cancer: Cellular Automata in Computed Tomography",
        "published_time": "2024-03-11T06:46:31Z",
        "abstract": "  AI for cancer detection encounters the bottleneck of data scarcity,\nannotation difficulty, and low prevalence of early tumors. Tumor synthesis\nseeks to create artificial tumors in medical images, which can greatly\ndiversify the data and annotations for AI training. However, current tumor\nsynthesis approaches are not applicable across different organs due to their\nneed for specific expertise and design. This paper establishes a set of generic\nrules to simulate tumor development. Each cell (pixel) is initially assigned a\nstate between zero and ten to represent the tumor population, and a tumor can\nbe developed based on three rules to describe the process of growth, invasion,\nand death. We apply these three generic rules to simulate tumor\ndevelopment--from pixel to cancer--using cellular automata. We then integrate\nthe tumor state into the original computed tomography (CT) images to generate\nsynthetic tumors across different organs. This tumor synthesis approach allows\nfor sampling tumors at multiple stages and analyzing tumor-organ interaction.\nClinically, a reader study involving three expert radiologists reveals that the\nsynthetic tumors and their developing trajectories are convincingly realistic.\nTechnically, we generate tumors at varied stages in 9,262 raw, unlabeled CT\nimages sourced from 68 hospitals worldwide. The performance in segmenting\ntumors in the liver, pancreas, and kidneys exceeds prevailing literature\nbenchmarks, underlining the immense potential of tumor synthesis, especially\nfor earlier cancer detection. The code and models are available at\nhttps://github.com/MrGiovanni/Pixel2Cancer\n"
    },
    {
        "title": "Learning the Unlearned: Mitigating Feature Suppression in Contrastive\n  Learning",
        "published_time": "2024-02-19T04:13:33Z",
        "abstract": "  Self-Supervised Contrastive Learning has proven effective in deriving\nhigh-quality representations from unlabeled data. However, a major challenge\nthat hinders both unimodal and multimodal contrastive learning is feature\nsuppression, a phenomenon where the trained model captures only a limited\nportion of the information from the input data while overlooking other\npotentially valuable content. This issue often leads to indistinguishable\nrepresentations for visually similar but semantically different inputs,\nadversely affecting downstream task performance, particularly those requiring\nrigorous semantic comprehension. To address this challenge, we propose a novel\nmodel-agnostic Multistage Contrastive Learning (MCL) framework. Unlike standard\ncontrastive learning which inherently captures one single biased feature\ndistribution, MCL progressively learns previously unlearned features through\nfeature-aware negative sampling at each stage, where the negative samples of an\nanchor are exclusively selected from the cluster it was assigned to in\npreceding stages. Meanwhile, MCL preserves the previously well-learned features\nby cross-stage representation integration, integrating features across all\nstages to form final representations. Our comprehensive evaluation demonstrates\nMCL's effectiveness and superiority across both unimodal and multimodal\ncontrastive learning, spanning a range of model architectures from ResNet to\nVision Transformers (ViT). Remarkably, in tasks where the original CLIP model\nhas shown limitations, MCL dramatically enhances performance, with improvements\nup to threefold on specific attributes in the recently proposed MMVP benchmark.\n"
    },
    {
        "title": "Ensemble Quadratic Assignment Network for Graph Matching",
        "published_time": "2024-03-11T06:34:05Z",
        "abstract": "  Graph matching is a commonly used technique in computer vision and pattern\nrecognition. Recent data-driven approaches have improved the graph matching\naccuracy remarkably, whereas some traditional algorithm-based methods are more\nrobust to feature noises, outlier nodes, and global transformation\n(e.g.~rotation). In this paper, we propose a graph neural network (GNN) based\napproach to combine the advantages of data-driven and traditional methods. In\nthe GNN framework, we transform traditional graph-matching solvers as\nsingle-channel GNNs on the association graph and extend the single-channel\narchitecture to the multi-channel network. The proposed model can be seen as an\nensemble method that fuses multiple algorithms at every iteration. Instead of\naveraging the estimates at the end of the ensemble, in our approach, the\nindependent iterations of the ensembled algorithms exchange their information\nafter each iteration via a 1x1 channel-wise convolution layer. Experiments show\nthat our model improves the performance of traditional algorithms\nsignificantly. In addition, we propose a random sampling strategy to reduce the\ncomputational complexity and GPU memory usage, so the model applies to matching\ngraphs with thousands of nodes. We evaluate the performance of our method on\nthree tasks: geometric graph matching, semantic feature matching, and few-shot\n3D shape classification. The proposed model performs comparably or outperforms\nthe best existing GNN-based methods.\n"
    },
    {
        "title": "FontCLIP: A Semantic Typography Visual-Language Model for Multilingual\n  Font Applications",
        "published_time": "2024-03-11T06:08:16Z",
        "abstract": "  Acquiring the desired font for various design tasks can be challenging and\nrequires professional typographic knowledge. While previous font retrieval or\ngeneration works have alleviated some of these difficulties, they often lack\nsupport for multiple languages and semantic attributes beyond the training data\ndomains. To solve this problem, we present FontCLIP: a model that connects the\nsemantic understanding of a large vision-language model with typographical\nknowledge. We integrate typography-specific knowledge into the comprehensive\nvision-language knowledge of a pretrained CLIP model through a novel finetuning\napproach. We propose to use a compound descriptive prompt that encapsulates\nadaptively sampled attributes from a font attribute dataset focusing on Roman\nalphabet characters. FontCLIP's semantic typographic latent space demonstrates\ntwo unprecedented generalization abilities. First, FontCLIP generalizes to\ndifferent languages including Chinese, Japanese, and Korean (CJK), capturing\nthe typographical features of fonts across different languages, even though it\nwas only finetuned using fonts of Roman characters. Second, FontCLIP can\nrecognize the semantic attributes that are not presented in the training data.\nFontCLIP's dual-modality and generalization abilities enable multilingual and\ncross-lingual font retrieval and letter shape optimization, reducing the burden\nof obtaining desired fonts.\n"
    },
    {
        "title": "Large Convolutional Model Tuning via Filter Subspace",
        "published_time": "2024-03-01T04:16:08Z",
        "abstract": "  Efficient fine-tuning methods are critical to address the high computational\nand parameter complexity while adapting large pre-trained models to downstream\ntasks. Our study is inspired by prior research that represents each convolution\nfilter as a linear combination of a small set of filter subspace elements,\nreferred to as filter atoms. In this paper, we propose to fine-tune pre-trained\nmodels by adjusting only filter atoms, which are responsible for spatial-only\nconvolution, while preserving spatially-invariant channel combination knowledge\nin atom coefficients. In this way, we bring a new filter subspace view for\nmodel tuning. Furthermore, each filter atom can be recursively decomposed as a\ncombination of another set of atoms, which naturally expands the number of\ntunable parameters in the filter subspace. By only adapting filter atoms\nconstructed by a small number of parameters, while maintaining the rest of\nmodel parameters constant, the proposed approach is highly parameter-efficient.\nIt effectively preserves the capabilities of pre-trained models and prevents\noverfitting to downstream tasks. Extensive experiments show that such a simple\nscheme surpasses previous tuning baselines for both discriminate and generative\ntasks.\n"
    },
    {
        "title": "Latent Semantic Consensus For Deterministic Geometric Model Fitting",
        "published_time": "2024-03-11T05:35:38Z",
        "abstract": "  Estimating reliable geometric model parameters from the data with severe\noutliers is a fundamental and important task in computer vision. This paper\nattempts to sample high-quality subsets and select model instances to estimate\nparameters in the multi-structural data. To address this, we propose an\neffective method called Latent Semantic Consensus (LSC). The principle of LSC\nis to preserve the latent semantic consensus in both data points and model\nhypotheses. Specifically, LSC formulates the model fitting problem into two\nlatent semantic spaces based on data points and model hypotheses, respectively.\nThen, LSC explores the distributions of points in the two latent semantic\nspaces, to remove outliers, generate high-quality model hypotheses, and\neffectively estimate model instances. Finally, LSC is able to provide\nconsistent and reliable solutions within only a few milliseconds for general\nmulti-structural model fitting, due to its deterministic fitting nature and\nefficiency. Compared with several state-of-the-art model fitting methods, our\nLSC achieves significant superiority for the performance of both accuracy and\nspeed on synthetic data and real images. The code will be available at\nhttps://github.com/guobaoxiao/LSC.\n"
    },
    {
        "title": "Temporal-Mapping Photography for Event Cameras",
        "published_time": "2024-03-11T05:29:46Z",
        "abstract": "  Event cameras, or Dynamic Vision Sensors (DVS) are novel neuromorphic sensors\nthat capture brightness changes as a continuous stream of ``events'' rather\nthan traditional intensity frames. Converting sparse events to dense intensity\nframes faithfully has long been an ill-posed problem. Previous methods have\nprimarily focused on converting events to video in dynamic scenes or with a\nmoving camera. In this paper, for the first time, we realize events to dense\nintensity image conversion using a stationary event camera in static scenes.\nDifferent from traditional methods that mainly rely on event integration, the\nproposed Event-Based Temporal Mapping Photography (EvTemMap) measures the time\nof event emitting for each pixel. Then, the resulting Temporal Matrix is\nconverted to an intensity frame with a temporal mapping neural network. At the\nhardware level, the proposed EvTemMap is implemented by combining a\ntransmittance adjustment device with a DVS, named Adjustable Transmittance\nDynamic Vision Sensor. Additionally, we collected TemMat dataset under various\nconditions including low-light and high dynamic range scenes. The experimental\nresults showcase the high dynamic range, fine-grained details, and\nhigh-grayscale-resolution of the proposed EvTemMap, as well as the enhanced\nperformance on downstream computer vision tasks compared to other methods. The\ncode and TemMat dataset will be made publicly available.\n"
    },
    {
        "title": "Learning Group Activity Features Through Person Attribute Prediction",
        "published_time": "2024-03-05T08:19:44Z",
        "abstract": "  This paper proposes Group Activity Feature (GAF) learning in which features\nof multi-person activity are learned as a compact latent vector. Unlike prior\nwork in which the manual annotation of group activities is required for\nsupervised learning, our method learns the GAF through person attribute\nprediction without group activity annotations. By learning the whole network in\nan end-to-end manner so that the GAF is required for predicting the person\nattributes of people in a group, the GAF is trained as the features of\nmulti-person activity. As a person attribute, we propose to use a person's\naction class and appearance features because the former is easy to annotate due\nto its simpleness, and the latter requires no manual annotation. In addition,\nwe introduce a location-guided attribute prediction to disentangle the complex\nGAF for extracting the features of each target person properly. Various\nexperimental results validate that our method outperforms SOTA methods\nquantitatively and qualitatively on two public datasets. Visualization of our\nGAF also demonstrates that our method learns the GAF representing fined-grained\ngroup activity classes. Code: https://github.com/chihina/GAFL-CVPR2024.\n"
    },
    {
        "title": "UFORecon: Generalizable Sparse-View Surface Reconstruction from\n  Arbitrary and UnFavOrable Sets",
        "published_time": "2024-03-08T06:27:13Z",
        "abstract": "  Generalizable neural implicit surface reconstruction aims to obtain an\naccurate underlying geometry given a limited number of multi-view images from\nunseen scenes. However, existing methods select only informative and relevant\nviews using predefined scores for training and testing phases. This constraint\nrenders the model impractical in real-world scenarios, where the availability\nof favorable combinations cannot always be ensured. We introduce and validate a\nview-combination score to indicate the effectiveness of the input view\ncombination. We observe that previous methods output degenerate solutions under\narbitrary and unfavorable sets. Building upon this finding, we propose\nUFORecon, a robust view-combination generalizable surface reconstruction\nframework. To achieve this, we apply cross-view matching transformers to model\ninteractions between source images and build correlation frustums to capture\nglobal correlations. Additionally, we explicitly encode pairwise feature\nsimilarities as view-consistent priors. Our proposed framework significantly\noutperforms previous methods in terms of view-combination generalizability and\nalso in the conventional generalizable protocol trained with favorable\nview-combinations. The code is available at\nhttps://github.com/Youngju-Na/UFORecon.\n"
    },
    {
        "title": "Leveraging Swin Transformer for Local-to-Global Weakly Supervised\n  Semantic Segmentation",
        "published_time": "2024-01-31T13:41:17Z",
        "abstract": "  In recent years, weakly supervised semantic segmentation using image-level\nlabels as supervision has received significant attention in the field of\ncomputer vision. Most existing methods have addressed the challenges arising\nfrom the lack of spatial information in these labels by focusing on\nfacilitating supervised learning through the generation of pseudo-labels from\nclass activation maps (CAMs). Due to the localized pattern detection of CNNs,\nCAMs often emphasize only the most discriminative parts of an object, making it\nchallenging to accurately distinguish foreground objects from each other and\nthe background. Recent studies have shown that Vision Transformer (ViT)\nfeatures, due to their global view, are more effective in capturing the scene\nlayout than CNNs. However, the use of hierarchical ViTs has not been\nextensively explored in this field. This work explores the use of Swin\nTransformer by proposing \"SWTformer\" to enhance the accuracy of the initial\nseed CAMs by bringing local and global views together. SWTformer-V1 generates\nclass probabilities and CAMs using only the patch tokens as features.\nSWTformer-V2 incorporates a multi-scale feature fusion mechanism to extract\nadditional information and utilizes a background-aware mechanism to generate\nmore accurate localization maps with improved cross-object discrimination.\nBased on experiments on the PascalVOC 2012 dataset, SWTformer-V1 achieves a\n0.98% mAP higher localization accuracy, outperforming state-of-the-art models.\nIt also yields comparable performance by 0.82% mIoU on average higher than\nother methods in generating initial localization maps, depending only on the\nclassification network. SWTformer-V2 further improves the accuracy of the\ngenerated seed CAMs by 5.32% mIoU, further proving the effectiveness of the\nlocal-to-global view provided by the Swin transformer. Code available at:\nhttps://github.com/RozhanAhmadi/SWTformer\n"
    },
    {
        "title": "Beyond Finite Data: Towards Data-free Out-of-distribution Generalization\n  via Extrapolation",
        "published_time": "2024-03-08T18:44:23Z",
        "abstract": "  Out-of-distribution (OOD) generalization is a favorable yet challenging\nproperty for deep neural networks. The core challenges lie in the limited\navailability of source domains that help models learn an invariant\nrepresentation from the spurious features. Various domain augmentation have\nbeen proposed but largely rely on interpolating existing domains and frequently\nface difficulties in creating truly \"novel\" domains. Humans, on the other hand,\ncan easily extrapolate novel domains, thus, an intriguing question arises: How\ncan neural networks extrapolate like humans and achieve OOD generalization?\n  We introduce a novel approach to domain extrapolation that leverages\nreasoning ability and the extensive knowledge encapsulated within large\nlanguage models (LLMs) to synthesize entirely new domains. Starting with the\nclass of interest, we query the LLMs to extract relevant knowledge for these\nnovel domains. We then bridge the gap between the text-centric knowledge\nderived from LLMs and the pixel input space of the model using text-to-image\ngeneration techniques. By augmenting the training set of domain generalization\ndatasets with high-fidelity, photo-realistic images of these new domains, we\nachieve significant improvements over all existing methods, as demonstrated in\nboth single and multi-domain generalization across various benchmarks.\n  With the ability to extrapolate any domains for any class, our method has the\npotential to learn a generalized model for any task without any data. To\nillustrate, we put forth a much more difficult setting termed, data-free domain\ngeneralization, that aims to learn a generalized model in the absence of any\ncollected data. Our empirical findings support the above argument and our\nmethods exhibit commendable performance in this setting, even surpassing the\nsupervised setting by approximately 1-2\\% on datasets such as VLCS.\n"
    },
    {
        "title": "Fine-Grained Pillar Feature Encoding Via Spatio-Temporal Virtual Grid\n  for 3D Object Detection",
        "published_time": "2024-03-11T04:58:36Z",
        "abstract": "  Developing high-performance, real-time architectures for LiDAR-based 3D\nobject detectors is essential for the successful commercialization of\nautonomous vehicles. Pillar-based methods stand out as a practical choice for\nonboard deployment due to their computational efficiency. However, despite\ntheir efficiency, these methods can sometimes underperform compared to\nalternative point encoding techniques such as Voxel-encoding or PointNet++. We\nargue that current pillar-based methods have not sufficiently captured the\nfine-grained distributions of LiDAR points within each pillar structure.\nConsequently, there exists considerable room for improvement in pillar feature\nencoding. In this paper, we introduce a novel pillar encoding architecture\nreferred to as Fine-Grained Pillar Feature Encoding (FG-PFE). FG-PFE utilizes\nSpatio-Temporal Virtual (STV) grids to capture the distribution of point clouds\nwithin each pillar across vertical, temporal, and horizontal dimensions.\nThrough STV grids, points within each pillar are individually encoded using\nVertical PFE (V-PFE), Temporal PFE (T-PFE), and Horizontal PFE (H-PFE). These\nencoded features are then aggregated through an Attentive Pillar Aggregation\nmethod. Our experiments conducted on the nuScenes dataset demonstrate that\nFG-PFE achieves significant performance improvements over baseline models such\nas PointPillar, CenterPoint-Pillar, and PillarNet, with only a minor increase\nin computational overhead.\n"
    },
    {
        "title": "STARFlow: Spatial Temporal Feature Re-embedding with Attentive Learning\n  for Real-world Scene Flow",
        "published_time": "2024-03-11T04:56:10Z",
        "abstract": "  Scene flow prediction is a crucial underlying task in understanding dynamic\nscenes as it offers fundamental motion information. However, contemporary scene\nflow methods encounter three major challenges. Firstly, flow estimation solely\nbased on local receptive fields lacks long-dependency matching of point pairs.\nTo address this issue, we propose global attentive flow embedding to match\nall-to-all point pairs in both feature space and Euclidean space, providing\nglobal initialization before local refinement. Secondly, there are deformations\nexisting in non-rigid objects after warping, which leads to variations in the\nspatiotemporal relation between the consecutive frames. For a more precise\nestimation of residual flow, a spatial temporal feature re-embedding module is\ndevised to acquire the sequence features after deformation. Furthermore,\nprevious methods perform poor generalization due to the significant domain gap\nbetween the synthesized and LiDAR-scanned datasets. We leverage novel domain\nadaptive losses to effectively bridge the gap of motion inference from\nsynthetic to real-world. Experiments demonstrate that our approach achieves\nstate-of-the-art performance across various datasets, with particularly\noutstanding results on real-world LiDAR-scanned datasets. Our code is available\nat https://github.com/O-VIGIA/StarFlow.\n"
    },
    {
        "title": "Bayesian Differentiable Physics for Cloth Digitalization",
        "published_time": "2024-02-27T16:35:07Z",
        "abstract": "  We propose a new method for cloth digitalization. Deviating from existing\nmethods which learn from data captured under relatively casual settings, we\npropose to learn from data captured in strictly tested measuring protocols, and\nfind plausible physical parameters of the cloths. However, such data is\ncurrently absent, so we first propose a new dataset with accurate cloth\nmeasurements. Further, the data size is considerably smaller than the ones in\ncurrent deep learning, due to the nature of the data capture process. To learn\nfrom small data, we propose a new Bayesian differentiable cloth model to\nestimate the complex material heterogeneity of real cloths. It can provide\nhighly accurate digitalization from very limited data samples. Through\nexhaustive evaluation and comparison, we show our method is accurate in cloth\ndigitalization, efficient in learning from limited data samples, and general in\ncapturing material variations. Code and data are available\nhttps://github.com/realcrane/Bayesian-Differentiable-Physics-for-Cloth-Digitalization\n"
    },
    {
        "title": "AS-FIBA: Adaptive Selective Frequency-Injection for Backdoor Attack on\n  Deep Face Restoration",
        "published_time": "2024-03-11T04:44:26Z",
        "abstract": "  Deep learning-based face restoration models, increasingly prevalent in smart\ndevices, have become targets for sophisticated backdoor attacks. These attacks,\nthrough subtle trigger injection into input face images, can lead to unexpected\nrestoration outcomes. Unlike conventional methods focused on classification\ntasks, our approach introduces a unique degradation objective tailored for\nattacking restoration models. Moreover, we propose the Adaptive Selective\nFrequency Injection Backdoor Attack (AS-FIBA) framework, employing a neural\nnetwork for input-specific trigger generation in the frequency domain,\nseamlessly blending triggers with benign images. This results in imperceptible\nyet effective attacks, guiding restoration predictions towards subtly degraded\noutputs rather than conspicuous targets. Extensive experiments demonstrate the\nefficacy of the degradation objective on state-of-the-art face restoration\nmodels. Additionally, it is notable that AS-FIBA can insert effective backdoors\nthat are more imperceptible than existing backdoor attack methods, including\nWaNet, ISSBA, and FIBA.\n"
    },
    {
        "title": "Bridging Domains with Approximately Shared Features",
        "published_time": "2024-03-11T04:25:41Z",
        "abstract": "  Multi-source domain adaptation aims to reduce performance degradation when\napplying machine learning models to unseen domains. A fundamental challenge is\ndevising the optimal strategy for feature selection. Existing literature is\nsomewhat paradoxical: some advocate for learning invariant features from source\ndomains, while others favor more diverse features. To address the challenge, we\npropose a statistical framework that distinguishes the utilities of features\nbased on the variance of their correlation to label $y$ across domains. Under\nour framework, we design and analyze a learning procedure consisting of\nlearning approximately shared feature representation from source tasks and\nfine-tuning it on the target task. Our theoretical analysis necessitates the\nimportance of learning approximately shared features instead of only the\nstrictly invariant features and yields an improved population risk compared to\nprevious results on both source and target tasks, thus partly resolving the\nparadox mentioned above. Inspired by our theory, we proposed a more practical\nway to isolate the content (invariant+approximately shared) from environmental\nfeatures and further consolidate our theoretical findings.\n"
    },
    {
        "title": "A Comparative Study of Perceptual Quality Metrics for Audio-driven\n  Talking Head Videos",
        "published_time": "2024-03-11T04:13:38Z",
        "abstract": "  The rapid advancement of Artificial Intelligence Generated Content (AIGC)\ntechnology has propelled audio-driven talking head generation, gaining\nconsiderable research attention for practical applications. However,\nperformance evaluation research lags behind the development of talking head\ngeneration techniques. Existing literature relies on heuristic quantitative\nmetrics without human validation, hindering accurate progress assessment. To\naddress this gap, we collect talking head videos generated from four generative\nmethods and conduct controlled psychophysical experiments on visual quality,\nlip-audio synchronization, and head movement naturalness. Our experiments\nvalidate consistency between model predictions and human annotations,\nidentifying metrics that align better with human opinions than widely-used\nmeasures. We believe our work will facilitate performance evaluation and model\ndevelopment, providing insights into AIGC in a broader context. Code and data\nwill be made available at https://github.com/zwx8981/ADTH-QA.\n"
    },
    {
        "title": "Enhanced Sparsification via Stimulative Training",
        "published_time": "2024-03-11T04:05:17Z",
        "abstract": "  Sparsification-based pruning has been an important category in model\ncompression. Existing methods commonly set sparsity-inducing penalty terms to\nsuppress the importance of dropped weights, which is regarded as the suppressed\nsparsification paradigm. However, this paradigm inactivates the dropped parts\nof networks causing capacity damage before pruning, thereby leading to\nperformance degradation. To alleviate this issue, we first study and reveal the\nrelative sparsity effect in emerging stimulative training and then propose a\nstructured pruning framework, named STP, based on an enhanced sparsification\nparadigm which maintains the magnitude of dropped weights and enhances the\nexpressivity of kept weights by self-distillation. Besides, to find an optimal\narchitecture for the pruned network, we propose a multi-dimension architecture\nspace and a knowledge distillation-guided exploration strategy. To reduce the\nhuge capacity gap of distillation, we propose a subnet mutating expansion\ntechnique. Extensive experiments on various benchmarks indicate the\neffectiveness of STP. Specifically, without fine-tuning, our method\nconsistently achieves superior performance at different budgets, especially\nunder extremely aggressive pruning scenarios, e.g., remaining 95.11% Top-1\naccuracy (72.43% in 76.15%) while reducing 85% FLOPs for ResNet-50 on ImageNet.\nCodes will be released soon.\n"
    },
    {
        "title": "SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic\n  Segmentation",
        "published_time": "2024-01-22T09:41:05Z",
        "abstract": "  Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation\nmodels using image data with only image-level supervision. Since precise\npixel-level annotations are not accessible, existing methods typically focus on\nproducing pseudo masks for training segmentation models by refining CAM-like\nheatmaps. However, the produced heatmaps may capture only the discriminative\nimage regions of object categories or the associated co-occurring backgrounds.\nTo address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS)\nframework, which learns to effectively prompt the CLIP latent space to enhance\nthe semantic alignment between the segmented regions and the target object\ncategories. More specifically, we propose Contrastive Prompt Learning and\nPrompt-guided Semantic Refinement to learn the prompts that adequately describe\nand suppress the co-occurring backgrounds associated with each target object\ncategory. In this way, SemPLeS can perform better semantic alignment between\nobject regions and the associated class labels, resulting in desired pseudo\nmasks for training the segmentation model. The proposed SemPLeS framework\nachieves SOTA performance on the standard WSSS benchmarks, PASCAL VOC and MS\nCOCO, and shows compatibility with other WSSS methods. The source codes are\nprovided in the supplementary.\n"
    },
    {
        "title": "Can LLMs' Tuning Methods Work in Medical Multimodal Domain?",
        "published_time": "2024-03-11T03:38:48Z",
        "abstract": "  While large language models (LLMs) excel in world knowledge understanding,\nadapting them to specific subfields requires precise adjustments. Due to the\nmodel's vast scale, traditional global fine-tuning methods for large models can\nbe computationally expensive and impact generalization. To address this\nchallenge, a range of innovative Parameters-Efficient Fine-Tuning (PEFT)\nmethods have emerged and achieved remarkable success in both LLMs and Large\nVision-Language Models (LVLMs). In the medical domain, fine-tuning a medical\nVision-Language Pretrained (VLP) model is essential for adapting it to specific\ntasks. Can the fine-tuning methods for large models be transferred to the\nmedical field to enhance transfer learning efficiency? In this paper, we delve\ninto the fine-tuning methods of LLMs and conduct extensive experiments to\ninvestigate the impact of fine-tuning methods for large models on existing\nmultimodal models in the medical domain from the training data level and the\nmodel structure level. We show the different impacts of fine-tuning methods for\nlarge models on medical VLMs and develop the most efficient ways to fine-tune\nmedical VLP models. We hope this research can guide medical domain researchers\nin optimizing VLMs' training costs, fostering the broader application of VLMs\nin healthcare fields. Code and dataset will be released upon acceptance.\n"
    },
    {
        "title": "Comparison of No-Reference Image Quality Models via MAP Estimation in\n  Diffusion Latents",
        "published_time": "2024-03-11T03:35:41Z",
        "abstract": "  Contemporary no-reference image quality assessment (NR-IQA) models can\neffectively quantify the perceived image quality, with high correlations\nbetween model predictions and human perceptual scores on fixed test sets.\nHowever, little progress has been made in comparing NR-IQA models from a\nperceptual optimization perspective. Here, for the first time, we demonstrate\nthat NR-IQA models can be plugged into the maximum a posteriori (MAP)\nestimation framework for image enhancement. This is achieved by taking the\ngradients in differentiable and bijective diffusion latents rather than in the\nraw pixel domain. Different NR-IQA models are likely to induce different\nenhanced images, which are ultimately subject to psychophysical testing. This\nleads to a new computational method for comparing NR-IQA models within the\nanalysis-by-synthesis framework. Compared to conventional correlation-based\nmetrics, our method provides complementary insights into the relative strengths\nand weaknesses of the competing NR-IQA models in the context of perceptual\noptimization.\n"
    },
    {
        "title": "AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge\n  Distillation",
        "published_time": "2024-03-11T03:34:14Z",
        "abstract": "  Due to privacy or patent concerns, a growing number of large models are\nreleased without granting access to their training data, making transferring\ntheir knowledge inefficient and problematic. In response, Data-Free Knowledge\nDistillation (DFKD) methods have emerged as direct solutions. However, simply\nadopting models derived from DFKD for real-world applications suffers\nsignificant performance degradation, due to the discrepancy between teachers'\ntraining data and real-world scenarios (student domain). The degradation stems\nfrom the portions of teachers' knowledge that are not applicable to the student\ndomain. They are specific to the teacher domain and would undermine students'\nperformance. Hence, selectively transferring teachers' appropriate knowledge\nbecomes the primary challenge in DFKD. In this work, we propose a simple but\neffective method AuG-KD. It utilizes an uncertainty-guided and sample-specific\nanchor to align student-domain data with the teacher domain and leverages a\ngenerative method to progressively trade off the learning process between OOD\nknowledge distillation and domain-specific information learning via mixup\nlearning. Extensive experiments in 3 datasets and 8 settings demonstrate the\nstability and superiority of our approach. Code available at\nhttps://github.com/IshiKura-a/AuG-KD .\n"
    },
    {
        "title": "PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via\n  Foundation Models",
        "published_time": "2024-03-11T03:28:20Z",
        "abstract": "  Recent success of vision foundation models have shown promising performance\nfor the 2D perception tasks. However, it is difficult to train a 3D foundation\nnetwork directly due to the limited dataset and it remains under explored\nwhether existing foundation models can be lifted to 3D space seamlessly. In\nthis paper, we present PointSeg, a novel training-free paradigm that leverages\noff-the-shelf vision foundation models to address 3D scene perception tasks.\nPointSeg can segment anything in 3D scene by acquiring accurate 3D prompts to\nalign their corresponding pixels across frames. Concretely, we design a\ntwo-branch prompts learning structure to construct the 3D point-box prompts\npairs, combining with the bidirectional matching strategy for accurate point\nand proposal prompts generation. Then, we perform the iterative post-refinement\nadaptively when cooperated with different vision foundation models. Moreover,\nwe design a affinity-aware merging algorithm to improve the final ensemble\nmasks. PointSeg demonstrates impressive segmentation performance across various\ndatasets, all without training. Specifically, our approach significantly\nsurpasses the state-of-the-art specialist model by 13.4$\\%$, 11.3$\\%$, and\n12$\\%$ mAP on ScanNet, ScanNet++, and KITTI-360 datasets, respectively. On top\nof that, PointSeg can incorporate with various segmentation models and even\nsurpasses the supervised methods.\n"
    },
    {
        "title": "Refining Segmentation On-the-Fly: An Interactive Framework for Point\n  Cloud Semantic Segmentation",
        "published_time": "2024-03-11T03:24:58Z",
        "abstract": "  Existing interactive point cloud segmentation approaches primarily focus on\nthe object segmentation, which aim to determine which points belong to the\nobject of interest guided by user interactions. This paper concentrates on an\nunexplored yet meaningful task, i.e., interactive point cloud semantic\nsegmentation, which assigns high-quality semantic labels to all points in a\nscene with user corrective clicks. Concretely, we presents the first\ninteractive framework for point cloud semantic segmentation, named InterPCSeg,\nwhich seamlessly integrates with off-the-shelf semantic segmentation networks\nwithout offline re-training, enabling it to run in an on-the-fly manner. To\nachieve online refinement, we treat user interactions as sparse training\nexamples during the test-time. To address the instability caused by the sparse\nsupervision, we design a stabilization energy to regulate the test-time\ntraining process. For objective and reproducible evaluation, we develop an\ninteraction simulation scheme tailored for the interactive point cloud semantic\nsegmentation task. We evaluate our framework on the S3DIS and ScanNet datasets\nwith off-the-shelf segmentation networks, incorporating interactions from both\nthe proposed interaction simulator and real users. Quantitative and qualitative\nexperimental results demonstrate the efficacy of our framework in refining the\nsemantic segmentation results with user interactions. The source code will be\npublicly available.\n"
    },
    {
        "title": "DivCon: Divide and Conquer for Progressive Text-to-Image Generation",
        "published_time": "2024-03-11T03:24:44Z",
        "abstract": "  Diffusion-driven text-to-image (T2I) generation has achieved remarkable\nadvancements. To further improve T2I models' capability in numerical and\nspatial reasoning, the layout is employed as an intermedium to bridge large\nlanguage models and layout-based diffusion models. However, these methods still\nstruggle with generating images from textural prompts with multiple objects and\ncomplicated spatial relationships. To tackle this challenge, we introduce a\ndivide-and-conquer approach which decouples the T2I generation task into simple\nsubtasks. Our approach divides the layout prediction stage into numerical \\&\nspatial reasoning and bounding box prediction. Then, the layout-to-image\ngeneration stage is conducted in an iterative manner to reconstruct objects\nfrom easy ones to difficult ones. We conduct experiments on the HRS and NSR-1K\nbenchmarks and our approach outperforms previous state-of-the-art models with\nnotable margins. In addition, visual results demonstrate that our approach\nsignificantly improves the controllability and consistency in generating\nmultiple objects from complex textural prompts.\n"
    },
    {
        "title": "A Segmentation Foundation Model for Diverse-type Tumors",
        "published_time": "2024-03-11T03:05:05Z",
        "abstract": "  Large pre-trained models with their numerous model parameters and extensive\ntraining datasets have shown excellent performance in various tasks. Many\npublicly available medical image datasets do not have a sufficient amount of\ndata so there are few large-scale models in medical imaging. We propose a\nlarge-scale Tumor Segmentation Foundation Model (TSFM) with 1.6 billion\nparameters using Resblock-backbone and Transformer-bottleneck,which has good\ntransfer ability for downstream tasks. To make TSFM exhibit good performance in\ntumor segmentation, we make full use of the strong spatial correlation between\ntumors and organs in the medical image, innovatively fuse 7 tumor datasets and\n3 multi-organ datasets to build a 3D medical dataset pool, including 2779 cases\nwith totally 300k medical images, whose size currently exceeds many other\nsingle publicly available datasets. TSFM is the pre-trained model for medical\nimage segmentation, which also can be transferred to multiple downstream tasks\nfor fine-tuning learning. The average performance of our pre-trained model is\n2% higher than that of nnU-Net across various tumor types. In the transfer\nlearning task, TSFM only needs 5% training epochs of nnU-Net to achieve similar\nperformance and can surpass nnU-Net by 2% on average with 10% training epoch.\nPre-trained TSFM and its code will be released soon.\n"
    },
    {
        "title": "Beyond MOT: Semantic Multi-Object Tracking",
        "published_time": "2024-03-08T03:54:22Z",
        "abstract": "  Current multi-object tracking (MOT) aims to predict trajectories of targets\n(i.e.,\"where\") in videos. Yet, knowing merely \"where\" is insufficient in many\ncrucial applications. In comparison, semantic understanding such as\nfine-grained behaviors, interactions, and overall summarized captions (i.e.,\n\"what\") from videos, associated with \"where\", is highly-desired for\ncomprehensive video analysis. Thus motivated, we introduce Semantic\nMulti-Object Tracking (SMOT), that aims to estimate object trajectories and\nmeanwhile understand semantic details of associated trajectories including\ninstance captions, instance interactions, and overall video captions,\nintegrating \"where\" and \"what\" for tracking. In order to foster the exploration\nof SMOT, we propose BenSMOT, a large-scale Benchmark for Semantic MOT.\nSpecifically, BenSMOT comprises 3,292 videos with 151K frames, covering various\nscenarios for semantic tracking of humans. BenSMOT provides annotations for the\ntrajectories of targets, along with associated instance captions in natural\nlanguage, instance interactions, and overall caption for each video sequence.\nTo our best knowledge, BenSMOT is the first publicly available benchmark for\nSMOT. Besides, to encourage future research, we present a novel tracker named\nSMOTer, which is specially designed and end-to-end trained for SMOT, showing\npromising performance. By releasing BenSMOT, we expect to go beyond\nconventional MOT by predicting \"where\" and \"what\" for SMOT, opening up a new\ndirection in tracking for video understanding. Our BenSMOT and SMOTer will be\nreleased.\n"
    },
    {
        "title": "Pre-Trained Model Recommendation for Downstream Fine-tuning",
        "published_time": "2024-03-11T02:24:32Z",
        "abstract": "  As a fundamental problem in transfer learning, model selection aims to rank\noff-the-shelf pre-trained models and select the most suitable one for the new\ntarget task. Existing model selection techniques are often constrained in their\nscope and tend to overlook the nuanced relationships between models and tasks.\nIn this paper, we present a pragmatic framework \\textbf{Fennec}, delving into a\ndiverse, large-scale model repository while meticulously considering the\nintricate connections between tasks and models. The key insight is to map all\nmodels and historical tasks into a transfer-related subspace, where the\ndistance between model vectors and task vectors represents the magnitude of\ntransferability. A large vision model, as a proxy, infers a new task's\nrepresentation in the transfer space, thereby circumventing the computational\nburden of extensive forward passes. We also investigate the impact of the\ninherent inductive bias of models on transfer results and propose a novel\nmethod called \\textbf{archi2vec} to encode the intricate structures of models.\nThe transfer score is computed through straightforward vector arithmetic with a\ntime complexity of $\\mathcal{O}(1)$. Finally, we make a substantial\ncontribution to the field by releasing a comprehensive benchmark. We validate\nthe effectiveness of our framework through rigorous testing on two benchmarks.\nThe benchmark and the code will be publicly available in the near future.\n"
    },
    {
        "title": "Enhancing Semantic Fidelity in Text-to-Image Synthesis: Attention\n  Regulation in Diffusion Models",
        "published_time": "2024-03-11T02:18:27Z",
        "abstract": "  Recent advancements in diffusion models have notably improved the perceptual\nquality of generated images in text-to-image synthesis tasks. However,\ndiffusion models often struggle to produce images that accurately reflect the\nintended semantics of the associated text prompts. We examine cross-attention\nlayers in diffusion models and observe a propensity for these layers to\ndisproportionately focus on certain tokens during the generation process,\nthereby undermining semantic fidelity. To address the issue of dominant\nattention, we introduce attention regulation, a computation-efficient\non-the-fly optimization approach at inference time to align attention maps with\nthe input text prompt. Notably, our method requires no additional training or\nfine-tuning and serves as a plug-in module on a model. Hence, the generation\ncapacity of the original model is fully preserved. We compare our approach with\nalternative approaches across various datasets, evaluation metrics, and\ndiffusion models. Experiment results show that our method consistently\noutperforms other baselines, yielding images that more faithfully reflect the\ndesired concepts with reduced computation overhead. Code is available at\nhttps://github.com/YaNgZhAnG-V5/attention_regulation.\n"
    },
    {
        "title": "Eliminating Warping Shakes for Unsupervised Online Video Stitching",
        "published_time": "2024-03-11T02:05:31Z",
        "abstract": "  In this paper, we retarget video stitching to an emerging issue, named\nwarping shake, when extending image stitching to video stitching. It unveils\nthe temporal instability of warped content in non-overlapping regions, despite\nimage stitching having endeavored to preserve the natural structures.\nTherefore, in most cases, even if the input videos to be stitched are stable,\nthe stitched video will inevitably cause undesired warping shakes and affect\nthe visual experience. To eliminate the shakes, we propose StabStitch to\nsimultaneously realize video stitching and video stabilization in a unified\nunsupervised learning framework. Starting from the camera paths in video\nstabilization, we first derive the expression of stitching trajectories in\nvideo stitching by elaborately integrating spatial and temporal warps. Then a\nwarp smoothing model is presented to optimize them with a comprehensive\nconsideration regarding content alignment, trajectory smoothness, spatial\nconsistency, and online collaboration. To establish an evaluation benchmark and\ntrain the learning framework, we build a video stitching dataset with a rich\ndiversity in camera motions and scenes. Compared with existing stitching\nsolutions, StabStitch exhibits significant superiority in scene robustness and\ninference speed in addition to stitching and stabilization performance,\ncontributing to a robust and real-time online video stitching system. The code\nand dataset will be available at https://github.com/nie-lang/StabStitch.\n"
    },
    {
        "title": "Local Feature Matching Using Deep Learning: A Survey",
        "published_time": "2024-01-31T04:32:41Z",
        "abstract": "  Local feature matching enjoys wide-ranging applications in the realm of\ncomputer vision, encompassing domains such as image retrieval, 3D\nreconstruction, and object recognition. However, challenges persist in\nimproving the accuracy and robustness of matching due to factors like viewpoint\nand lighting variations. In recent years, the introduction of deep learning\nmodels has sparked widespread exploration into local feature matching\ntechniques. The objective of this endeavor is to furnish a comprehensive\noverview of local feature matching methods. These methods are categorized into\ntwo key segments based on the presence of detectors. The Detector-based\ncategory encompasses models inclusive of Detect-then-Describe, Joint Detection\nand Description, Describe-then-Detect, as well as Graph Based techniques. In\ncontrast, the Detector-free category comprises CNN Based, Transformer Based,\nand Patch Based methods. Our study extends beyond methodological analysis,\nincorporating evaluations of prevalent datasets and metrics to facilitate a\nquantitative comparison of state-of-the-art techniques. The paper also explores\nthe practical application of local feature matching in diverse domains such as\nStructure from Motion, Remote Sensing Image Registration, and Medical Image\nRegistration, underscoring its versatility and significance across various\nfields. Ultimately, we endeavor to outline the current challenges faced in this\ndomain and furnish future research directions, thereby serving as a reference\nfor researchers involved in local feature matching and its interconnected\ndomains. A comprehensive list of studies in this survey is available at\nhttps://github.com/vignywang/Awesome-Local-Feature-Matching .\n"
    },
    {
        "title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning",
        "published_time": "2024-02-29T18:57:01Z",
        "abstract": "  It is challenging to perform question-answering over complex, multimodal\ncontent such as television clips. This is in part because current\nvideo-language models rely on single-modality reasoning, have lowered\nperformance on long inputs, and lack interpetability. We propose TV-TREES, the\nfirst multimodal entailment tree generator. TV-TREES serves as an approach to\nvideo understanding that promotes interpretable joint-modality reasoning by\nproducing trees of entailment relationships between simple premises directly\nentailed by the videos and higher-level conclusions. We then introduce the task\nof multimodal entailment tree generation to evaluate the reasoning quality of\nsuch methods. Our method's experimental results on the challenging TVQA dataset\ndemonstrate intepretable, state-of-the-art zero-shot performance on full video\nclips, illustrating a best-of-both-worlds contrast to black-box methods.\n"
    },
    {
        "title": "See Through Their Minds: Learning Transferable Neural Representation\n  from Cross-Subject fMRI",
        "published_time": "2024-03-11T01:18:49Z",
        "abstract": "  Deciphering visual content from functional Magnetic Resonance Imaging (fMRI)\nhelps illuminate the human vision system. However, the scarcity of fMRI data\nand noise hamper brain decoding model performance. Previous approaches\nprimarily employ subject-specific models, sensitive to training sample size. In\nthis paper, we explore a straightforward but overlooked solution to address\ndata scarcity. We propose shallow subject-specific adapters to map\ncross-subject fMRI data into unified representations. Subsequently, a shared\ndeeper decoding model decodes cross-subject features into the target feature\nspace. During training, we leverage both visual and textual supervision for\nmulti-modal brain decoding. Our model integrates a high-level perception\ndecoding pipeline and a pixel-wise reconstruction pipeline guided by high-level\nperceptions, simulating bottom-up and top-down processes in neuroscience.\nEmpirical experiments demonstrate robust neural representation learning across\nsubjects for both pipelines. Moreover, merging high-level and low-level\ninformation improves both low-level and high-level reconstruction metrics.\nAdditionally, we successfully transfer learned general knowledge to new\nsubjects by training new adapters with limited training data. Compared to\nprevious state-of-the-art methods, notably pre-training-based methods (Mind-Vis\nand fMRI-PTE), our approach achieves comparable or superior results across\ndiverse tasks, showing promise as an alternative method for cross-subject fMRI\ndata pre-training. Our code and pre-trained weights will be publicly released\nat https://github.com/YulongBonjour/See_Through_Their_Minds.\n"
    },
    {
        "title": "Continuous Memory Representation for Anomaly Detection",
        "published_time": "2024-02-28T12:38:44Z",
        "abstract": "  There have been significant advancements in anomaly detection in an\nunsupervised manner, where only normal images are available for training.\nSeveral recent methods aim to detect anomalies based on a memory, comparing or\nreconstructing the input with directly stored normal features (or trained\nfeatures with normal images). However, such memory-based approaches operate on\na discrete feature space implemented by the nearest neighbor or attention\nmechanism, suffering from poor generalization or an identity shortcut issue\noutputting the same as input, respectively. Furthermore, the majority of\nexisting methods are designed to detect single-class anomalies, resulting in\nunsatisfactory performance when presented with multiple classes of objects. To\ntackle all of the above challenges, we propose CRAD, a novel anomaly detection\nmethod for representing normal features within a \"continuous\" memory, enabled\nby transforming spatial features into coordinates and mapping them to\ncontinuous grids. Furthermore, we carefully design the grids tailored for\nanomaly detection, representing both local and global normal features and\nfusing them effectively. Our extensive experiments demonstrate that CRAD\nsuccessfully generalizes the normal features and mitigates the identity\nshortcut, furthermore, CRAD effectively handles diverse classes in a single\nmodel thanks to the high-granularity continuous representation. In an\nevaluation using the MVTec AD dataset, CRAD significantly outperforms the\nprevious state-of-the-art method by reducing 65.0% of the error for multi-class\nunified anomaly detection. The project page is available at\nhttps://tae-mo.github.io/crad/.\n"
    },
    {
        "title": "Video Generation with Consistency Tuning",
        "published_time": "2024-03-11T01:11:28Z",
        "abstract": "  Currently, various studies have been exploring generation of long videos.\nHowever, the generated frames in these videos often exhibit jitter and noise.\nTherefore, in order to generate the videos without these noise, we propose a\nnovel framework composed of four modules: separate tuning module, average\nfusion module, combined tuning module, and inter-frame consistency module. By\napplying our newly proposed modules subsequently, the consistency of the\nbackground and foreground in each video frames is optimized. Besides, the\nexperimental results demonstrate that videos generated by our method exhibit a\nhigh quality in comparison of the state-of-the-art methods.\n"
    },
    {
        "title": "Multi-modal Semantic Understanding with Contrastive Cross-modal Feature\n  Alignment",
        "published_time": "2024-03-11T01:07:36Z",
        "abstract": "  Multi-modal semantic understanding requires integrating information from\ndifferent modalities to extract users' real intention behind words. Most\nprevious work applies a dual-encoder structure to separately encode image and\ntext, but fails to learn cross-modal feature alignment, making it hard to\nachieve cross-modal deep information interaction. This paper proposes a novel\nCLIP-guided contrastive-learning-based architecture to perform multi-modal\nfeature alignment, which projects the features derived from different\nmodalities into a unified deep space. On multi-modal sarcasm detection (MMSD)\nand multi-modal sentiment analysis (MMSA) tasks, the experimental results show\nthat our proposed model significantly outperforms several baselines, and our\nfeature alignment strategy brings obvious performance gain over models with\ndifferent aggregating methods and models even enriched with knowledge. More\nimportantly, our model is simple to implement without using task-specific\nexternal knowledge, and thus can easily migrate to other multi-modal tasks. Our\nsource codes are available at https://github.com/ChangKe123/CLFA.\n"
    },
    {
        "title": "Exploring Hardware Friendly Bottleneck Architecture in CNN for Embedded\n  Computing Systems",
        "published_time": "2024-03-11T01:02:01Z",
        "abstract": "  In this paper, we explore how to design lightweight CNN architecture for\nembedded computing systems. We propose L-Mobilenet model for ZYNQ based\nhardware platform. L-Mobilenet can adapt well to the hardware computing and\naccelerating, and its network structure is inspired by the state-of-the-art\nwork of Inception-ResnetV1 and MobilenetV2, which can effectively reduce\nparameters and delay while maintaining the accuracy of inference. We deploy our\nL-Mobilenet model to ZYNQ embedded platform for fully evaluating the\nperformance of our design. By measuring in cifar10 and cifar100 datasets,\nL-Mobilenet model is able to gain 3x speed up and 3.7x fewer parameters than\nMobileNetV2 while maintaining a similar accuracy. It also can obtain 2x speed\nup and 1.5x fewer parameters than ShufflenetV2 while maintaining the same\naccuracy. Experiments show that our network model can obtain better performance\nbecause of the special considerations for hardware accelerating and\nsoftware-hardware co-design strategies in our L-Mobilenet bottleneck\narchitecture.\n"
    },
    {
        "title": "Put Myself in Your Shoes: Lifting the Egocentric Perspective from\n  Exocentric Videos",
        "published_time": "2024-03-11T01:00:00Z",
        "abstract": "  We investigate exocentric-to-egocentric cross-view translation, which aims to\ngenerate a first-person (egocentric) view of an actor based on a video\nrecording that captures the actor from a third-person (exocentric) perspective.\nTo this end, we propose a generative framework called Exo2Ego that decouples\nthe translation process into two stages: high-level structure transformation,\nwhich explicitly encourages cross-view correspondence between exocentric and\negocentric views, and a diffusion-based pixel-level hallucination, which\nincorporates a hand layout prior to enhance the fidelity of the generated\negocentric view. To pave the way for future advancements in this field, we\ncurate a comprehensive exo-to-ego cross-view translation benchmark. It consists\nof a diverse collection of synchronized ego-exo tabletop activity video pairs\nsourced from three public datasets: H2O, Aria Pilot, and Assembly101. The\nexperimental results validate that Exo2Ego delivers photorealistic video\nresults with clear hand manipulation details and outperforms several baselines\nin terms of both synthesis quality and generalization ability to new actions.\n"
    },
    {
        "title": "MOAB: Multi-Modal Outer Arithmetic Block For Fusion Of Histopathological\n  Images And Genetic Data For Brain Tumor Grading",
        "published_time": "2024-03-11T00:33:28Z",
        "abstract": "  Brain tumors are an abnormal growth of cells in the brain. They can be\nclassified into distinct grades based on their growth. Often grading is\nperformed based on a histological image and is one of the most significant\npredictors of a patients prognosis, the higher the grade, the more aggressive\nthe tumor. Correct diagnosis of a tumor grade remains challenging. Though\nhistopathological grading has been shown to be prognostic, results are subject\nto interobserver variability, even among experienced pathologists. Recently,\nthe World Health Organization reported that advances in molecular genetics have\nled to improvements in tumor classification. This paper seeks to integrate\nhistological images and genetic data for improved computer-aided diagnosis. We\npropose a novel Multi-modal Outer Arithmetic Block (MOAB) based on arithmetic\noperations to combine latent representations of the different modalities for\npredicting the tumor grade (Grade \\rom{2}, \\rom{3} and \\rom{4}). Extensive\nexperiments evaluate the effectiveness of our approach. By applying MOAB to The\nCancer Genome Atlas (TCGA) glioma dataset, we show that it can improve\nseparation between similar classes (Grade \\rom{2} and \\rom{3}) and outperform\nprior state-of-the-art grade classification techniques.\n"
    },
    {
        "title": "FOAA: Flattened Outer Arithmetic Attention For Multimodal Tumor\n  Classification",
        "published_time": "2024-03-10T23:12:40Z",
        "abstract": "  Fusion of multimodal healthcare data holds great promise to provide a\nholistic view of a patient's health, taking advantage of the complementarity of\ndifferent modalities while leveraging their correlation. This paper proposes a\nsimple and effective approach, inspired by attention, to fuse discriminative\nfeatures from different modalities. We propose a novel attention mechanism,\ncalled Flattened Outer Arithmetic Attention (FOAA), which relies on outer\narithmetic operators (addition, subtraction, product, and division) to compute\nattention scores from keys, queries and values derived from flattened\nembeddings of each modality. We demonstrate how FOAA can be implemented for\nself-attention and cross-attention, providing a reusable component in neural\nnetwork architectures. We evaluate FOAA on two datasets for multimodal tumor\nclassification and achieve state-of-the-art results, and we demonstrate that\nfeatures enriched by FOAA are superior to those derived from other fusion\napproaches. The code is publicly available at\n\\href{https://github.com/omniaalwazzan/FOAA}{https://github.com/omniaalwazzan/FOAA}\n"
    },
    {
        "title": "Leveraging Computer Vision in the Intensive Care Unit (ICU) for\n  Examining Visitation and Mobility",
        "published_time": "2024-03-10T21:43:47Z",
        "abstract": "  Despite the importance of closely monitoring patients in the Intensive Care\nUnit (ICU), many aspects are still assessed in a limited manner due to the time\nconstraints imposed on healthcare providers. For example, although excessive\nvisitations during rest hours can potentially exacerbate the risk of circadian\nrhythm disruption and delirium, it is not captured in the ICU. Likewise, while\nmobility can be an important indicator of recovery or deterioration in ICU\npatients, it is only captured sporadically or not captured at all. In the past\nfew years, the computer vision field has found application in many domains by\nreducing the human burden. Using computer vision systems in the ICU can also\npotentially enable non-existing assessments or enhance the frequency and\naccuracy of existing assessments while reducing the staff workload. In this\nstudy, we leverage a state-of-the-art noninvasive computer vision system based\non depth imaging to characterize ICU visitations and patients' mobility. We\nthen examine the relationship between visitation and several patient outcomes,\nsuch as pain, acuity, and delirium. We found an association between\ndeteriorating patient acuity and the incidence of delirium with increased\nvisitations. In contrast, self-reported pain, reported using the Defense and\nVeteran Pain Rating Scale (DVPRS), was correlated with decreased visitations.\nOur findings highlight the feasibility and potential of using noninvasive\nautonomous systems to monitor ICU patients.\n"
    },
    {
        "title": "An End-to-End Deep Learning Generative Framework for Refinable Shape\n  Matching and Generation",
        "published_time": "2024-03-10T21:33:53Z",
        "abstract": "  Generative modelling for shapes is a prerequisite for In-Silico Clinical\nTrials (ISCTs), which aim to cost-effectively validate medical device\ninterventions using synthetic anatomical shapes, often represented as 3D\nsurface meshes. However, constructing AI models to generate shapes closely\nresembling the real mesh samples is challenging due to variable vertex counts,\nconnectivities, and the lack of dense vertex-wise correspondences across the\ntraining data. Employing graph representations for meshes, we develop a novel\nunsupervised geometric deep-learning model to establish refinable shape\ncorrespondences in a latent space, construct a population-derived atlas and\ngenerate realistic synthetic shapes. We additionally extend our proposed base\nmodel to a joint shape generative-clustering multi-atlas framework to\nincorporate further variability and preserve more details in the generated\nshapes. Experimental results using liver and left-ventricular models\ndemonstrate the approach's applicability to computational medicine,\nhighlighting its suitability for ISCTs through a comparative analysis.\n"
    },
    {
        "title": "A streamlined Approach to Multimodal Few-Shot Class Incremental Learning\n  for Fine-Grained Datasets",
        "published_time": "2024-03-10T19:50:03Z",
        "abstract": "  Few-shot Class-Incremental Learning (FSCIL) poses the challenge of retaining\nprior knowledge while learning from limited new data streams, all without\noverfitting. The rise of Vision-Language models (VLMs) has unlocked numerous\napplications, leveraging their existing knowledge to fine-tune on custom data.\nHowever, training the whole model is computationally prohibitive, and VLMs\nwhile being versatile in general domains still struggle with fine-grained\ndatasets crucial for many applications. We tackle these challenges with two\nproposed simple modules. The first, Session-Specific Prompts (SSP), enhances\nthe separability of image-text embeddings across sessions. The second,\nHyperbolic distance, compresses representations of image-text pairs within the\nsame class while expanding those from different classes, leading to better\nrepresentations. Experimental results demonstrate an average 10-point increase\ncompared to baselines while requiring at least 8 times fewer trainable\nparameters. This improvement is further underscored on our three newly\nintroduced fine-grained datasets.\n"
    },
    {
        "title": "Transformer based Multitask Learning for Image Captioning and Object\n  Detection",
        "published_time": "2024-03-10T19:31:13Z",
        "abstract": "  In several real-world scenarios like autonomous navigation and mobility, to\nobtain a better visual understanding of the surroundings, image captioning and\nobject detection play a crucial role. This work introduces a novel multitask\nlearning framework that combines image captioning and object detection into a\njoint model. We propose TICOD, Transformer-based Image Captioning and Object\ndetection model for jointly training both tasks by combining the losses\nobtained from image captioning and object detection networks. By leveraging\njoint training, the model benefits from the complementary information shared\nbetween the two tasks, leading to improved performance for image captioning.\nOur approach utilizes a transformer-based architecture that enables end-to-end\nnetwork integration for image captioning and object detection and performs both\ntasks jointly. We evaluate the effectiveness of our approach through\ncomprehensive experiments on the MS-COCO dataset. Our model outperforms the\nbaselines from image captioning literature by achieving a 3.65% improvement in\nBERTScore.\n"
    },
    {
        "title": "Understanding and Mitigating Human-Labelling Errors in Supervised\n  Contrastive Learning",
        "published_time": "2024-03-10T19:05:12Z",
        "abstract": "  Human-annotated vision datasets inevitably contain a fraction of human\nmislabelled examples. While the detrimental effects of such mislabelling on\nsupervised learning are well-researched, their influence on Supervised\nContrastive Learning (SCL) remains largely unexplored. In this paper, we show\nthat human-labelling errors not only differ significantly from synthetic label\nerrors, but also pose unique challenges in SCL, different to those in\ntraditional supervised learning methods. Specifically, our results indicate\nthey adversely impact the learning process in the ~99% of cases when they occur\nas false positive samples. Existing noise-mitigating methods primarily focus on\nsynthetic label errors and tackle the unrealistic setting of very high\nsynthetic noise rates (40-80%), but they often underperform on common image\ndatasets due to overfitting. To address this issue, we introduce a novel SCL\nobjective with robustness to human-labelling errors, SCL-RHE. SCL-RHE is\ndesigned to mitigate the effects of real-world mislabelled examples, typically\ncharacterized by much lower noise rates (<5%). We demonstrate that SCL-RHE\nconsistently outperforms state-of-the-art representation learning and\nnoise-mitigating methods across various vision benchmarks, by offering improved\nresilience against human-labelling errors.\n"
    },
    {
        "title": "Probing Image Compression For Class-Incremental Learning",
        "published_time": "2024-03-10T18:58:14Z",
        "abstract": "  Image compression emerges as a pivotal tool in the efficient handling and\ntransmission of digital images. Its ability to substantially reduce file size\nnot only facilitates enhanced data storage capacity but also potentially brings\nadvantages to the development of continual machine learning (ML) systems, which\nlearn new knowledge incrementally from sequential data. Continual ML systems\noften rely on storing representative samples, also known as exemplars, within a\nlimited memory constraint to maintain the performance on previously learned\ndata. These methods are known as memory replay-based algorithms and have proven\neffective at mitigating the detrimental effects of catastrophic forgetting.\nNonetheless, the limited memory buffer size often falls short of adequately\nrepresenting the entire data distribution. In this paper, we explore the use of\nimage compression as a strategy to enhance the buffer's capacity, thereby\nincreasing exemplar diversity. However, directly using compressed exemplars\nintroduces domain shift during continual ML, marked by a discrepancy between\ncompressed training data and uncompressed testing data. Additionally, it is\nessential to determine the appropriate compression algorithm and select the\nmost effective rate for continual ML systems to balance the trade-off between\nexemplar quality and quantity. To this end, we introduce a new framework to\nincorporate image compression for continual ML including a pre-processing data\ncompression step and an efficient compression rate/algorithm selection method.\nWe conduct extensive experiments on CIFAR-100 and ImageNet datasets and show\nthat our method significantly improves image classification accuracy in\ncontinual ML settings.\n"
    },
    {
        "title": "UNICORN: Ultrasound Nakagami Imaging via Score Matching and Adaptation",
        "published_time": "2024-03-10T18:05:41Z",
        "abstract": "  Nakagami imaging holds promise for visualizing and quantifying tissue\nscattering in ultrasound waves, with potential applications in tumor diagnosis\nand fat fraction estimation which are challenging to discern by conventional\nultrasound B-mode images. Existing methods struggle with optimal window size\nselection and suffer from estimator instability, leading to degraded resolution\nimages. To address this, here we propose a novel method called UNICORN\n(Ultrasound Nakagami Imaging via Score Matching and Adaptation), that offers an\naccurate, closed-form estimator for Nakagami parameter estimation in terms of\nthe score function of ultrasonic envelope. Extensive experiments using\nsimulation and real ultrasound RF data demonstrate UNICORN's superiority over\nconventional approaches in accuracy and resolution quality.\n"
    },
    {
        "title": "FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video\n  Editing",
        "published_time": "2024-03-10T17:12:01Z",
        "abstract": "  Diffusion models have demonstrated remarkable capabilities in text-to-image\nand text-to-video generation, opening up possibilities for video editing based\non textual input. However, the computational cost associated with sequential\nsampling in diffusion models poses challenges for efficient video editing.\nExisting approaches relying on image generation models for video editing suffer\nfrom time-consuming one-shot fine-tuning, additional condition extraction, or\nDDIM inversion, making real-time applications impractical. In this work, we\npropose FastVideoEdit, an efficient zero-shot video editing approach inspired\nby Consistency Models (CMs). By leveraging the self-consistency property of\nCMs, we eliminate the need for time-consuming inversion or additional condition\nextraction, reducing editing time. Our method enables direct mapping from\nsource video to target video with strong preservation ability utilizing a\nspecial variance schedule. This results in improved speed advantages, as fewer\nsampling steps can be used while maintaining comparable generation quality.\nExperimental results validate the state-of-the-art performance and speed\nadvantages of FastVideoEdit across evaluation metrics encompassing editing\nspeed, temporal consistency, and text-video alignment.\n"
    },
    {
        "title": "Physics-Guided Abnormal Trajectory Gap Detection",
        "published_time": "2024-03-10T17:07:28Z",
        "abstract": "  Given trajectories with gaps (i.e., missing data), we investigate algorithms\nto identify abnormal gaps in trajectories which occur when a given moving\nobject did not report its location, but other moving objects in the same\ngeographic region periodically did. The problem is important due to its\nsocietal applications, such as improving maritime safety and regulatory\nenforcement for global security concerns such as illegal fishing, illegal oil\ntransfers, and trans-shipments. The problem is challenging due to the\ndifficulty of bounding the possible locations of the moving object during a\ntrajectory gap, and the very high computational cost of detecting gaps in such\na large volume of location data. The current literature on anomalous trajectory\ndetection assumes linear interpolation within gaps, which may not be able to\ndetect abnormal gaps since objects within a given region may have traveled away\nfrom their shortest path. In preliminary work, we introduced an abnormal gap\nmeasure that uses a classical space-time prism model to bound an object's\npossible movement during the trajectory gap and provided a scalable memoized\ngap detection algorithm (Memo-AGD). In this paper, we propose a Space\nTime-Aware Gap Detection (STAGD) approach to leverage space-time indexing and\nmerging of trajectory gaps. We also incorporate a Dynamic Region Merge-based\n(DRM) approach to efficiently compute gap abnormality scores. We provide\ntheoretical proofs that both algorithms are correct and complete and also\nprovide analysis of asymptotic time complexity. Experimental results on\nsynthetic and real-world maritime trajectory data show that the proposed\napproach substantially improves computation time over the baseline technique.\n"
    },
    {
        "title": "Poly Kernel Inception Network for Remote Sensing Detection",
        "published_time": "2024-03-10T16:56:44Z",
        "abstract": "  Object detection in remote sensing images (RSIs) often suffers from several\nincreasing challenges, including the large variation in object scales and the\ndiverse-ranging context. Prior methods tried to address these challenges by\nexpanding the spatial receptive field of the backbone, either through\nlarge-kernel convolution or dilated convolution. However, the former typically\nintroduces considerable background noise, while the latter risks generating\noverly sparse feature representations. In this paper, we introduce the Poly\nKernel Inception Network (PKINet) to handle the above challenges. PKINet\nemploys multi-scale convolution kernels without dilation to extract object\nfeatures of varying scales and capture local context. In addition, a Context\nAnchor Attention (CAA) module is introduced in parallel to capture long-range\ncontextual information. These two components work jointly to advance the\nperformance of PKINet on four challenging remote sensing detection benchmarks,\nnamely DOTA-v1.0, DOTA-v1.5, HRSC2016, and DIOR-R.\n"
    },
    {
        "title": "Deep spatial context: when attention-based models meet spatial\n  regression",
        "published_time": "2024-01-18T15:08:42Z",
        "abstract": "  We propose 'Deep spatial context' (DSCon) method, which serves for\ninvestigation of the attention-based vision models using the concept of spatial\ncontext. It was inspired by histopathologists, however, the method can be\napplied to various domains. The DSCon allows for a quantitative measure of the\nspatial context's role using three Spatial Context Measures: $SCM_{features}$,\n$SCM_{targets}$, $SCM_{residuals}$ to distinguish whether the spatial context\nis observable within the features of neighboring regions, their target values\n(attention scores) or residuals, respectively. It is achieved by integrating\nspatial regression into the pipeline. The DSCon helps to verify research\nquestions. The experiments reveal that spatial relationships are much bigger in\nthe case of the classification of tumor lesions than normal tissues. Moreover,\nit turns out that the larger the size of the neighborhood taken into account\nwithin spatial regression, the less valuable contextual information is.\nFurthermore, it is observed that the spatial context measure is the largest\nwhen considered within the feature space as opposed to the targets and\nresiduals.\n"
    },
    {
        "title": "Online Multi-spectral Neuron Tracing",
        "published_time": "2024-03-10T16:34:21Z",
        "abstract": "  In this paper, we propose an online multi-spectral neuron tracing method with\nuniquely designed modules, where no offline training are required. Our method\nis trained online to update our enhanced discriminative correlation filter to\nconglutinate the tracing process. This distinctive offline-training-free schema\ndifferentiates us from other training-dependent tracing approaches like deep\nlearning methods since no annotation is needed for our method. Besides,\ncompared to other tracing methods requiring complicated set-up such as for\nclustering and graph multi-cut, our approach is much easier to be applied to\nnew images. In fact, it only needs a starting bounding box of the tracing\nneuron, significantly reducing users' configuration effort. Our extensive\nexperiments show that our training-free and easy-configured methodology allows\nfast and accurate neuron reconstructions in multi-spectral images.\n"
    },
    {
        "title": "Text-Guided Variational Image Generation for Industrial Anomaly\n  Detection and Segmentation",
        "published_time": "2024-03-10T16:11:17Z",
        "abstract": "  We propose a text-guided variational image generation method to address the\nchallenge of getting clean data for anomaly detection in industrial\nmanufacturing. Our method utilizes text information about the target object,\nlearned from extensive text library documents, to generate non-defective data\nimages resembling the input image. The proposed framework ensures that the\ngenerated non-defective images align with anticipated distributions derived\nfrom textual and image-based knowledge, ensuring stability and generality.\nExperimental results demonstrate the effectiveness of our approach, surpassing\nprevious methods even with limited non-defective data. Our approach is\nvalidated through generalization tests across four baseline models and three\ndistinct datasets. We present an additional analysis to enhance the\neffectiveness of anomaly detection models by utilizing the generated images.\n"
    },
    {
        "title": "WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text\n  and Image Inputs",
        "published_time": "2024-03-10T16:09:02Z",
        "abstract": "  Several text-to-video diffusion models have demonstrated commendable\ncapabilities in synthesizing high-quality video content. However, it remains a\nformidable challenge pertaining to maintaining temporal consistency and\nensuring action smoothness throughout the generated sequences. In this paper,\nwe present an innovative video generation AI agent that harnesses the power of\nSora-inspired multimodal learning to build skilled world models framework based\non textual prompts and accompanying images. The framework includes two parts:\nprompt enhancer and full video translation. The first part employs the\ncapabilities of ChatGPT to meticulously distill and proactively construct\nprecise prompts for each subsequent step, thereby guaranteeing the utmost\naccuracy in prompt communication and accurate execution in following model\noperations. The second part employ compatible with existing advanced diffusion\ntechniques to expansively generate and refine the key frame at the conclusion\nof a video. Then we can expertly harness the power of leading and trailing key\nframes to craft videos with enhanced temporal consistency and action\nsmoothness. The experimental results confirm that our method has strong\neffectiveness and novelty in constructing world models from text and image\ninputs over the other methods.\n"
    },
    {
        "title": "Democratizing Fine-grained Visual Recognition with Large Language Models",
        "published_time": "2024-01-24T22:28:26Z",
        "abstract": "  Identifying subordinate-level categories from images is a longstanding task\nin computer vision and is referred to as fine-grained visual recognition\n(FGVR). It has tremendous significance in real-world applications since an\naverage layperson does not excel at differentiating species of birds or\nmushrooms due to subtle differences among the species. A major bottleneck in\ndeveloping FGVR systems is caused by the need of high-quality paired expert\nannotations. To circumvent the need of expert knowledge we propose Fine-grained\nSemantic Category Reasoning (FineR) that internally leverages the world\nknowledge of large language models (LLMs) as a proxy in order to reason about\nfine-grained category names. In detail, to bridge the modality gap between\nimages and LLM, we extract part-level visual attributes from images as text and\nfeed that information to a LLM. Based on the visual attributes and its internal\nworld knowledge the LLM reasons about the subordinate-level category names. Our\ntraining-free FineR outperforms several state-of-the-art FGVR and language and\nvision assistant models and shows promise in working in the wild and in new\ndomains where gathering expert annotation is arduous.\n"
    },
    {
        "title": "BlazeBVD: Make Scale-Time Equalization Great Again for Blind Video\n  Deflickering",
        "published_time": "2024-03-10T15:56:55Z",
        "abstract": "  Developing blind video deflickering (BVD) algorithms to enhance video\ntemporal consistency, is gaining importance amid the flourish of image\nprocessing and video generation. However, the intricate nature of video data\ncomplicates the training of deep learning methods, leading to high resource\nconsumption and instability, notably under severe lighting flicker. This\nunderscores the critical need for a compact representation beyond pixel values\nto advance BVD research and applications. Inspired by the classic scale-time\nequalization (STE), our work introduces the histogram-assisted solution, called\nBlazeBVD, for high-fidelity and rapid BVD. Compared with STE, which directly\ncorrects pixel values by temporally smoothing color histograms, BlazeBVD\nleverages smoothed illumination histograms within STE filtering to ease the\nchallenge of learning temporal data using neural networks. In technique,\nBlazeBVD begins by condensing pixel values into illumination histograms that\nprecisely capture flickering and local exposure variations. These histograms\nare then smoothed to produce singular frames set, filtered illumination maps,\nand exposure maps. Resorting to these deflickering priors, BlazeBVD utilizes a\n2D network to restore faithful and consistent texture impacted by lighting\nchanges or localized exposure issues. BlazeBVD also incorporates a lightweight\n3D network to amend slight temporal inconsistencies, avoiding the resource\nconsumption issue. Comprehensive experiments on synthetic, real-world and\ngenerated videos, showcase the superior qualitative and quantitative results of\nBlazeBVD, achieving inference speeds up to 10x faster than state-of-the-arts.\n"
    },
    {
        "title": "Domain adaptation, Explainability & Fairness in AI for Medical Image\n  Analysis: Diagnosis of COVID-19 based on 3-D Chest CT-scans",
        "published_time": "2024-03-04T16:31:58Z",
        "abstract": "  The paper presents the DEF-AI-MIA COV19D Competition, which is organized in\nthe framework of the 'Domain adaptation, Explainability, Fairness in AI for\nMedical Image Analysis (DEF-AI-MIA)' Workshop of the 2024 Computer Vision and\nPattern Recognition (CVPR) Conference. The Competition is the 4th in the\nseries, following the first three Competitions held in the framework of ICCV\n2021, ECCV 2022 and ICASSP 2023 International Conferences respectively. It\nincludes two Challenges on: i) Covid-19 Detection and ii) Covid-19 Domain\nAdaptation. The Competition use data from COV19-CT-DB database, which is\ndescribed in the paper and includes a large number of chest CT scan series.\nEach chest CT scan series consists of a sequence of 2-D CT slices, the number\nof which is between 50 and 700. Training, validation and test datasets have\nbeen extracted from COV19-CT-DB and provided to the participants in both\nChallenges. The paper presents the baseline models used in the Challenges and\nthe performance which was obtained respectively.\n"
    },
    {
        "title": "Finding Visual Saliency in Continuous Spike Stream",
        "published_time": "2024-03-10T15:15:35Z",
        "abstract": "  As a bio-inspired vision sensor, the spike camera emulates the operational\nprinciples of the fovea, a compact retinal region, by employing spike\ndischarges to encode the accumulation of per-pixel luminance intensity.\nLeveraging its high temporal resolution and bio-inspired neuromorphic design,\nthe spike camera holds significant promise for advancing computer vision\napplications. Saliency detection mimics the behavior of human beings and\ncaptures the most salient region from the scenes. In this paper, we investigate\nthe visual saliency in the continuous spike stream for the first time. To\neffectively process the binary spike stream, we propose a Recurrent Spiking\nTransformer (RST) framework, which is based on a full spiking neural network.\nOur framework enables the extraction of spatio-temporal features from the\ncontinuous spatio-temporal spike stream while maintaining low power\nconsumption. To facilitate the training and validation of our proposed model,\nwe build a comprehensive real-world spike-based visual saliency dataset,\nenriched with numerous light conditions. Extensive experiments demonstrate the\nsuperior performance of our Recurrent Spiking Transformer framework in\ncomparison to other spike neural network-based methods. Our framework exhibits\na substantial margin of improvement in capturing and highlighting visual\nsaliency in the spike stream, which not only provides a new perspective for\nspike-based saliency segmentation but also shows a new paradigm for full\nSNN-based transformer models. The code and dataset are available at\n\\url{https://github.com/BIT-Vision/SVS}.\n"
    },
    {
        "title": "Benchmarking Segmentation Models with Mask-Preserved Attribute Editing",
        "published_time": "2024-03-02T15:20:09Z",
        "abstract": "  When deploying segmentation models in practice, it is critical to evaluate\ntheir behaviors in varied and complex scenes. Different from the previous\nevaluation paradigms only in consideration of global attribute variations (e.g.\nadverse weather), we investigate both local and global attribute variations for\nrobustness evaluation. To achieve this, we construct a mask-preserved attribute\nediting pipeline to edit visual attributes of real images with precise control\nof structural information. Therefore, the original segmentation labels can be\nreused for the edited images. Using our pipeline, we construct a benchmark\ncovering both object and image attributes (e.g. color, material, pattern,\nstyle). We evaluate a broad variety of semantic segmentation models, spanning\nfrom conventional close-set models to recent open-vocabulary large models on\ntheir robustness to different types of variations. We find that both local and\nglobal attribute variations affect segmentation performances, and the\nsensitivity of models diverges across different variation types. We argue that\nlocal attributes have the same importance as global attributes, and should be\nconsidered in the robustness evaluation of segmentation models. Code:\nhttps://github.com/PRIS-CV/Pascal-EA.\n"
    },
    {
        "title": "PEPSI: Pathology-Enhanced Pulse-Sequence-Invariant Representations for\n  Brain MRI",
        "published_time": "2024-03-10T14:33:55Z",
        "abstract": "  Remarkable progress has been made by data-driven machine-learning methods in\nthe analysis of MRI scans. However, most existing MRI analysis approaches are\ncrafted for specific MR pulse sequences (MR contrasts) and usually require\nnearly isotropic acquisitions. This limits their applicability to diverse\nreal-world clinical data, where scans commonly exhibit variations in\nappearances due to being obtained with varying sequence parameters,\nresolutions, and orientations -- especially in the presence of pathology. In\nthis paper, we propose PEPSI, the first pathology-enhanced, and\npulse-sequence-invariant feature representation learning model for brain MRI.\nPEPSI is trained entirely on synthetic images with a novel pathology encoding\nstrategy, and enables co-training across datasets with diverse pathologies and\nmissing modalities. Despite variations in pathology appearances across\ndifferent MR pulse sequences or the quality of acquired images (e.g.,\nresolution, orientation, artifacts, etc), PEPSI produces a high-resolution\nimage of reference contrast (MP-RAGE) that captures anatomy, along with an\nimage specifically highlighting the pathology. Our experiments demonstrate\nPEPSI's remarkable capability for image synthesis compared with the\nstate-of-the-art, contrast-agnostic synthesis models, as it accurately\nreconstructs anatomical structures while differentiating between pathology and\nnormal tissue. We further illustrate the efficiency and effectiveness of PEPSI\nfeatures for downstream pathology segmentations on five public datasets\ncovering white matter hyperintensities and stroke lesions. Code is available at\nhttps://github.com/peirong26/PEPSI.\n"
    },
    {
        "title": "MoST: Motion Style Transformer between Diverse Action Contents",
        "published_time": "2024-03-10T14:11:25Z",
        "abstract": "  While existing motion style transfer methods are effective between two\nmotions with identical content, their performance significantly diminishes when\ntransferring style between motions with different contents. This challenge lies\nin the lack of clear separation between content and style of a motion. To\ntackle this challenge, we propose a novel motion style transformer that\neffectively disentangles style from content and generates a plausible motion\nwith transferred style from a source motion. Our distinctive approach to\nachieving the goal of disentanglement is twofold: (1) a new architecture for\nmotion style transformer with 'part-attentive style modulator across body\nparts' and 'Siamese encoders that encode style and content features\nseparately'; (2) style disentanglement loss. Our method outperforms existing\nmethods and demonstrates exceptionally high quality, particularly in motion\npairs with different contents, without the need for heuristic post-processing.\nCodes are available at https://github.com/Boeun-Kim/MoST.\n"
    },
    {
        "title": "Enhancing Quality of Compressed Images by Mitigating Enhancement Bias\n  Towards Compression Domain",
        "published_time": "2024-02-27T04:37:04Z",
        "abstract": "  Existing quality enhancement methods for compressed images focus on aligning\nthe enhancement domain with the raw domain to yield realistic images. However,\nthese methods exhibit a pervasive enhancement bias towards the compression\ndomain, inadvertently regarding it as more realistic than the raw domain. This\nbias makes enhanced images closely resemble their compressed counterparts, thus\ndegrading their perceptual quality. In this paper, we propose a simple yet\neffective method to mitigate this bias and enhance the quality of compressed\nimages. Our method employs a conditional discriminator with the compressed\nimage as a key condition, and then incorporates a domain-divergence\nregularization to actively distance the enhancement domain from the compression\ndomain. Through this dual strategy, our method enables the discrimination\nagainst the compression domain, and brings the enhancement domain closer to the\nraw domain. Comprehensive quality evaluations confirm the superiority of our\nmethod over other state-of-the-art methods without incurring inference\noverheads.\n"
    },
    {
        "title": "$V_kD:$ Improving Knowledge Distillation using Orthogonal Projections",
        "published_time": "2024-03-10T13:26:24Z",
        "abstract": "  Knowledge distillation is an effective method for training small and\nefficient deep learning models. However, the efficacy of a single method can\ndegenerate when transferring to other tasks, modalities, or even other\narchitectures. To address this limitation, we propose a novel constrained\nfeature distillation method. This method is derived from a small set of core\nprinciples, which results in two emerging components: an orthogonal projection\nand a task-specific normalisation. Equipped with both of these components, our\ntransformer models can outperform all previous methods on ImageNet and reach up\nto a 4.4% relative improvement over the previous state-of-the-art methods. To\nfurther demonstrate the generality of our method, we apply it to object\ndetection and image generation, whereby we obtain consistent and substantial\nperformance improvements over state-of-the-art. Code and models are publicly\navailable: https://github.com/roymiles/vkd\n"
    },
    {
        "title": "SuPRA: Surgical Phase Recognition and Anticipation for Intra-Operative\n  Planning",
        "published_time": "2024-03-10T12:46:33Z",
        "abstract": "  Intra-operative recognition of surgical phases holds significant potential\nfor enhancing real-time contextual awareness in the operating room. However, we\nargue that online recognition, while beneficial, primarily lends itself to\npost-operative video analysis due to its limited direct impact on the actual\nsurgical decisions and actions during ongoing procedures. In contrast, we\ncontend that the prediction and anticipation of surgical phases are inherently\nmore valuable for intra-operative assistance, as they can meaningfully\ninfluence a surgeon's immediate and long-term planning by providing foresight\ninto future steps. To address this gap, we propose a dual approach that\nsimultaneously recognises the current surgical phase and predicts upcoming\nones, thus offering comprehensive intra-operative assistance and guidance on\nthe expected remaining workflow. Our novel method, Surgical Phase Recognition\nand Anticipation (SuPRA), leverages past and current information for accurate\nintra-operative phase recognition while using future segments for phase\nprediction. This unified approach challenges conventional frameworks that treat\nthese objectives separately. We have validated SuPRA on two reputed datasets,\nCholec80 and AutoLaparo21, where it demonstrated state-of-the-art performance\nwith recognition accuracies of 91.8% and 79.3%, respectively. Additionally, we\nintroduce and evaluate our model using new segment-level evaluation metrics,\nnamely Edit and F1 Overlap scores, for a more temporal assessment of segment\nclassification. In conclusion, SuPRA presents a new multi-task approach that\npaves the way for improved intra-operative assistance through surgical phase\nrecognition and prediction of future events.\n"
    },
    {
        "title": "DrFuse: Learning Disentangled Representation for Clinical Multi-Modal\n  Fusion with Missing Modality and Modal Inconsistency",
        "published_time": "2024-03-10T12:41:34Z",
        "abstract": "  The combination of electronic health records (EHR) and medical images is\ncrucial for clinicians in making diagnoses and forecasting prognosis.\nStrategically fusing these two data modalities has great potential to improve\nthe accuracy of machine learning models in clinical prediction tasks. However,\nthe asynchronous and complementary nature of EHR and medical images presents\nunique challenges. Missing modalities due to clinical and administrative\nfactors are inevitable in practice, and the significance of each data modality\nvaries depending on the patient and the prediction target, resulting in\ninconsistent predictions and suboptimal model performance. To address these\nchallenges, we propose DrFuse to achieve effective clinical multi-modal fusion.\nIt tackles the missing modality issue by disentangling the features shared\nacross modalities and those unique within each modality. Furthermore, we\naddress the modal inconsistency issue via a disease-wise attention layer that\nproduces the patient- and disease-wise weighting for each modality to make the\nfinal prediction. We validate the proposed method using real-world large-scale\ndatasets, MIMIC-IV and MIMIC-CXR. Experimental results show that the proposed\nmethod significantly outperforms the state-of-the-art models. Our\nimplementation is publicly available at https://github.com/dorothy-yao/drfuse.\n"
    },
    {
        "title": "On depth prediction for autonomous driving using self-supervised\n  learning",
        "published_time": "2024-03-10T12:33:12Z",
        "abstract": "  Perception of the environment is a critical component for enabling autonomous\ndriving. It provides the vehicle with the ability to comprehend its\nsurroundings and make informed decisions. Depth prediction plays a pivotal role\nin this process, as it helps the understanding of the geometry and motion of\nthe environment. This thesis focuses on the challenge of depth prediction using\nmonocular self-supervised learning techniques. The problem is approached from a\nbroader perspective first, exploring conditional generative adversarial\nnetworks (cGANs) as a potential technique to achieve better generalization was\nperformed. In doing so, a fundamental contribution to the conditional GANs, the\nacontrario cGAN was proposed. The second contribution entails a single\nimage-to-depth self-supervised method, proposing a solution for the rigid-scene\nassumption using a novel transformer-based method that outputs a pose for each\ndynamic object. The third significant aspect involves the introduction of a\nvideo-to-depth map forecasting approach. This method serves as an extension of\nself-supervised techniques to predict future depths. This involves the creation\nof a novel transformer model capable of predicting the future depth of a given\nscene. Moreover, the various limitations of the aforementioned methods were\naddressed and a video-to-video depth maps model was proposed. This model\nleverages the spatio-temporal consistency of the input and output sequence to\npredict a more accurate depth sequence output. These methods have significant\napplications in autonomous driving (AD) and advanced driver assistance systems\n(ADAS).\n"
    },
    {
        "title": "Harmonious Group Choreography with Trajectory-Controllable Diffusion",
        "published_time": "2024-03-10T12:11:34Z",
        "abstract": "  Creating group choreography from music has gained attention in cultural\nentertainment and virtual reality, aiming to coordinate visually cohesive and\ndiverse group movements. Despite increasing interest, recent works face\nchallenges in achieving aesthetically appealing choreography, primarily for two\nkey issues: multi-dancer collision and single-dancer foot slide. To address\nthese issues, we propose a Trajectory-Controllable Diffusion (TCDiff), a novel\napproach that harnesses non-overlapping trajectories to facilitate coherent\ndance movements. Specifically, to tackle dancer collisions, we introduce a\nDance-Beat Navigator capable of generating trajectories for multiple dancers\nbased on the music, complemented by a Distance-Consistency loss to maintain\nappropriate spacing among trajectories within a reasonable threshold. To\nmitigate foot sliding, we present a Footwork Adaptor that utilizes trajectory\ndisplacement from adjacent frames to enable flexible footwork, coupled with a\nRelative Forward-Kinematic loss to adjust the positioning of individual\ndancers' root nodes and joints. Extensive experiments demonstrate that our\nmethod achieves state-of-the-art results.\n"
    },
    {
        "title": "DiffuMatting: Synthesizing Arbitrary Objects with Matting-level\n  Annotation",
        "published_time": "2024-03-10T10:39:32Z",
        "abstract": "  Due to the difficulty and labor-consuming nature of getting highly accurate\nor matting annotations, there only exists a limited amount of highly accurate\nlabels available to the public. To tackle this challenge, we propose a\nDiffuMatting which inherits the strong Everything generation ability of\ndiffusion and endows the power of \"matting anything\". Our DiffuMatting can 1).\nact as an anything matting factory with high accurate annotations 2). be\nwell-compatible with community LoRAs or various conditional control approaches\nto achieve the community-friendly art design and controllable generation.\nSpecifically, inspired by green-screen-matting, we aim to teach the diffusion\nmodel to paint on a fixed green screen canvas. To this end, a large-scale\ngreenscreen dataset (Green100K) is collected as a training dataset for\nDiffuMatting. Secondly, a green background control loss is proposed to keep the\ndrawing board as a pure green color to distinguish the foreground and\nbackground. To ensure the synthesized object has more edge details, a\ndetailed-enhancement of transition boundary loss is proposed as a guideline to\ngenerate objects with more complicated edge structures. Aiming to\nsimultaneously generate the object and its matting annotation, we build a\nmatting head to make a green color removal in the latent space of the VAE\ndecoder. Our DiffuMatting shows several potential applications (e.g.,\nmatting-data generator, community-friendly art design and controllable\ngeneration). As a matting-data generator, DiffuMatting synthesizes general\nobject and portrait matting sets, effectively reducing the relative MSE error\nby 15.4% in General Object Matting and 11.4% in Portrait Matting tasks.\n"
    },
    {
        "title": "Large receptive field strategy and important feature extraction strategy\n  in 3D object detection",
        "published_time": "2024-01-22T13:01:28Z",
        "abstract": "  The enhancement of 3D object detection is pivotal for precise environmental\nperception and improved task execution capabilities in autonomous driving.\nLiDAR point clouds, offering accurate depth information, serve as a crucial\ninformation for this purpose. Our study focuses on key challenges in 3D target\ndetection. To tackle the challenge of expanding the receptive field of a 3D\nconvolutional kernel, we introduce the Dynamic Feature Fusion Module (DFFM).\nThis module achieves adaptive expansion of the 3D convolutional kernel's\nreceptive field, balancing the expansion with acceptable computational loads.\nThis innovation reduces operations, expands the receptive field, and allows the\nmodel to dynamically adjust to different object requirements. Simultaneously,\nwe identify redundant information in 3D features. Employing the Feature\nSelection Module (FSM) quantitatively evaluates and eliminates non-important\nfeatures, achieving the separation of output box fitting and feature\nextraction. This innovation enables the detector to focus on critical features,\nresulting in model compression, reduced computational burden, and minimized\ncandidate frame interference. Extensive experiments confirm that both DFFM and\nFSM not only enhance current benchmarks, particularly in small target\ndetection, but also accelerate network performance. Importantly, these modules\nexhibit effective complementarity.\n"
    },
    {
        "title": "Cross-Cluster Shifting for Efficient and Effective 3D Object Detection\n  in Autonomous Driving",
        "published_time": "2024-03-10T10:36:32Z",
        "abstract": "  We present a new 3D point-based detector model, named Shift-SSD, for precise\n3D object detection in autonomous driving. Traditional point-based 3D object\ndetectors often employ architectures that rely on a progressive downsampling of\npoints. While this method effectively reduces computational demands and\nincreases receptive fields, it will compromise the preservation of crucial\nnon-local information for accurate 3D object detection, especially in the\ncomplex driving scenarios. To address this, we introduce an intriguing\nCross-Cluster Shifting operation to unleash the representation capacity of the\npoint-based detector by efficiently modeling longer-range inter-dependency\nwhile including only a negligible overhead. Concretely, the Cross-Cluster\nShifting operation enhances the conventional design by shifting partial\nchannels from neighboring clusters, which enables richer interaction with\nnon-local regions and thus enlarges the receptive field of clusters. We conduct\nextensive experiments on the KITTI, Waymo, and nuScenes datasets, and the\nresults demonstrate the state-of-the-art performance of Shift-SSD in both\ndetection accuracy and runtime efficiency.\n"
    },
    {
        "title": "Platypose: Calibrated Zero-Shot Multi-Hypothesis 3D Human Motion\n  Estimation",
        "published_time": "2024-03-10T10:30:34Z",
        "abstract": "  Single camera 3D pose estimation is an ill-defined problem due to inherent\nambiguities from depth, occlusion or keypoint noise. Multi-hypothesis pose\nestimation accounts for this uncertainty by providing multiple 3D poses\nconsistent with the 2D measurements. Current research has predominantly\nconcentrated on generating multiple hypotheses for single frame static pose\nestimation. In this study we focus on the new task of multi-hypothesis motion\nestimation. Motion estimation is not simply pose estimation applied to multiple\nframes, which would ignore temporal correlation across frames. Instead, it\nrequires distributions which are capable of generating temporally consistent\nsamples, which is significantly more challenging. To this end, we introduce\nPlatypose, a framework that uses a diffusion model pretrained on 3D human\nmotion sequences for zero-shot 3D pose sequence estimation. Platypose\noutperforms baseline methods on multiple hypotheses for motion estimation.\nAdditionally, Platypose also achieves state-of-the-art calibration and\ncompetitive joint error when tested on static poses from Human3.6M,\nMPI-INF-3DHP and 3DPW. Finally, because it is zero-shot, our method generalizes\nflexibly to different settings such as multi-camera inference.\n"
    },
    {
        "title": "Cracking the neural code for word recognition in convolutional neural\n  networks",
        "published_time": "2024-03-10T10:12:32Z",
        "abstract": "  Learning to read places a strong challenge on the visual system. Years of\nexpertise lead to a remarkable capacity to separate highly similar letters and\nencode their relative positions, thus distinguishing words such as FORM and\nFROM, invariantly over a large range of sizes and absolute positions. How\nneural circuits achieve invariant word recognition remains unknown. Here, we\naddress this issue by training deep neural network models to recognize written\nwords and then analyzing how reading-specialized units emerge and operate\nacross different layers of the network. With literacy, a small subset of units\nbecomes specialized for word recognition in the learned script, similar to the\n\"visual word form area\" of the human brain. We show that these units are\nsensitive to specific letter identities and their distance from the blank space\nat the left or right of a word, thus acting as \"space bigrams\". These units\nspecifically encode ordinal positions and operate by pooling across low and\nhigh-frequency detector units from early layers of the network. The proposed\nneural code provides a mechanistic insight into how information on letter\nidentity and position is extracted and allow for invariant word recognition,\nand leads to predictions for reading behavior, error patterns, and the\nneurophysiology of reading.\n"
    },
    {
        "title": "GPTSee: Enhancing Moment Retrieval and Highlight Detection via\n  Description-Based Similarity Features",
        "published_time": "2024-03-03T08:24:28Z",
        "abstract": "  Moment retrieval (MR) and highlight detection (HD) aim to identify relevant\nmoments and highlights in video from corresponding natural language query.\nLarge language models (LLMs) have demonstrated proficiency in various computer\nvision tasks. However, existing methods for MR\\&HD have not yet been integrated\nwith LLMs. In this letter, we propose a novel two-stage model that takes the\noutput of LLMs as the input to the second-stage transformer encoder-decoder.\nFirst, MiniGPT-4 is employed to generate the detailed description of the video\nframe and rewrite the query statement, fed into the encoder as new features.\nThen, semantic similarity is computed between the generated description and the\nrewritten queries. Finally, continuous high-similarity video frames are\nconverted into span anchors, serving as prior position information for the\ndecoder. Experiments demonstrate that our approach achieves a state-of-the-art\nresult, and by using only span anchors and similarity scores as outputs,\npositioning accuracy outperforms traditional methods, like Moment-DETR.\n"
    },
    {
        "title": "Decoupled Contrastive Learning for Long-Tailed Recognition",
        "published_time": "2024-03-10T09:46:28Z",
        "abstract": "  Supervised Contrastive Loss (SCL) is popular in visual representation\nlearning. Given an anchor image, SCL pulls two types of positive samples, i.e.,\nits augmentation and other images from the same class together, while pushes\nnegative images apart to optimize the learned embedding. In the scenario of\nlong-tailed recognition, where the number of samples in each class is\nimbalanced, treating two types of positive samples equally leads to the biased\noptimization for intra-category distance. In addition, similarity relationship\namong negative samples, that are ignored by SCL, also presents meaningful\nsemantic cues. To improve the performance on long-tailed recognition, this\npaper addresses those two issues of SCL by decoupling the training objective.\nSpecifically, it decouples two types of positives in SCL and optimizes their\nrelations toward different objectives to alleviate the influence of the\nimbalanced dataset. We further propose a patch-based self distillation to\ntransfer knowledge from head to tail classes to relieve the\nunder-representation of tail classes. It uses patch-based features to mine\nshared visual patterns among different instances and leverages a self\ndistillation procedure to transfer such knowledge. Experiments on different\nlong-tailed classification benchmarks demonstrate the superiority of our\nmethod. For instance, it achieves the 57.7% top-1 accuracy on the ImageNet-LT\ndataset. Combined with the ensemble-based method, the performance can be\nfurther boosted to 59.7%, which substantially outperforms many recent works.\nThe code is available at https://github.com/SY-Xuan/DSCL.\n"
    },
    {
        "title": "All-in-one platform for AI R&D in medical imaging, encompassing data\n  collection, selection, annotation, and pre-processing",
        "published_time": "2024-03-10T09:24:53Z",
        "abstract": "  Deep Learning is advancing medical imaging Research and Development (R&D),\nleading to the frequent clinical use of Artificial Intelligence/Machine\nLearning (AI/ML)-based medical devices. However, to advance AI R&D, two\nchallenges arise: 1) significant data imbalance, with most data from\nEurope/America and under 10% from Asia, despite its 60% global population\nshare; and 2) hefty time and investment needed to curate proprietary datasets\nfor commercial use. In response, we established the first commercial medical\nimaging platform, encompassing steps like: 1) data collection, 2) data\nselection, 3) annotation, and 4) pre-processing. Moreover, we focus on\nharnessing under-represented data from Japan and broader Asia, including\nComputed Tomography, Magnetic Resonance Imaging, and Whole Slide Imaging scans.\nUsing the collected data, we are preparing/providing ready-to-use datasets for\nmedical AI R&D by 1) offering these datasets to AI firms, biopharma, and\nmedical device makers and 2) using them as training/test data to develop\ntailored AI solutions for such entities. We also aim to merge Blockchain for\ndata security and plan to synthesize rare disease data via generative AI.\nDataHub Website: https://medical-datahub.ai/\n"
    },
    {
        "title": "Bayesian Random Semantic Data Augmentation for Medical Image\n  Classification",
        "published_time": "2024-03-10T08:56:02Z",
        "abstract": "  Data augmentation is a critical regularization technique for deep neural\nnetworks, particularly in medical image classification. Popular data\naugmentation approaches include image transformation-based methods, generative\ndata augmentation, and automatic data augmentation. However, these approaches\nencounter notable limitations: image transformation-based and automated data\naugmentation techniques cannot implement semantic transformations, leading to a\nconstrained variety of augmented samples, and generative data augmentation\nmethods are computationally expensive. In response to these challenges, we\nproposed Bayesian Random Semantic Data Augmentation (BRSDA), a novel,\nefficient, and plug-and-play semantic data augmentation method. BRSDA is\nmotivated by a simple translation in the feature space along specific\ndirections that can effectuate semantic transformations. When given a feature,\nwe define its augmentable semantic magnitude as a random variable and estimate\nits distribution using variational Bayesian, then sample semantic magnitude and\nadd to the randomly selected semantic direction to achieve semantic data\naugmentation. We demonstrate the effectiveness of BRSDA on five 2D and six 3D\nmedical image datasets covering nine modalities. We also test BRSDA with\nmainstream neural network architectures, showcasing its robustness.\nFurthermore, combining BRSDA with other leading data augmentation methods\nachieves superior performance. Code is available online at\n\\url{https://github.com/YaoyaoZhu19/BRSDA}.\n"
    },
    {
        "title": "RESTORE: Towards Feature Shift for Vision-Language Prompt Learning",
        "published_time": "2024-03-10T08:52:48Z",
        "abstract": "  Prompt learning is effective for fine-tuning foundation models to improve\ntheir generalization across a variety of downstream tasks. However, the prompts\nthat are independently optimized along a single modality path, may sacrifice\nthe vision-language alignment of pre-trained models in return for improved\nperformance on specific tasks and classes, leading to poorer generalization. In\nthis paper, we first demonstrate that prompt tuning along only one single\nbranch of CLIP (e.g., language or vision) is the reason why the misalignment\noccurs. Without proper regularization across the learnable parameters in\ndifferent modalities, prompt learning violates the original pre-training\nconstraints inherent in the two-tower architecture. To address such\nmisalignment, we first propose feature shift, which is defined as the variation\nof embeddings after introducing the learned prompts, to serve as an explanatory\ntool. We dive into its relation with generalizability and thereafter propose\nRESTORE, a multi-modal prompt learning method that exerts explicit constraints\non cross-modal consistency. To be more specific, to prevent feature\nmisalignment, a feature shift consistency is introduced to synchronize\ninter-modal feature shifts by measuring and regularizing the magnitude of\ndiscrepancy during prompt tuning. In addition, we propose a \"surgery\" block to\navoid short-cut hacking, where cross-modal misalignment can still be severe if\nthe feature shift of each modality varies drastically at the same rate. It is\nimplemented as feed-forward adapters upon both modalities to alleviate the\nmisalignment problem. Extensive experiments on 15 datasets demonstrate that our\nmethod outperforms the state-of-the-art prompt tuning methods without\ncompromising feature alignment.\n"
    },
    {
        "title": "Learning Exposure Correction in Dynamic Scenes",
        "published_time": "2024-02-27T08:19:51Z",
        "abstract": "  Capturing videos with wrong exposure usually produces unsatisfactory visual\neffects. While image exposure correction is a popular topic, the video\ncounterpart is less explored in the literature. Directly applying prior\nimage-based methods to input videos often results in temporal incoherence with\nlow visual quality. Existing research in this area is also limited by the lack\nof high-quality benchmark datasets. To address these issues, we construct the\nfirst real-world paired video dataset, including both underexposure and\noverexposure dynamic scenes. To achieve spatial alignment, we utilize two DSLR\ncameras and a beam splitter to simultaneously capture improper and normal\nexposure videos. In addition, we propose a Video Exposure Correction Network\n(VECNet) based on Retinex theory, which incorporates a two-stream illumination\nlearning mechanism to enhance the overexposure and underexposure factors,\nrespectively. The estimated multi-frame reflectance and dual-path illumination\ncomponents are fused at both feature and image levels, leading to visually\nappealing results. Experimental results demonstrate that the proposed method\noutperforms existing image exposure correction and underexposed video\nenhancement methods. The code and dataset will be available soon.\n"
    },
    {
        "title": "MACE: Mass Concept Erasure in Diffusion Models",
        "published_time": "2024-03-10T08:50:56Z",
        "abstract": "  The rapid expansion of large-scale text-to-image diffusion models has raised\ngrowing concerns regarding their potential misuse in creating harmful or\nmisleading content. In this paper, we introduce MACE, a finetuning framework\nfor the task of mass concept erasure. This task aims to prevent models from\ngenerating images that embody unwanted concepts when prompted. Existing concept\nerasure methods are typically restricted to handling fewer than five concepts\nsimultaneously and struggle to find a balance between erasing concept synonyms\n(generality) and maintaining unrelated concepts (specificity). In contrast,\nMACE differs by successfully scaling the erasure scope up to 100 concepts and\nby achieving an effective balance between generality and specificity. This is\nachieved by leveraging closed-form cross-attention refinement along with LoRA\nfinetuning, collectively eliminating the information of undesirable concepts.\nFurthermore, MACE integrates multiple LoRAs without mutual interference. We\nconduct extensive evaluations of MACE against prior methods across four\ndifferent tasks: object erasure, celebrity erasure, explicit content erasure,\nand artistic style erasure. Our results reveal that MACE surpasses prior\nmethods in all evaluated tasks. Code is available at\nhttps://github.com/Shilin-LU/MACE.\n"
    },
    {
        "title": "TNF: Tri-branch Neural Fusion for Multimodal Medical Data Classification",
        "published_time": "2024-03-04T07:47:05Z",
        "abstract": "  This paper presents a Tri-branch Neural Fusion (TNF) approach designed for\nclassifying multimodal medical images and tabular data. It also introduces two\nsolutions to address the challenge of label inconsistency in multimodal\nclassification. Traditional methods in multi-modality medical data\nclassification often rely on single-label approaches, typically merging\nfeatures from two distinct input modalities. This becomes problematic when\nfeatures are mutually exclusive or labels differ across modalities, leading to\nreduced accuracy. To overcome this, our TNF approach implements a tri-branch\nframework that manages three separate outputs: one for image modality, another\nfor tabular modality, and a third hybrid output that fuses both image and\ntabular data. The final decision is made through an ensemble method that\nintegrates likelihoods from all three branches. We validate the effectiveness\nof TNF through extensive experiments, which illustrate its superiority over\ntraditional fusion and ensemble methods in various convolutional neural\nnetworks and transformer-based architectures across multiple datasets.\n"
    },
    {
        "title": "ClickVOS: Click Video Object Segmentation",
        "published_time": "2024-03-10T08:37:37Z",
        "abstract": "  Video Object Segmentation (VOS) task aims to segment objects in videos.\nHowever, previous settings either require time-consuming manual masks of target\nobjects at the first frame during inference or lack the flexibility to specify\narbitrary objects of interest. To address these limitations, we propose the\nsetting named Click Video Object Segmentation (ClickVOS) which segments objects\nof interest across the whole video according to a single click per object in\nthe first frame. And we provide the extended datasets DAVIS-P and YouTubeVOSP\nthat with point annotations to support this task. ClickVOS is of significant\npractical applications and research implications due to its only 1-2 seconds\ninteraction time for indicating an object, comparing annotating the mask of an\nobject needs several minutes. However, ClickVOS also presents increased\nchallenges. To address this task, we propose an end-to-end baseline approach\nnamed called Attention Before Segmentation (ABS), motivated by the attention\nprocess of humans. ABS utilizes the given point in the first frame to perceive\nthe target object through a concise yet effective segmentation attention.\nAlthough the initial object mask is possibly inaccurate, in our ABS, as the\nvideo goes on, the initially imprecise object mask can self-heal instead of\ndeteriorating due to error accumulation, which is attributed to our designed\nimprovement memory that continuously records stable global object memory and\nupdates detailed dense memory. In addition, we conduct various baseline\nexplorations utilizing off-the-shelf algorithms from related fields, which\ncould provide insights for the further exploration of ClickVOS. The\nexperimental results demonstrate the superiority of the proposed ABS approach.\nExtended datasets and codes will be available at\nhttps://github.com/PinxueGuo/ClickVOS.\n"
    },
    {
        "title": "Low-dose CT Denoising with Language-engaged Dual-space Alignment",
        "published_time": "2024-03-10T08:21:50Z",
        "abstract": "  While various deep learning methods were proposed for low-dose computed\ntomography (CT) denoising, they often suffer from over-smoothing, blurring, and\nlack of explainability. To alleviate these issues, we propose a plug-and-play\nLanguage-Engaged Dual-space Alignment loss (LEDA) to optimize low-dose CT\ndenoising models. Our idea is to leverage large language models (LLMs) to align\ndenoised CT and normal dose CT images in both the continuous perceptual space\nand discrete semantic space, which is the first LLM-based scheme for low-dose\nCT denoising. LEDA involves two steps: the first is to pretrain an LLM-guided\nCT autoencoder, which can encode a CT image into continuous high-level features\nand quantize them into a token space to produce semantic tokens derived from\nthe LLM's vocabulary; and the second is to minimize the discrepancy between the\ndenoised CT images and normal dose CT in terms of both encoded high-level\nfeatures and quantized token embeddings derived by the LLM-guided CT\nautoencoder. Extensive experimental results on two public LDCT denoising\ndatasets demonstrate that our LEDA can enhance existing denoising models in\nterms of quantitative metrics and qualitative evaluation, and also provide\nexplainability through language-level image understanding. Source code is\navailable at https://github.com/hao1635/LEDA.\n"
    },
    {
        "title": "In-context Prompt Learning for Test-time Vision Recognition with Frozen\n  Vision-language Model",
        "published_time": "2024-03-10T08:15:51Z",
        "abstract": "  Existing pre-trained vision-language models, e.g., CLIP, have demonstrated\nimpressive zero-shot generalization capabilities in various downstream tasks.\nHowever, the performance of these models will degrade significantly when test\ninputs present different distributions. To this end, we explore the concept of\ntest-time prompt tuning (TTPT), which enables the adaptation of the CLIP model\nto novel downstream tasks through only one step of optimization on an\nunsupervised objective that involves the test sample. Motivated by in-context\nlearning within field of natural language processing (NLP), we propose\nIn-Context Prompt Learning (InCPL) for test-time visual recognition task. InCPL\ninvolves associating a new test sample with very few or even just one labeled\nexample as its in-context prompt. As a result, it can reliably estimate a label\nfor the test sample, thereby facilitating the model adaptation process. InCPL\nfirst employs a token net to represent language descriptions as visual prompts\nthat the vision encoder of a CLIP model can comprehend. Paired with in-context\nexamples, we further propose a context-aware unsupervised loss to optimize test\nsample-aware visual prompts. This optimization allows a pre-trained, frozen\nCLIP model to be adapted to a test sample from any task using its learned\nadaptive prompt. Our method has demonstrated superior performance and achieved\nstate-of-the-art results across various downstream datasets.\n"
    },
    {
        "title": "PSS-BA: LiDAR Bundle Adjustment with Progressive Spatial Smoothing",
        "published_time": "2024-03-10T07:56:54Z",
        "abstract": "  Accurate and consistent construction of point clouds from LiDAR scanning data\nis fundamental for 3D modeling applications. Current solutions, such as\nmultiview point cloud registration and LiDAR bundle adjustment, predominantly\ndepend on the local plane assumption, which may be inadequate in complex\nenvironments lacking of planar geometries or substantial initial pose errors.\nTo mitigate this problem, this paper presents a LiDAR bundle adjustment with\nprogressive spatial smoothing, which is suitable for complex environments and\nexhibits improved convergence capabilities. The proposed method consists of a\nspatial smoothing module and a pose adjustment module, which combines the\nbenefits of local consistency and global accuracy. With the spatial smoothing\nmodule, we can obtain robust and rich surface constraints employing smoothing\nkernels across various scales. Then the pose adjustment module corrects all\nposes utilizing the novel surface constraints. Ultimately, the proposed method\nsimultaneously achieves fine poses and parametric surfaces that can be directly\nemployed for high-quality point cloud reconstruction. The effectiveness and\nrobustness of our proposed approach have been validated on both simulation and\nreal-world datasets. The experimental results demonstrate that the proposed\nmethod outperforms the existing methods and achieves better accuracy in complex\nenvironments with low planar structures.\n"
    },
    {
        "title": "Style Blind Domain Generalized Semantic Segmentation via Covariance\n  Alignment and Semantic Consistence Contrastive Learning",
        "published_time": "2024-03-10T07:44:41Z",
        "abstract": "  Deep learning models for semantic segmentation often experience performance\ndegradation when deployed to unseen target domains unidentified during the\ntraining phase. This is mainly due to variations in image texture (\\ie style)\nfrom different data sources. To tackle this challenge, existing domain\ngeneralized semantic segmentation (DGSS) methods attempt to remove style\nvariations from the feature. However, these approaches struggle with the\nentanglement of style and content, which may lead to the unintentional removal\nof crucial content information, causing performance degradation. This study\naddresses this limitation by proposing BlindNet, a novel DGSS approach that\nblinds the style without external modules or datasets. The main idea behind our\nproposed approach is to alleviate the effect of style in the encoder whilst\nfacilitating robust segmentation in the decoder. To achieve this, BlindNet\ncomprises two key components: covariance alignment and semantic consistency\ncontrastive learning. Specifically, the covariance alignment trains the encoder\nto uniformly recognize various styles and preserve the content information of\nthe feature, rather than removing the style-sensitive factor. Meanwhile,\nsemantic consistency contrastive learning enables the decoder to construct\ndiscriminative class embedding space and disentangles features that are\nvulnerable to misclassification. Through extensive experiments, our approach\noutperforms existing DGSS methods, exhibiting robustness and superior\nperformance for semantic segmentation on unseen target domains.\n"
    },
    {
        "title": "CLEAR: Cross-Transformers with Pre-trained Language Model is All you\n  need for Person Attribute Recognition and Retrieval",
        "published_time": "2024-03-10T07:31:06Z",
        "abstract": "  Person attribute recognition and attribute-based retrieval are two core\nhuman-centric tasks. In the recognition task, the challenge is specifying\nattributes depending on a person's appearance, while the retrieval task\ninvolves searching for matching persons based on attribute queries. There is a\nsignificant relationship between recognition and retrieval tasks. In this\nstudy, we demonstrate that if there is a sufficiently robust network to solve\nperson attribute recognition, it can be adapted to facilitate better\nperformance for the retrieval task. Another issue that needs addressing in the\nretrieval task is the modality gap between attribute queries and persons'\nimages. Therefore, in this paper, we present CLEAR, a unified network designed\nto address both tasks. We introduce a robust cross-transformers network to\nhandle person attribute recognition. Additionally, leveraging a pre-trained\nlanguage model, we construct pseudo-descriptions for attribute queries and\nintroduce an effective training strategy to train only a few additional\nparameters for adapters, facilitating the handling of the retrieval task.\nFinally, the unified CLEAR model is evaluated on five benchmarks: PETA, PA100K,\nMarket-1501, RAPv2, and UPAR-2024. Without bells and whistles, CLEAR achieves\nstate-of-the-art performance or competitive results for both tasks,\nsignificantly outperforming other competitors in terms of person retrieval\nperformance on the widely-used Market-1501 dataset.\n"
    },
    {
        "title": "nnMamba: 3D Biomedical Image Segmentation, Classification and Landmark\n  Detection with State Space Model",
        "published_time": "2024-02-05T21:28:47Z",
        "abstract": "  In the field of biomedical image analysis, the quest for architectures\ncapable of effectively capturing long-range dependencies is paramount,\nespecially when dealing with 3D image segmentation, classification, and\nlandmark detection. Traditional Convolutional Neural Networks (CNNs) struggle\nwith locality respective field, and Transformers have a heavy computational\nload when applied to high-dimensional medical images.In this paper, we\nintroduce nnMamba, a novel architecture that integrates the strengths of CNNs\nand the advanced long-range modeling capabilities of State Space Sequence\nModels (SSMs). Specifically, we propose the Mamba-In-Convolution with\nChannel-Spatial Siamese learning (MICCSS) block to model the long-range\nrelationship of the voxels. For the dense prediction and classification tasks,\nwe also design the channel-scaling and channel-sequential learning methods.\nExtensive experiments on 6 datasets demonstrate nnMamba's superiority over\nstate-of-the-art methods in a suite of challenging tasks, including 3D image\nsegmentation, classification, and landmark detection. nnMamba emerges as a\nrobust solution, offering both the local representation ability of CNNs and the\nefficient global context processing of SSMs, setting a new standard for\nlong-range dependency modeling in medical image analysis. Code is available at\nhttps://github.com/lhaof/nnMamba\n"
    },
    {
        "title": "Adversarial Training on Purification (AToP): Advancing Both Robustness\n  and Generalization",
        "published_time": "2024-01-29T17:56:42Z",
        "abstract": "  The deep neural networks are known to be vulnerable to well-designed\nadversarial attacks. The most successful defense technique based on adversarial\ntraining (AT) can achieve optimal robustness against particular attacks but\ncannot generalize well to unseen attacks. Another effective defense technique\nbased on adversarial purification (AP) can enhance generalization but cannot\nachieve optimal robustness. Meanwhile, both methods share one common limitation\non the degraded standard accuracy. To mitigate these issues, we propose a novel\npipeline called Adversarial Training on Purification (AToP), which comprises\ntwo components: perturbation destruction by random transforms (RT) and purifier\nmodel fine-tuned (FT) by adversarial loss. RT is essential to avoid\noverlearning to known attacks resulting in the robustness generalization to\nunseen attacks and FT is essential for the improvement of robustness. To\nevaluate our method in an efficient and scalable way, we conduct extensive\nexperiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our\nmethod achieves state-of-the-art results and exhibits generalization ability\nagainst unseen attacks.\n"
    },
    {
        "title": "Textureless Object Recognition: An Edge-based Approach",
        "published_time": "2024-03-10T06:27:04Z",
        "abstract": "  Textureless object recognition has become a significant task in Computer\nVision with the advent of Robotics and its applications in manufacturing\nsector. It has been challenging to obtain good accuracy in real time because of\nits lack of discriminative features and reflectance properties which makes the\ntechniques for textured object recognition insufficient for textureless\nobjects. A lot of work has been done in the last 20 years, especially in the\nrecent 5 years after the TLess and other textureless dataset were introduced.\nIn this project, by applying image processing techniques we created a robust\naugmented dataset from initial imbalanced smaller dataset. We extracted edge\nfeatures, feature combinations and RGB images enhanced with feature/feature\ncombinations to create 15 datasets, each with a size of ~340,000. We then\ntrained four classifiers on these 15 datasets to arrive at a conclusion as to\nwhich dataset performs the best overall and whether edge features are important\nfor textureless objects. Based on our experiments and analysis, RGB images\nenhanced with combination of 3 edge features performed the best compared to all\nothers. Model performance on dataset with HED edges performed comparatively\nbetter than other edge detectors like Canny or Prewitt.\n"
    },
    {
        "title": "Universal Debiased Editing for Fair Medical Image Classification",
        "published_time": "2024-03-10T06:15:42Z",
        "abstract": "  In the era of Foundation Models' (FMs) rising prominence in AI, our study\naddresses the challenge of biases in medical images while using FM API,\nparticularly spurious correlations between pixels and sensitive attributes.\nTraditional methods for bias mitigation face limitations due to the restricted\naccess to web-hosted FMs and difficulties in addressing the underlying bias\nencoded within the FM API. We propose an U(niversal) D(ebiased) E(diting)\nstrategy, termed UDE, which generates UDE noise to mask such spurious\ncorrelation. UDE is capable of mitigating bias both within the FM API embedding\nand the images themselves. Furthermore, UDE is suitable for both white-box and\nblack-box FM APIs, where we introduced G(reedy) (Z)eroth-O(rder) (GeZO)\noptimization for it when the gradient is inaccessible in black-box APIs. Our\nwhole pipeline enables fairness-aware image editing that can be applied across\nvarious medical contexts without requiring direct model manipulation or\nsignificant computational resources. Our empirical results demonstrate the\nmethod's effectiveness in maintaining fairness and utility across different\npatient groups and diseases. In the era of AI-driven medicine, this work\ncontributes to making healthcare diagnostics more equitable, showcasing a\npractical solution for bias mitigation in pre-trained image FMs.\n"
    },
    {
        "title": "Coherent Temporal Synthesis for Incremental Action Segmentation",
        "published_time": "2024-03-10T06:07:06Z",
        "abstract": "  Data replay is a successful incremental learning technique for images. It\nprevents catastrophic forgetting by keeping a reservoir of previous data,\noriginal or synthesized, to ensure the model retains past knowledge while\nadapting to novel concepts. However, its application in the video domain is\nrudimentary, as it simply stores frame exemplars for action recognition. This\npaper presents the first exploration of video data replay techniques for\nincremental action segmentation, focusing on action temporal modeling. We\npropose a Temporally Coherent Action (TCA) model, which represents actions\nusing a generative model instead of storing individual frames. The integration\nof a conditioning variable that captures temporal coherence allows our model to\nunderstand the evolution of action features over time. Therefore, action\nsegments generated by TCA for replay are diverse and temporally coherent. In a\n10-task incremental setup on the Breakfast dataset, our approach achieves\nsignificant increases in accuracy for up to 22% compared to the baselines.\n"
    },
    {
        "title": "NASH: Neural Architecture Search for Hardware-Optimized Machine Learning\n  Models",
        "published_time": "2024-03-04T08:51:38Z",
        "abstract": "  As machine learning (ML) algorithms get deployed in an ever-increasing number\nof applications, these algorithms need to achieve better trade-offs between\nhigh accuracy, high throughput and low latency. This paper introduces NASH, a\nnovel approach that applies neural architecture search to machine learning\nhardware. Using NASH, hardware designs can achieve not only high throughput and\nlow latency but also superior accuracy performance. We present four versions of\nthe NASH strategy in this paper, all of which show higher accuracy than the\noriginal models. The strategy can be applied to various convolutional neural\nnetworks, selecting specific model operations among many to guide the training\nprocess toward higher accuracy. Experimental results show that applying NASH on\nResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top\n5 accuracy increase of up to 2.2% compared to the non-NASH version when tested\non the ImageNet data set. We also integrated this approach into the FINN\nhardware model synthesis tool to automate the application of our approach and\nthe generation of the hardware model. Results show that using FINN can achieve\na maximum throughput of 324.5 fps. In addition, NASH models can also result in\na better trade-off between accuracy and hardware resource utilization. The\naccuracy-hardware (HW) Pareto curve shows that the models with the four NASH\nversions represent the best trade-offs achieving the highest accuracy for a\ngiven HW utilization. The code for our implementation is open-source and\npublicly available on GitHub at https://github.com/MFJI/NASH.\n"
    },
    {
        "title": "VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video\n  Diffusion Models",
        "published_time": "2024-03-10T05:40:12Z",
        "abstract": "  The arrival of Sora marks a new era for text-to-video diffusion models,\nbringing significant advancements in video generation and potential\napplications. However, Sora, as well as other text-to-video diffusion models,\nhighly relies on the prompts, and there is no publicly available dataset\nfeaturing a study of text-to-video prompts. In this paper, we introduce\nVidProM, the first large-scale dataset comprising 1.67 million unique\ntext-to-video prompts from real users. Additionally, the dataset includes 6.69\nmillion videos generated by four state-of-the-art diffusion models and some\nrelated data. We initially demonstrate the curation of this large-scale\ndataset, which is a time-consuming and costly process. Subsequently, we show\nhow the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery\ndataset for image generation. Based on the analysis of these prompts, we\nidentify the necessity for a new prompt dataset specifically designed for\ntext-to-video generation and gain insights into the preferences of real users\nwhen creating videos. Our large-scale and diverse dataset also inspires many\nexciting new research areas. For instance, to develop better, more efficient,\nand safer text-to-video diffusion models, we suggest exploring text-to-video\nprompt engineering, efficient video generation, and video copy detection for\ndiffusion models. We make the collected dataset VidProM publicly available at\nGitHub and Hugging Face under the CC-BY- NC 4.0 License.\n"
    },
    {
        "title": "Unbiased Estimator for Distorted Conics in Camera Calibration",
        "published_time": "2024-03-07T15:29:11Z",
        "abstract": "  In the literature, points and conics have been major features for camera\ngeometric calibration. Although conics are more informative features than\npoints, the loss of the conic property under distortion has critically limited\nthe utility of conic features in camera calibration. Many existing approaches\naddressed conic-based calibration by ignoring distortion or introducing 3D\nspherical targets to circumvent this limitation. In this paper, we present a\nnovel formulation for conic-based calibration using moments. Our derivation is\nbased on the mathematical finding that the first moment can be estimated\nwithout bias even under distortion. This allows us to track moment changes\nduring projection and distortion, ensuring the preservation of the first moment\nof the distorted conic. With an unbiased estimator, the circular patterns can\nbe accurately detected at the sub-pixel level and can now be fully exploited\nfor an entire calibration pipeline, resulting in significantly improved\ncalibration. The entire code is readily available from\nhttps://github.com/ChaehyeonSong/discocal.\n"
    },
    {
        "title": "Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors",
        "published_time": "2024-03-10T04:38:27Z",
        "abstract": "  Multi-camera-based 3D object detection has made notable progress in the past\nseveral years. However, we observe that there are cases (e.g. faraway regions)\nin which popular 2D object detectors are more reliable than state-of-the-art 3D\ndetectors. In this paper, to improve the performance of query-based 3D object\ndetectors, we present a novel query generating approach termed QAF2D, which\ninfers 3D query anchors from 2D detection results. A 2D bounding box of an\nobject in an image is lifted to a set of 3D anchors by associating each sampled\npoint within the box with depth, yaw angle, and size candidates. Then, the\nvalidity of each 3D anchor is verified by comparing its projection in the image\nwith its corresponding 2D box, and only valid anchors are kept and used to\nconstruct queries. The class information of the 2D bounding box associated with\neach query is also utilized to match the predicted boxes with ground truth for\nthe set-based loss. The image feature extraction backbone is shared between the\n3D detector and 2D detector by adding a small number of prompt parameters. We\nintegrate QAF2D into three popular query-based 3D object detectors and carry\nout comprehensive evaluations on the nuScenes dataset. The largest improvement\nthat QAF2D can bring about on the nuScenes validation subset is $2.3\\%$ NDS and\n$2.7\\%$ mAP. Code is available at https://github.com/nullmax-vision/QAF2D.\n"
    },
    {
        "title": "Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View\n  Synthesis?",
        "published_time": "2024-03-10T04:27:06Z",
        "abstract": "  Neural Radiance Field (NeRF) has achieved superior performance for novel view\nsynthesis by modeling the scene with a Multi-Layer Perception (MLP) and a\nvolume rendering procedure, however, when fewer known views are given (i.e.,\nfew-shot view synthesis), the model is prone to overfit the given views. To\nhandle this issue, previous efforts have been made towards leveraging learned\npriors or introducing additional regularizations. In contrast, in this paper,\nwe for the first time provide an orthogonal method from the perspective of\nnetwork structure. Given the observation that trivially reducing the number of\nmodel parameters alleviates the overfitting issue, but at the cost of missing\ndetails, we propose the multi-input MLP (mi-MLP) that incorporates the inputs\n(i.e., location and viewing direction) of the vanilla MLP into each layer to\nprevent the overfitting issue without harming detailed synthesis. To further\nreduce the artifacts, we propose to model colors and volume density separately\nand present two regularization terms. Extensive experiments on multiple\ndatasets demonstrate that: 1) although the proposed mi-MLP is easy to\nimplement, it is surprisingly effective as it boosts the PSNR of the baseline\nfrom $14.73$ to $24.23$. 2) the overall framework achieves state-of-the-art\nresults on a wide range of benchmarks. We will release the code upon\npublication.\n"
    },
    {
        "title": "Diffusion Models Trained with Large Data Are Transferable Visual Models",
        "published_time": "2024-03-10T04:23:24Z",
        "abstract": "  We show that, simply initializing image understanding models using a\npre-trained UNet (or transformer) of diffusion models, it is possible to\nachieve remarkable transferable performance on fundamental vision perception\ntasks using a moderate amount of target data (even synthetic data only),\nincluding monocular depth, surface normal, image segmentation, matting, human\npose estimation, among virtually many others. Previous works have adapted\ndiffusion models for various perception tasks, often reformulating these tasks\nas generation processes to align with the diffusion process. In sharp contrast,\nwe demonstrate that fine-tuning these models with minimal adjustments can be a\nmore effective alternative, offering the advantages of being embarrassingly\nsimple and significantly faster. As the backbone network of Stable Diffusion\nmodels is trained on giant datasets comprising billions of images, we observe\nvery robust generalization capabilities of the diffusion backbone. Experimental\nresults showcase the remarkable transferability of the backbone of diffusion\nmodels across diverse tasks and real-world datasets.\n"
    },
    {
        "title": "Knowledge Distillation of Convolutional Neural Networks through Feature\n  Map Transformation using Decision Trees",
        "published_time": "2024-03-10T04:20:51Z",
        "abstract": "  The interpretation of reasoning by Deep Neural Networks (DNN) is still\nchallenging due to their perceived black-box nature. Therefore, deploying DNNs\nin several real-world tasks is restricted by the lack of transparency of these\nmodels. We propose a distillation approach by extracting features from the\nfinal layer of the convolutional neural network (CNN) to address insights to\nits reasoning. The feature maps in the final layer of a CNN are transformed\ninto a one-dimensional feature vector using a fully connected layer.\nSubsequently, the extracted features are used to train a decision tree to\nachieve the best accuracy under constraints of depth and nodes. We use the\nmedical images of dermaMNIST, octMNIST, and pneumoniaMNIST from the medical\nMNIST datasets to demonstrate our proposed work. We observed that performance\nof the decision tree is as good as a CNN with minimum complexity. The results\nencourage interpreting decisions made by the CNNs using decision trees.\n"
    },
    {
        "title": "Towards In-Vehicle Multi-Task Facial Attribute Recognition:\n  Investigating Synthetic Data and Vision Foundation Models",
        "published_time": "2024-03-10T04:17:54Z",
        "abstract": "  In the burgeoning field of intelligent transportation systems, enhancing\nvehicle-driver interaction through facial attribute recognition, such as facial\nexpression, eye gaze, age, etc., is of paramount importance for safety,\npersonalization, and overall user experience. However, the scarcity of\ncomprehensive large-scale, real-world datasets poses a significant challenge\nfor training robust multi-task models. Existing literature often overlooks the\npotential of synthetic datasets and the comparative efficacy of\nstate-of-the-art vision foundation models in such constrained settings. This\npaper addresses these gaps by investigating the utility of synthetic datasets\nfor training complex multi-task models that recognize facial attributes of\npassengers of a vehicle, such as gaze plane, age, and facial expression.\nUtilizing transfer learning techniques with both pre-trained Vision Transformer\n(ViT) and Residual Network (ResNet) models, we explore various training and\nadaptation methods to optimize performance, particularly when data availability\nis limited. We provide extensive post-evaluation analysis, investigating the\neffects of synthetic data distributions on model performance in in-distribution\ndata and out-of-distribution inference. Our study unveils counter-intuitive\nfindings, notably the superior performance of ResNet over ViTs in our specific\nmulti-task context, which is attributed to the mismatch in model complexity\nrelative to task complexity. Our results highlight the challenges and\nopportunities for enhancing the use of synthetic data and vision foundation\nmodels in practical applications.\n"
    },
    {
        "title": "Multisize Dataset Condensation",
        "published_time": "2024-03-10T03:43:02Z",
        "abstract": "  While dataset condensation effectively enhances training efficiency, its\napplication in on-device scenarios brings unique challenges. 1) Due to the\nfluctuating computational resources of these devices, there's a demand for a\nflexible dataset size that diverges from a predefined size. 2) The limited\ncomputational power on devices often prevents additional condensation\noperations. These two challenges connect to the \"subset degradation problem\" in\ntraditional dataset condensation: a subset from a larger condensed dataset is\noften unrepresentative compared to directly condensing the whole dataset to\nthat smaller size. In this paper, we propose Multisize Dataset Condensation\n(MDC) by compressing N condensation processes into a single condensation\nprocess to obtain datasets with multiple sizes. Specifically, we introduce an\n\"adaptive subset loss\" on top of the basic condensation loss to mitigate the\n\"subset degradation problem\". Our MDC method offers several benefits: 1) No\nadditional condensation process is required; 2) reduced storage requirement by\nreusing condensed images. Experiments validate our findings on networks\nincluding ConvNet, ResNet and DenseNet, and datasets including SVHN, CIFAR-10,\nCIFAR-100 and ImageNet. For example, we achieved 6.40% average accuracy gains\non condensing CIFAR-10 to ten images per class. Code is available at:\nhttps://github.com/he-y/Multisize-Dataset-Condensation.\n"
    },
    {
        "title": "Bit-mask Robust Contrastive Knowledge Distillation for Unsupervised\n  Semantic Hashing",
        "published_time": "2024-03-10T03:33:59Z",
        "abstract": "  Unsupervised semantic hashing has emerged as an indispensable technique for\nfast image search, which aims to convert images into binary hash codes without\nrelying on labels. Recent advancements in the field demonstrate that employing\nlarge-scale backbones (e.g., ViT) in unsupervised semantic hashing models can\nyield substantial improvements. However, the inference delay has become\nincreasingly difficult to overlook. Knowledge distillation provides a means for\npractical model compression to alleviate this delay. Nevertheless, the\nprevailing knowledge distillation approaches are not explicitly designed for\nsemantic hashing. They ignore the unique search paradigm of semantic hashing,\nthe inherent necessities of the distillation process, and the property of hash\ncodes. In this paper, we propose an innovative Bit-mask Robust Contrastive\nknowledge Distillation (BRCD) method, specifically devised for the distillation\nof semantic hashing models. To ensure the effectiveness of two kinds of search\nparadigms in the context of semantic hashing, BRCD first aligns the semantic\nspaces between the teacher and student models through a contrastive knowledge\ndistillation objective. Additionally, to eliminate noisy augmentations and\nensure robust optimization, a cluster-based method within the knowledge\ndistillation process is introduced. Furthermore, through a bit-level analysis,\nwe uncover the presence of redundancy bits resulting from the bit independence\nproperty. To mitigate these effects, we introduce a bit mask mechanism in our\nknowledge distillation objective. Finally, extensive experiments not only\nshowcase the noteworthy performance of our BRCD method in comparison to other\nknowledge distillation methods but also substantiate the generality of our\nmethods across diverse semantic hashing models and backbones. The code for BRCD\nis available at https://github.com/hly1998/BRCD.\n"
    },
    {
        "title": "Reframe Anything: LLM Agent for Open World Video Reframing",
        "published_time": "2024-03-10T03:29:56Z",
        "abstract": "  The proliferation of mobile devices and social media has revolutionized\ncontent dissemination, with short-form video becoming increasingly prevalent.\nThis shift has introduced the challenge of video reframing to fit various\nscreen aspect ratios, a process that highlights the most compelling parts of a\nvideo. Traditionally, video reframing is a manual, time-consuming task\nrequiring professional expertise, which incurs high production costs. A\npotential solution is to adopt some machine learning models, such as video\nsalient object detection, to automate the process. However, these methods often\nlack generalizability due to their reliance on specific training data. The\nadvent of powerful large language models (LLMs) open new avenues for AI\ncapabilities. Building on this, we introduce Reframe Any Video Agent (RAVA), a\nLLM-based agent that leverages visual foundation models and human instructions\nto restructure visual content for video reframing. RAVA operates in three\nstages: perception, where it interprets user instructions and video content;\nplanning, where it determines aspect ratios and reframing strategies; and\nexecution, where it invokes the editing tools to produce the final video. Our\nexperiments validate the effectiveness of RAVA in video salient object\ndetection and real-world reframing tasks, demonstrating its potential as a tool\nfor AI-powered video editing.\n"
    },
    {
        "title": "Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and\n  Denoising",
        "published_time": "2024-03-10T03:22:57Z",
        "abstract": "  Conditional diffusion models have gained recognition for their effectiveness\nin image restoration tasks, yet their iterative denoising process, starting\nfrom Gaussian noise, often leads to slow inference speeds. As a promising\nalternative, the Image-to-Image Schr\\\"odinger Bridge (I2SB) initializes the\ngenerative process from corrupted images and integrates training techniques\nfrom conditional diffusion models. In this study, we extended the I2SB method\nby introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB),\ntransitioning its generative process to a non-Markovian process by\nincorporating corrupted images in each generative step. This enhancement\nempowers I3SB to generate images with better texture restoration using a small\nnumber of generative steps. The proposed method was validated on CT\nsuper-resolution and denoising tasks and outperformed existing methods,\nincluding the conditional denoising diffusion probabilistic model (cDDPM) and\nI2SB, in both visual quality and quantitative metrics. These findings\nunderscore the potential of I3SB in improving medical image restoration by\nproviding fast and accurate generative modeling.\n"
    },
    {
        "title": "CausalCellSegmenter: Causal Inference inspired Diversified Aggregation\n  Convolution for Pathology Image Segmentation",
        "published_time": "2024-03-10T03:04:13Z",
        "abstract": "  Deep learning models have shown promising performance for cell nucleus\nsegmentation in the field of pathology image analysis. However, training a\nrobust model from multiple domains remains a great challenge for cell nucleus\nsegmentation. Additionally, the shortcomings of background noise, highly\noverlapping between cell nucleus, and blurred edges often lead to poor\nperformance. To address these challenges, we propose a novel framework termed\nCausalCellSegmenter, which combines Causal Inference Module (CIM) with\nDiversified Aggregation Convolution (DAC) techniques. The DAC module is\ndesigned which incorporates diverse downsampling features through a simple,\nparameter-free attention module (SimAM), aiming to overcome the problems of\nfalse-positive identification and edge blurring. Furthermore, we introduce CIM\nto leverage sample weighting by directly removing the spurious correlations\nbetween features for every input sample and concentrating more on the\ncorrelation between features and labels. Extensive experiments on the\nMoNuSeg-2018 dataset achieves promising results, outperforming other\nstate-of-the-art methods, where the mIoU and DSC scores growing by 3.6% and\n2.65%.\n"
    },
    {
        "title": "Test-time Distribution Learning Adapter for Cross-modal Visual Reasoning",
        "published_time": "2024-03-10T01:34:45Z",
        "abstract": "  Vision-Language Pre-Trained (VLP) models, such as CLIP, have demonstrated\nremarkable effectiveness in learning generic visual representations. Several\napproaches aim to efficiently adapt VLP models to downstream tasks with limited\nsupervision, aiming to leverage the acquired knowledge from VLP models.\nHowever, these methods suffer from either introducing biased representations or\nrequiring high computational complexity, which hinders their effectiveness in\nfine-tuning the CLIP model. Moreover, when a model is trained on data specific\nto a particular domain, its ability to generalize to uncharted domains\ndiminishes. In this work, we propose Test-Time Distribution LearNing Adapter\n(TT-DNA) which directly works during the testing period. Specifically, we\nestimate Gaussian distributions to model visual features of the few-shot\nsupport images to capture the knowledge from the support set. The cosine\nsimilarity between query image and the feature distribution of support images\nis used as the prediction of visual adapter. Subsequently, the visual adapter's\nprediction merges with the original CLIP prediction via a residual connection,\nresulting in the final prediction. Our extensive experimental results on visual\nreasoning for human object interaction demonstrate that our proposed TT-DNA\noutperforms existing state-of-the-art methods by large margins.\n"
    },
    {
        "title": "Attacking Transformers with Feature Diversity Adversarial Perturbation",
        "published_time": "2024-03-10T00:55:58Z",
        "abstract": "  Understanding the mechanisms behind Vision Transformer (ViT), particularly\nits vulnerability to adversarial perturba tions, is crucial for addressing\nchallenges in its real-world applications. Existing ViT adversarial attackers\nrely on la bels to calculate the gradient for perturbation, and exhibit low\ntransferability to other structures and tasks. In this paper, we present a\nlabel-free white-box attack approach for ViT-based models that exhibits strong\ntransferability to various black box models, including most ViT variants, CNNs,\nand MLPs, even for models developed for other modalities. Our inspira tion\ncomes from the feature collapse phenomenon in ViTs, where the critical\nattention mechanism overly depends on the low-frequency component of features,\ncausing the features in middle-to-end layers to become increasingly similar and\neventually collapse. We propose the feature diversity attacker to naturally\naccelerate this process and achieve remarkable performance and transferability.\n"
    },
    {
        "title": "Texture image retrieval using a classification and contourlet-based\n  features",
        "published_time": "2024-03-10T00:07:47Z",
        "abstract": "  In this paper, we propose a new framework for improving Content Based Image\nRetrieval (CBIR) for texture images. This is achieved by using a new image\nrepresentation based on the RCT-Plus transform which is a novel variant of the\nRedundant Contourlet transform that extracts a richer directional information\nin the image. Moreover, the process of image search is improved through a\nlearning-based approach where the images of the database are classified using\nan adapted similarity metric to the statistical modeling of the RCT-Plus\ntransform. A query is then first classified to select the best texture class\nafter which the retained class images are ranked to select top ones. By this,\nwe have achieved significant improvements in the retrieval rates compared to\nprevious CBIR schemes.\n"
    },
    {
        "title": "MATRIX: Multi-Agent Trajectory Generation with Diverse Contexts",
        "published_time": "2024-03-09T23:28:54Z",
        "abstract": "  Data-driven methods have great advantages in modeling complicated human\nbehavioral dynamics and dealing with many human-robot interaction applications.\nHowever, collecting massive and annotated real-world human datasets has been a\nlaborious task, especially for highly interactive scenarios. On the other hand,\nalgorithmic data generation methods are usually limited by their model\ncapacities, making them unable to offer realistic and diverse data needed by\nvarious application users. In this work, we study trajectory-level data\ngeneration for multi-human or human-robot interaction scenarios and propose a\nlearning-based automatic trajectory generation model, which we call Multi-Agent\nTRajectory generation with dIverse conteXts (MATRIX). MATRIX is capable of\ngenerating interactive human behaviors in realistic diverse contexts. We\nachieve this goal by modeling the explicit and interpretable objectives so that\nMATRIX can generate human motions based on diverse destinations and\nheterogeneous behaviors. We carried out extensive comparison and ablation\nstudies to illustrate the effectiveness of our approach across various metrics.\nWe also presented experiments that demonstrate the capability of MATRIX to\nserve as data augmentation for imitation-based motion planning.\n"
    },
    {
        "title": "Semi-Supervised Multimodal Multi-Instance Learning for Aortic Stenosis\n  Diagnosis",
        "published_time": "2024-03-09T22:23:45Z",
        "abstract": "  Automated interpretation of ultrasound imaging of the heart (echocardiograms)\ncould improve the detection and treatment of aortic stenosis (AS), a deadly\nheart disease. However, existing deep learning pipelines for assessing AS from\nechocardiograms have two key limitations. First, most methods rely on limited\n2D cineloops, thereby ignoring widely available Doppler imaging that contains\nimportant complementary information about pressure gradients and blood flow\nabnormalities associated with AS. Second, obtaining labeled data is difficult.\nThere are often far more unlabeled echocardiogram recordings available, but\nthese remain underutilized by existing methods. To overcome these limitations,\nwe introduce Semi-supervised Multimodal Multiple-Instance Learning (SMMIL), a\nnew deep learning framework for automatic interpretation for structural heart\ndiseases like AS. When deployed, SMMIL can combine information from two input\nmodalities, spectral Dopplers and 2D cineloops, to produce a study-level AS\ndiagnosis. During training, SMMIL can combine a smaller labeled set and an\nabundant unlabeled set of both modalities to improve its classifier.\nExperiments demonstrate that SMMIL outperforms recent alternatives at 3-level\nAS severity classification as well as several clinically relevant AS detection\ntasks.\n"
    },
    {
        "title": "Weighted Monte Carlo augmented spherical Fourier-Bessel convolutional\n  layers for 3D abdominal organ segmentation",
        "published_time": "2024-02-26T18:51:15Z",
        "abstract": "  Filter-decomposition-based group equivariant convolutional neural networks\nshow promising stability and data efficiency for 3D image feature extraction.\nHowever, the existing filter-decomposition-based 3D group equivariant neural\nnetworks rely on parameter-sharing designs and are mostly limited to rotation\ntransformation groups, where the chosen spherical harmonic filter bases\nconsider only angular orthogonality. These limitations hamper its application\nto deep neural network architectures for medical image segmentation. To address\nthese issues, this paper describes a non-parameter-sharing affine group\nequivariant neural network for 3D medical image segmentation based on an\nadaptive aggregation of Monte Carlo augmented spherical Fourier Bessel filter\nbases. The efficiency and flexibility of the adopted non-parameter-sharing\nstrategy enable for the first time an efficient implementation of 3D affine\ngroup equivariant convolutional neural networks for volumetric data. The\nintroduced spherical Bessel Fourier filter basis combines both angular and\nradial orthogonality for better feature extraction. The 3D image segmentation\nexperiments on two abdominal medical image sets, BTCV and the NIH Pancreas\ndatasets, show that the proposed methods excel the state-of-the-art 3D neural\nnetworks with high training stability and data efficiency. The code will be\navailable at https://github.com/ZhaoWenzhao/WMCSFB.\n"
    },
    {
        "title": "Multi-conditioned Graph Diffusion for Neural Architecture Search",
        "published_time": "2024-03-09T21:45:31Z",
        "abstract": "  Neural architecture search automates the design of neural network\narchitectures usually by exploring a large and thus complex architecture search\nspace. To advance the architecture search, we present a graph diffusion-based\nNAS approach that uses discrete conditional graph diffusion processes to\ngenerate high-performing neural network architectures. We then propose a\nmulti-conditioned classifier-free guidance approach applied to graph diffusion\nnetworks to jointly impose constraints such as high accuracy and low hardware\nlatency. Unlike the related work, our method is completely differentiable and\nrequires only a single model training. In our evaluations, we show promising\nresults on six standard benchmarks, yielding novel and unique architectures at\na fast speed, i.e. less than 0.2 seconds per architecture. Furthermore, we\ndemonstrate the generalisability and efficiency of our method through\nexperiments on ImageNet dataset.\n"
    },
    {
        "title": "Hard-label based Small Query Black-box Adversarial Attack",
        "published_time": "2024-03-09T21:26:22Z",
        "abstract": "  We consider the hard label based black box adversarial attack setting which\nsolely observes predicted classes from the target model. Most of the attack\nmethods in this setting suffer from impractical number of queries required to\nachieve a successful attack. One approach to tackle this drawback is utilising\nthe adversarial transferability between white box surrogate models and black\nbox target model. However, the majority of the methods adopting this approach\nare soft label based to take the full advantage of zeroth order optimisation.\nUnlike mainstream methods, we propose a new practical setting of hard label\nbased attack with an optimisation process guided by a pretrained surrogate\nmodel. Experiments show the proposed method significantly improves the query\nefficiency of the hard label based black-box attack across various target model\narchitectures. We find the proposed method achieves approximately 5 times\nhigher attack success rate compared to the benchmarks, especially at the small\nquery budgets as 100 and 250.\n"
    },
    {
        "title": "Are Classification Robustness and Explanation Robustness Really Strongly\n  Correlated? An Analysis Through Input Loss Landscape",
        "published_time": "2024-03-09T21:26:10Z",
        "abstract": "  This paper delves into the critical area of deep learning robustness,\nchallenging the conventional belief that classification robustness and\nexplanation robustness in image classification systems are inherently\ncorrelated. Through a novel evaluation approach leveraging clustering for\nefficient assessment of explanation robustness, we demonstrate that enhancing\nexplanation robustness does not necessarily flatten the input loss landscape\nwith respect to explanation loss - contrary to flattened loss landscapes\nindicating better classification robustness. To deeply investigate this\ncontradiction, a groundbreaking training method designed to adjust the loss\nlandscape with respect to explanation loss is proposed. Through the new\ntraining method, we uncover that although such adjustments can impact the\nrobustness of explanations, they do not have an influence on the robustness of\nclassification. These findings not only challenge the prevailing assumption of\na strong correlation between the two forms of robustness but also pave new\npathways for understanding relationship between loss landscape and explanation\nloss.\n"
    },
    {
        "title": "VLP: Vision Language Planning for Autonomous Driving",
        "published_time": "2024-01-10T23:00:40Z",
        "abstract": "  Autonomous driving is a complex and challenging task that aims at safe motion\nplanning through scene understanding and reasoning. While vision-only\nautonomous driving methods have recently achieved notable performance, through\nenhanced scene understanding, several key issues, including lack of reasoning,\nlow generalization performance and long-tail scenarios, still need to be\naddressed. In this paper, we present VLP, a novel Vision-Language-Planning\nframework that exploits language models to bridge the gap between linguistic\nunderstanding and autonomous driving. VLP enhances autonomous driving systems\nby strengthening both the source memory foundation and the self-driving car's\ncontextual understanding. VLP achieves state-of-the-art end-to-end planning\nperformance on the challenging NuScenes dataset by achieving 35.9\\% and 60.5\\%\nreduction in terms of average L2 error and collision rates, respectively,\ncompared to the previous best method. Moreover, VLP shows improved performance\nin challenging long-tail scenarios and strong generalization capabilities when\nfaced with new urban environments.\n"
    },
    {
        "title": "TUMTraf Event: Calibration and Fusion Resulting in a Dataset for\n  Roadside Event-Based and RGB Cameras",
        "published_time": "2024-01-16T16:25:37Z",
        "abstract": "  Event-based cameras are predestined for Intelligent Transportation Systems\n(ITS). They provide very high temporal resolution and dynamic range, which can\neliminate motion blur and improve detection performance at night. However,\nevent-based images lack color and texture compared to images from a\nconventional RGB camera. Considering that, data fusion between event-based and\nconventional cameras can combine the strengths of both modalities. For this\npurpose, extrinsic calibration is necessary. To the best of our knowledge, no\ntargetless calibration between event-based and RGB cameras can handle multiple\nmoving objects, nor does data fusion optimized for the domain of roadside ITS\nexist. Furthermore, synchronized event-based and RGB camera datasets\nconsidering roadside perspective are not yet published. To fill these research\ngaps, based on our previous work, we extended our targetless calibration\napproach with clustering methods to handle multiple moving objects.\nFurthermore, we developed an early fusion, simple late fusion, and a novel\nspatiotemporal late fusion method. Lastly, we published the TUMTraf Event\nDataset, which contains more than 4,111 synchronized event-based and RGB images\nwith 50,496 labeled 2D boxes. During our extensive experiments, we verified the\neffectiveness of our calibration method with multiple moving objects.\nFurthermore, compared to a single RGB camera, we increased the detection\nperformance of up to +9 % mAP in the day and up to +13 % mAP during the\nchallenging night with our presented event-based sensor fusion methods. The\nTUMTraf Event Dataset is available at\nhttps://innovation-mobility.com/tumtraf-dataset.\n"
    },
    {
        "title": "D4C glove-train: solving the RPM and Bongard-logo problem by\n  distributing and Circumscribing concepts",
        "published_time": "2024-03-06T04:36:43Z",
        "abstract": "  This paper achieves significant progress in the field of abstract reasoning,\nparticularly in addressing Raven's Progressive Matrices (RPM) and Bongard-Logo\nproblems. We propose the D2C approach, which redefines conceptual boundaries in\nthese domains and bridges the gap between high-level concepts and their\nlow-dimensional representations. Based on this, we further introduce the D3C\nmethod that handles Bongard-Logo problems and significantly improves reasoning\naccuracy by estimating the distribution of image representations and measuring\ntheir Sinkhorn distance. To enhance computational efficiency, we introduce the\nD3C-cos variant, which provides an efficient and accurate solution for RPM\nproblems by constraining distribution distances. Additionally, we present\nLico-Net, a network that combines D3C and D3C-cos to achieve state-of-the-art\nperformance in both problem-solving and interpretability. Finally, we extend\nour approach to D4C, employing adversarial strategies to further refine\nconceptual boundaries and demonstrate notable improvements for both RPM and\nBongard-Logo problems. Overall, our contributions offer a new perspective and\npractical solutions to the field of abstract reasoning.\n"
    },
    {
        "title": "Solving the bongard-logo problem by modeling a probabilistic model",
        "published_time": "2024-03-05T18:08:29Z",
        "abstract": "  Abstract reasoning problems challenge the perceptual and cognitive abilities\nof AI algorithms, demanding deeper pattern discernment and inductive reasoning\nbeyond explicit image features. This study introduces PMoC, a tailored\nprobability model for the Bongard-Logo problem, achieving high reasoning\naccuracy by constructing independent probability models. Additionally, we\npresent Pose-Transformer, an enhanced Transformer-Encoder designed for complex\nabstract reasoning tasks, including Bongard-Logo, RAVEN, I-RAVEN, and PGM.\nPose-Transformer incorporates positional information learning, inspired by\ncapsule networks' pose matrices, enhancing its focus on local positional\nrelationships in image data processing. When integrated with PMoC, it further\nimproves reasoning accuracy. Our approach effectively addresses reasoning\ndifficulties associated with abstract entities' positional changes,\noutperforming previous models on the OIG, D3$\\times$3 subsets of RAVEN, and PGM\ndatabases. This research contributes to advancing AI's capabilities in abstract\nreasoning and cognitive pattern recognition.\n"
    },
    {
        "title": "Triple-CFN: Restructuring Conceptual Spaces for Enhancing Abstract\n  Reasoning process",
        "published_time": "2024-03-05T18:29:17Z",
        "abstract": "  Abstract reasoning problems pose significant challenges to artificial\nintelligence algorithms, demanding cognitive capabilities beyond those required\nfor perception tasks. This study introduces the Triple-CFN approach to tackle\nthe Bongard-Logo problem, achieving notable reasoning accuracy by implicitly\nreorganizing the concept space of conflicting instances. Additionally, the\nTriple-CFN paradigm proves effective for the RPM problem with necessary\nmodifications, yielding competitive results. To further enhance performance on\nthe RPM issue, we develop the Meta Triple-CFN network, which explicitly\nstructures the problem space while maintaining interpretability on progressive\npatterns. The success of Meta Triple-CFN is attributed to its paradigm of\nmodeling the conceptual space, equivalent to normalizing reasoning information.\nBased on this ideology, we introduce the Re-space layer, enhancing the\nperformance of both Meta Triple-CFN and Triple-CFN. This paper aims to\ncontribute to advancements in machine intelligence by exploring innovative\nnetwork designs for addressing abstract reasoning problems, paving the way for\nfurther breakthroughs in this domain.\n"
    },
    {
        "title": "Can Generative Models Improve Self-Supervised Representation Learning?",
        "published_time": "2024-03-09T17:17:07Z",
        "abstract": "  The rapid advancement in self-supervised learning (SSL) has highlighted its\npotential to leverage unlabeled data for learning powerful visual\nrepresentations. However, existing SSL approaches, particularly those employing\ndifferent views of the same image, often rely on a limited set of predefined\ndata augmentations. This constrains the diversity and quality of\ntransformations, which leads to sub-optimal representations. In this paper, we\nintroduce a novel framework that enriches the SSL paradigm by utilizing\ngenerative models to produce semantically consistent image augmentations. By\ndirectly conditioning generative models on a source image representation, our\nmethod enables the generation of diverse augmentations while maintaining the\nsemantics of the source image, thus offering a richer set of data for\nself-supervised learning. Our experimental results demonstrate that our\nframework significantly enhances the quality of learned visual representations.\nThis research demonstrates that incorporating generative models into the SSL\nworkflow opens new avenues for exploring the potential of unlabeled visual\ndata. This development paves the way for more robust and versatile\nrepresentation learning techniques.\n"
    },
    {
        "title": "Robust Emotion Recognition in Context Debiasing",
        "published_time": "2024-03-09T17:05:43Z",
        "abstract": "  Context-aware emotion recognition (CAER) has recently boosted the practical\napplications of affective computing techniques in unconstrained environments.\nMainstream CAER methods invariably extract ensemble representations from\ndiverse contexts and subject-centred characteristics to perceive the target\nperson's emotional state. Despite advancements, the biggest challenge remains\ndue to context bias interference. The harmful bias forces the models to rely on\nspurious correlations between background contexts and emotion labels in\nlikelihood estimation, causing severe performance bottlenecks and confounding\nvaluable context priors. In this paper, we propose a counterfactual emotion\ninference (CLEF) framework to address the above issue. Specifically, we first\nformulate a generalized causal graph to decouple the causal relationships among\nthe variables in CAER. Following the causal graph, CLEF introduces a\nnon-invasive context branch to capture the adverse direct effect caused by the\ncontext bias. During the inference, we eliminate the direct context effect from\nthe total causal effect by comparing factual and counterfactual outcomes,\nresulting in bias mitigation and robust prediction. As a model-agnostic\nframework, CLEF can be readily integrated into existing methods, bringing\nconsistent performance gains.\n"
    },
    {
        "title": "IOI: Invisible One-Iteration Adversarial Attack on No-Reference Image-\n  and Video-Quality Metrics",
        "published_time": "2024-03-09T16:33:30Z",
        "abstract": "  No-reference image- and video-quality metrics are widely used in video\nprocessing benchmarks. The robustness of learning-based metrics under video\nattacks has not been widely studied. In addition to having success, attacks\nthat can be employed in video processing benchmarks must be fast and\nimperceptible. This paper introduces an Invisible One-Iteration (IOI)\nadversarial attack on no reference image and video quality metrics. We compared\nour method alongside eight prior approaches using image and video datasets via\nobjective and subjective tests. Our method exhibited superior visual quality\nacross various attacked metric architectures while maintaining comparable\nattack success and speed. We made the code available on GitHub.\n"
    },
    {
        "title": "Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A\n  GRU LSTM Hybrid Approach",
        "published_time": "2024-03-09T16:05:31Z",
        "abstract": "  Accurate classification of objects in 3D point clouds is a significant\nproblem in several applications, such as autonomous navigation and\naugmented/virtual reality scenarios, which has become a research hot spot. In\nthis paper, we presented a deep learning strategy for 3D object classification\nin augmented reality. The proposed approach is a combination of the GRU and\nLSTM. LSTM networks learn longer dependencies well, but due to the number of\ngates, it takes longer to train; on the other hand, GRU networks have a weaker\nperformance than LSTM, but their training speed is much higher than GRU, which\nis The speed is due to its fewer gates. The proposed approach used the\ncombination of speed and accuracy of these two networks. The proposed approach\nachieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes\neight classes (unlabeled, man-made terrain, natural terrain, high vegetation,\nlow vegetation, buildings, hardscape, scanning artifacts, cars). Meanwhile, the\ntraditional machine learning approaches could achieve a maximum accuracy of\n0.9489 in the best case. Keywords: Point Cloud Classification, Virtual Reality,\nHybrid Model, GRULSTM, GRU, LSTM\n"
    },
    {
        "title": "Instilling Multi-round Thinking to Text-guided Image Generation",
        "published_time": "2024-01-16T16:19:58Z",
        "abstract": "  This paper delves into the text-guided image editing task, focusing on\nmodifying a reference image according to user-specified textual feedback to\nembody specific attributes. Despite recent advancements, a persistent challenge\nremains that the single-round generation often overlooks crucial details,\nparticularly in the realm of fine-grained changes like shoes or sleeves. This\nissue compounds over multiple rounds of interaction, severely limiting\ncustomization quality. In an attempt to address this challenge, we introduce a\nnew self-supervised regularization, \\ie, multi-round regularization, which is\ncompatible with existing methods. Specifically, the multi-round regularization\nencourages the model to maintain consistency across different modification\norders. It builds upon the observation that the modification order generally\nshould not affect the final result. Different from traditional one-round\ngeneration, the mechanism underpinning the proposed method is the error\namplification of initially minor inaccuracies in capturing intricate details.\nQualitative and quantitative experiments affirm that the proposed method\nachieves high-fidelity editing quality, especially the local modification, in\nboth single-round and multiple-round generation, while also showcasing robust\ngeneralization to irregular text inputs. The effectiveness of our semantic\nalignment with textual feedback is further substantiated by the retrieval\nimprovements on FahisonIQ and Fashion200k.\n"
    },
    {
        "title": "Endo-4DGS: Endoscopic Monocular Scene Reconstruction with 4D Gaussian\n  Splatting",
        "published_time": "2024-01-29T18:55:29Z",
        "abstract": "  In the realm of robot-assisted minimally invasive surgery, dynamic scene\nreconstruction can significantly enhance downstream tasks and improve surgical\noutcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to\nprominence for their exceptional ability to reconstruct scenes but are hampered\nby slow inference speed, prolonged training, and inconsistent depth estimation.\nSome previous work utilizes ground truth depth for optimization but is hard to\nacquire in the surgical domain. To overcome these obstacles, we present\nEndo-4DGS, a real-time endoscopic dynamic reconstruction approach that utilizes\n3D Gaussian Splatting (GS) for 3D representation. Specifically, we propose\nlightweight MLPs to capture temporal dynamics with Gaussian deformation fields.\nTo obtain a satisfactory Gaussian Initialization, we exploit a powerful depth\nestimation foundation model, Depth-Anything, to generate pseudo-depth maps as a\ngeometry prior. We additionally propose confidence-guided learning to tackle\nthe ill-pose problems in monocular depth estimation and enhance the\ndepth-guided reconstruction with surface normal constraints and depth\nregularization. Our approach has been validated on two surgical datasets, where\nit can effectively render in real-time, compute efficiently, and reconstruct\nwith remarkable accuracy.\n"
    },
    {
        "title": "Wavelet-Like Transform-Based Technology in Response to the Call for\n  Proposals on Neural Network-Based Image Coding",
        "published_time": "2024-03-09T15:13:49Z",
        "abstract": "  Neural network-based image coding has been developing rapidly since its\nbirth. Until 2022, its performance has surpassed that of the best-performing\ntraditional image coding framework -- H.266/VVC. Witnessing such success, the\nIEEE 1857.11 working subgroup initializes a neural network-based image coding\nstandard project and issues a corresponding call for proposals (CfP). In\nresponse to the CfP, this paper introduces a novel wavelet-like transform-based\nend-to-end image coding framework -- iWaveV3. iWaveV3 incorporates many new\nfeatures such as affine wavelet-like transform, perceptual-friendly quality\nmetric, and more advanced training and online optimization strategies into our\nprevious wavelet-like transform-based framework iWave++. While preserving the\nfeatures of supporting lossy and lossless compression simultaneously, iWaveV3\nalso achieves state-of-the-art compression efficiency for objective quality and\nis very competitive for perceptual quality. As a result, iWaveV3 is adopted as\na candidate scheme for developing the IEEE Standard for neural-network-based\nimage coding.\n"
    },
    {
        "title": "Learned 3D volumetric recovery of clouds and its uncertainty for climate\n  analysis",
        "published_time": "2024-03-09T14:57:03Z",
        "abstract": "  Significant uncertainty in climate prediction and cloud physics is tied to\nobservational gaps relating to shallow scattered clouds. Addressing these\nchallenges requires remote sensing of their three-dimensional (3D)\nheterogeneous volumetric scattering content. This calls for passive scattering\ncomputed tomography (CT). We design a learning-based model (ProbCT) to achieve\nCT of such clouds, based on noisy multi-view spaceborne images. ProbCT infers -\nfor the first time - the posterior probability distribution of the\nheterogeneous extinction coefficient, per 3D location. This yields arbitrary\nvaluable statistics, e.g., the 3D field of the most probable extinction and its\nuncertainty. ProbCT uses a neural-field representation, making essentially\nreal-time inference. ProbCT undergoes supervised training by a new labeled\nmulti-class database of physics-based volumetric fields of clouds and their\ncorresponding images. To improve out-of-distribution inference, we incorporate\nself-supervised learning through differential rendering. We demonstrate the\napproach in simulations and on real-world data, and indicate the relevance of\n3D recovery and uncertainty to precipitation and renewable energy.\n"
    },
    {
        "title": "GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual\n  Affective Computing",
        "published_time": "2024-03-09T13:56:25Z",
        "abstract": "  Multimodal language models (MLMs) are designed to process and integrate\ninformation from multiple sources, such as text, speech, images, and videos.\nDespite its success in language understanding, it is critical to evaluate the\nperformance of downstream tasks for better human-centric applications. This\npaper assesses the application of MLMs with 5 crucial abilities for affective\ncomputing, spanning from visual affective tasks and reasoning tasks. The\nresults show that GPT4 has high accuracy in facial action unit recognition and\nmicro-expression detection while its general facial expression recognition\nperformance is not accurate. We also highlight the challenges of achieving\nfine-grained micro-expression recognition and the potential for further study\nand demonstrate the versatility and potential of GPT4 for handling advanced\ntasks in emotion recognition and related fields by integrating with\ntask-related agents for more complex tasks, such as heart rate estimation\nthrough signal processing. In conclusion, this paper provides valuable insights\ninto the potential applications and challenges of MLMs in human-centric\ncomputing. The interesting samples are available at\n\\url{https://github.com/LuPaoPao/GPT4Affectivity}.\n"
    },
    {
        "title": "Short-Form Videos and Mental Health: A Knowledge-Guided Multimodal\n  Neural Topic Model",
        "published_time": "2024-01-11T03:36:47Z",
        "abstract": "  While short-form videos head to reshape the entire social media landscape,\nexperts are exceedingly worried about their depressive impacts on viewers, as\nevidenced by medical studies. To prevent widespread consequences, platforms are\neager to predict these videos' impact on viewers' mental health. Subsequently,\nthey can take intervention measures, such as revising recommendation algorithms\nand displaying viewer discretion. Nevertheless, applicable predictive methods\nlack relevance to well-established medical knowledge, which outlines clinically\nproven external and environmental factors of depression. To account for such\nmedical knowledge, we resort to an emergent methodological discipline, seeded\nNeural Topic Models (NTMs). However, existing seeded NTMs suffer from the\nlimitations of single-origin topics, unknown topic sources, unclear seed\nsupervision, and suboptimal convergence. To address those challenges, we\ndevelop a novel Knowledge-guided Multimodal NTM to predict a short-form video's\ndepressive impact on viewers. Extensive empirical analyses using TikTok and\nDouyin datasets prove that our method outperforms state-of-the-art benchmarks.\nOur method also discovers medically relevant topics from videos that are linked\nto depressive impact. We contribute to IS with a novel video analytics method\nthat is generalizable to other video classification problems. Practically, our\nmethod can help platforms understand videos' mental impacts, thus adjusting\nrecommendations and video topic disclosure.\n"
    },
    {
        "title": "Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic\n  Segmentation",
        "published_time": "2024-03-09T13:37:02Z",
        "abstract": "  Tumor lesion segmentation on CT or MRI images plays a critical role in cancer\ndiagnosis and treatment planning. Considering the inherent differences in tumor\nlesion segmentation data across various medical imaging modalities and\nequipment, integrating medical knowledge into the Segment Anything Model (SAM)\npresents promising capability due to its versatility and generalization\npotential. Recent studies have attempted to enhance SAM with medical expertise\nby pre-training on large-scale medical segmentation datasets. However,\nchallenges still exist in 3D tumor lesion segmentation owing to tumor\ncomplexity and the imbalance in foreground and background regions. Therefore,\nwe introduce Mask-Enhanced SAM (M-SAM), an innovative architecture tailored for\n3D tumor lesion segmentation. We propose a novel Mask-Enhanced Adapter (MEA)\nwithin M-SAM that enriches the semantic information of medical images with\npositional data from coarse segmentation masks, facilitating the generation of\nmore precise segmentation masks. Furthermore, an iterative refinement scheme is\nimplemented in M-SAM to refine the segmentation masks progressively, leading to\nimproved performance. Extensive experiments on seven tumor lesion segmentation\ndatasets indicate that our M-SAM not only achieves high segmentation accuracy\nbut also exhibits robust generalization.\n"
    },
    {
        "title": "Lightning NeRF: Efficient Hybrid Scene Representation for Autonomous\n  Driving",
        "published_time": "2024-03-09T13:14:27Z",
        "abstract": "  Recent studies have highlighted the promising application of NeRF in\nautonomous driving contexts. However, the complexity of outdoor environments,\ncombined with the restricted viewpoints in driving scenarios, complicates the\ntask of precisely reconstructing scene geometry. Such challenges often lead to\ndiminished quality in reconstructions and extended durations for both training\nand rendering. To tackle these challenges, we present Lightning NeRF. It uses\nan efficient hybrid scene representation that effectively utilizes the geometry\nprior from LiDAR in autonomous driving scenarios. Lightning NeRF significantly\nimproves the novel view synthesis performance of NeRF and reduces computational\noverheads. Through evaluations on real-world datasets, such as KITTI-360,\nArgoverse2, and our private dataset, we demonstrate that our approach not only\nexceeds the current state-of-the-art in novel view synthesis quality but also\nachieves a five-fold increase in training speed and a ten-fold improvement in\nrendering speed. Codes are available at\nhttps://github.com/VISION-SJTU/Lightning-NeRF .\n"
    },
    {
        "title": "Segmentation Guided Sparse Transformer for Under-Display Camera Image\n  Restoration",
        "published_time": "2024-03-09T13:11:59Z",
        "abstract": "  Under-Display Camera (UDC) is an emerging technology that achieves\nfull-screen display via hiding the camera under the display panel. However, the\ncurrent implementation of UDC causes serious degradation. The incident light\nrequired for camera imaging undergoes attenuation and diffraction when passing\nthrough the display panel, leading to various artifacts in UDC imaging.\nPresently, the prevailing UDC image restoration methods predominantly utilize\nconvolutional neural network architectures, whereas Transformer-based methods\nhave exhibited superior performance in the majority of image restoration tasks.\nThis is attributed to the Transformer's capability to sample global features\nfor the local reconstruction of images, thereby achieving high-quality image\nrestoration. In this paper, we observe that when using the Vision Transformer\nfor UDC degraded image restoration, the global attention samples a large amount\nof redundant information and noise. Furthermore, compared to the ordinary\nTransformer employing dense attention, the Transformer utilizing sparse\nattention can alleviate the adverse impact of redundant information and noise.\nBuilding upon this discovery, we propose a Segmentation Guided Sparse\nTransformer method (SGSFormer) for the task of restoring high-quality images\nfrom UDC degraded images. Specifically, we utilize sparse self-attention to\nfilter out redundant information and noise, directing the model's attention to\nfocus on the features more relevant to the degraded regions in need of\nreconstruction. Moreover, we integrate the instance segmentation map as prior\ninformation to guide the sparse self-attention in filtering and focusing on the\ncorrect regions.\n"
    },
    {
        "title": "RealNet: A Feature Selection Network with Realistic Synthetic Anomaly\n  for Anomaly Detection",
        "published_time": "2024-03-09T12:25:01Z",
        "abstract": "  Self-supervised feature reconstruction methods have shown promising advances\nin industrial image anomaly detection and localization. Despite this progress,\nthese methods still face challenges in synthesizing realistic and diverse\nanomaly samples, as well as addressing the feature redundancy and pre-training\nbias of pre-trained feature. In this work, we introduce RealNet, a feature\nreconstruction network with realistic synthetic anomaly and adaptive feature\nselection. It is incorporated with three key innovations: First, we propose\nStrength-controllable Diffusion Anomaly Synthesis (SDAS), a diffusion\nprocess-based synthesis strategy capable of generating samples with varying\nanomaly strengths that mimic the distribution of real anomalous samples.\nSecond, we develop Anomaly-aware Features Selection (AFS), a method for\nselecting representative and discriminative pre-trained feature subsets to\nimprove anomaly detection performance while controlling computational costs.\nThird, we introduce Reconstruction Residuals Selection (RRS), a strategy that\nadaptively selects discriminative residuals for comprehensive identification of\nanomalous regions across multiple levels of granularity. We assess RealNet on\nfour benchmark datasets, and our results demonstrate significant improvements\nin both Image AUROC and Pixel AUROC compared to the current state-o-the-art\nmethods. The code, data, and models are available at\nhttps://github.com/cnulab/RealNet.\n"
    },
    {
        "title": "Fast Kernel Scene Flow",
        "published_time": "2024-03-09T12:24:49Z",
        "abstract": "  In contrast to current state-of-the-art methods, such as NSFP [25], which\nemploy deep implicit neural functions for modeling scene flow, we present a\nnovel approach that utilizes classical kernel representations. This\nrepresentation enables our approach to effectively handle dense lidar points\nwhile demonstrating exceptional computational efficiency -- compared to recent\ndeep approaches -- achieved through the solution of a linear system. As a\nruntime optimization-based method, our model exhibits impressive\ngeneralizability across various out-of-distribution scenarios, achieving\ncompetitive performance on large-scale lidar datasets. We propose a new\npositional encoding-based kernel that demonstrates state-of-the-art performance\nin efficient lidar scene flow estimation on large-scale point clouds. An\nimportant highlight of our method is its near real-time performance (~150-170\nms) with dense lidar data (~8k-144k points), enabling a variety of practical\napplications in robotics and autonomous driving scenarios.\n"
    },
    {
        "title": "DO3D: Self-supervised Learning of Decomposed Object-aware 3D Motion and\n  Depth from Monocular Videos",
        "published_time": "2024-03-09T12:22:46Z",
        "abstract": "  Although considerable advancements have been attained in self-supervised\ndepth estimation from monocular videos, most existing methods often treat all\nobjects in a video as static entities, which however violates the dynamic\nnature of real-world scenes and fails to model the geometry and motion of\nmoving objects. In this paper, we propose a self-supervised method to jointly\nlearn 3D motion and depth from monocular videos. Our system contains a depth\nestimation module to predict depth, and a new decomposed object-wise 3D motion\n(DO3D) estimation module to predict ego-motion and 3D object motion. Depth and\nmotion networks work collaboratively to faithfully model the geometry and\ndynamics of real-world scenes, which, in turn, benefits both depth and 3D\nmotion estimation. Their predictions are further combined to synthesize a novel\nvideo frame for self-supervised training. As a core component of our framework,\nDO3D is a new motion disentanglement module that learns to predict camera\nego-motion and instance-aware 3D object motion separately. To alleviate the\ndifficulties in estimating non-rigid 3D object motions, they are decomposed to\nobject-wise 6-DoF global transformations and a pixel-wise local 3D motion\ndeformation field. Qualitative and quantitative experiments are conducted on\nthree benchmark datasets, including KITTI, Cityscapes, and VKITTI2, where our\nmodel delivers superior performance in all evaluated settings. For the depth\nestimation task, our model outperforms all compared research works in the\nhigh-resolution setting, attaining an absolute relative depth error (abs rel)\nof 0.099 on the KITTI benchmark. Besides, our optical flow estimation results\n(an overall EPE of 7.09 on KITTI) also surpass state-of-the-art methods and\nlargely improve the estimation of dynamic regions, demonstrating the\neffectiveness of our motion model. Our code will be available.\n"
    },
    {
        "title": "Frequency Attention for Knowledge Distillation",
        "published_time": "2024-03-09T12:18:48Z",
        "abstract": "  Knowledge distillation is an attractive approach for learning compact deep\nneural networks, which learns a lightweight student model by distilling\nknowledge from a complex teacher model. Attention-based knowledge distillation\nis a specific form of intermediate feature-based knowledge distillation that\nuses attention mechanisms to encourage the student to better mimic the teacher.\nHowever, most of the previous attention-based distillation approaches perform\nattention in the spatial domain, which primarily affects local regions in the\ninput image. This may not be sufficient when we need to capture the broader\ncontext or global information necessary for effective knowledge transfer. In\nfrequency domain, since each frequency is determined from all pixels of the\nimage in spatial domain, it can contain global information about the image.\nInspired by the benefits of the frequency domain, we propose a novel module\nthat functions as an attention mechanism in the frequency domain. The module\nconsists of a learnable global filter that can adjust the frequencies of\nstudent's features under the guidance of the teacher's features, which\nencourages the student's features to have patterns similar to the teacher's\nfeatures. We then propose an enhanced knowledge review-based distillation model\nby leveraging the proposed frequency attention module. The extensive\nexperiments with various teacher and student architectures on image\nclassification and object detection benchmark datasets show that the proposed\napproach outperforms other knowledge distillation methods.\n"
    },
    {
        "title": "Generalizing to Out-of-Sample Degradations via Model Reprogramming",
        "published_time": "2024-03-09T11:56:26Z",
        "abstract": "  Existing image restoration models are typically designed for specific tasks\nand struggle to generalize to out-of-sample degradations not encountered during\ntraining. While zero-shot methods can address this limitation by fine-tuning\nmodel parameters on testing samples, their effectiveness relies on predefined\nnatural priors and physical models of specific degradations. Nevertheless,\ndetermining out-of-sample degradations faced in real-world scenarios is always\nimpractical. As a result, it is more desirable to train restoration models with\ninherent generalization ability. To this end, this work introduces the\nOut-of-Sample Restoration (OSR) task, which aims to develop restoration models\ncapable of handling out-of-sample degradations. An intuitive solution involves\npre-translating out-of-sample degradations to known degradations of restoration\nmodels. However, directly translating them in the image space could lead to\ncomplex image translation issues. To address this issue, we propose a model\nreprogramming framework, which translates out-of-sample degradations by quantum\nmechanic and wave functions. Specifically, input images are decoupled as wave\nfunctions of amplitude and phase terms. The translation of out-of-sample\ndegradation is performed by adapting the phase term. Meanwhile, the image\ncontent is maintained and enhanced in the amplitude term. By taking these two\nterms as inputs, restoration models are able to handle out-of-sample\ndegradations without fine-tuning. Through extensive experiments across multiple\nevaluation cases, we demonstrate the effectiveness and flexibility of our\nproposed framework. Our codes are available at\n\\href{https://github.com/ddghjikle/Out-of-sample-restoration}{Github}.\n"
    },
    {
        "title": "Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation",
        "published_time": "2024-03-07T10:14:23Z",
        "abstract": "  The pursuit of accurate 3D hand pose estimation stands as a keystone for\nunderstanding human activity in the realm of egocentric vision. The majority of\nexisting estimation methods still rely on single-view images as input, leading\nto potential limitations, e.g., limited field-of-view and ambiguity in depth.\nTo address these problems, adding another camera to better capture the shape of\nhands is a practical direction. However, existing multi-view hand pose\nestimation methods suffer from two main drawbacks: 1) Requiring multi-view\nannotations for training, which are expensive. 2) During testing, the model\nbecomes inapplicable if camera parameters/layout are not the same as those used\nin training. In this paper, we propose a novel Single-to-Dual-view adaptation\n(S2DHand) solution that adapts a pre-trained single-view estimator to dual\nviews. Compared with existing multi-view training methods, 1) our adaptation\nprocess is unsupervised, eliminating the need for multi-view annotation. 2)\nMoreover, our method can handle arbitrary dual-view pairs with unknown camera\nparameters, making the model applicable to diverse camera settings.\nSpecifically, S2DHand is built on certain stereo constraints, including\npair-wise cross-view consensus and invariance of transformation between both\nviews. These two stereo constraints are used in a complementary manner to\ngenerate pseudo-labels, allowing reliable adaptation. Evaluation results reveal\nthat S2DHand achieves significant improvements on arbitrary camera pairs under\nboth in-dataset and cross-dataset settings, and outperforms existing adaptation\nmethods with leading performance. Project page:\nhttps://github.com/MickeyLLG/S2DHand.\n"
    },
    {
        "title": "SPAFormer: Sequential 3D Part Assembly with Transformers",
        "published_time": "2024-03-09T10:53:11Z",
        "abstract": "  We introduce SPAFormer, an innovative model designed to overcome the\ncombinatorial explosion challenge in the 3D Part Assembly (3D-PA) task. This\ntask requires accurate prediction of each part's pose and shape in sequential\nsteps, and as the number of parts increases, the possible assembly combinations\nincrease exponentially, leading to a combinatorial explosion that severely\nhinders the efficacy of 3D-PA. SPAFormer addresses this problem by leveraging\nweak constraints from assembly sequences, effectively reducing the solution\nspace's complexity. Since assembly part sequences convey construction rules\nsimilar to sentences being structured through words, our model explores both\nparallel and autoregressive generation. It further enhances assembly through\nknowledge enhancement strategies that utilize the attributes of parts and their\nsequence information, enabling it to capture the inherent assembly pattern and\nrelationships among sequentially ordered parts. We also construct a more\nchallenging benchmark named PartNet-Assembly covering 21 varied categories to\nmore comprehensively validate the effectiveness of SPAFormer. Extensive\nexperiments demonstrate the superior generalization capabilities of SPAFormer,\nparticularly with multi-tasking and in scenarios requiring long-horizon\nassembly. Codes and model weights will be released at\n\\url{https://github.com/xuboshen/SPAFormer}.\n"
    },
    {
        "title": "ControlCap: Controllable Region-level Captioning",
        "published_time": "2024-01-31T15:15:41Z",
        "abstract": "  Region-level captioning is challenged by the caption degeneration issue,\nwhich refers to that pre-trained multimodal models tend to predict the most\nfrequent captions but miss the less frequent ones. In this study, we propose a\ncontrollable region-level captioning (ControlCap) approach, which introduces\ncontrol words to a multimodal model to address the caption degeneration issue.\nIn specific, ControlCap leverages a discriminative module to generate control\nwords within the caption space to partition it to multiple sub-spaces. The\nmultimodal model is constrained to generate captions within a few sub-spaces\ncontaining the control words, which increases the opportunity of hitting less\nfrequent captions, alleviating the caption degeneration issue. Furthermore,\ninteractive control words can be given by either a human or an expert model,\nwhich enables captioning beyond the training caption space, enhancing the\nmodel's generalization ability. Extensive experiments on Visual Genome and\nRefCOCOg datasets show that ControlCap respectively improves the CIDEr score by\n21.6 and 2.2, outperforming the state-of-the-arts by significant margins. Code\nis available at https://github.com/callsys/ControlCap.\n"
    },
    {
        "title": "POV: Prompt-Oriented View-Agnostic Learning for Egocentric Hand-Object\n  Interaction in the Multi-View World",
        "published_time": "2024-03-09T09:54:44Z",
        "abstract": "  We humans are good at translating third-person observations of hand-object\ninteractions (HOI) into an egocentric view. However, current methods struggle\nto replicate this ability of view adaptation from third-person to first-person.\nAlthough some approaches attempt to learn view-agnostic representation from\nlarge-scale video datasets, they ignore the relationships among multiple\nthird-person views. To this end, we propose a Prompt-Oriented View-agnostic\nlearning (POV) framework in this paper, which enables this view adaptation with\nfew egocentric videos. Specifically, We introduce interactive masking prompts\nat the frame level to capture fine-grained action information, and view-aware\nprompts at the token level to learn view-agnostic representation. To verify our\nmethod, we establish two benchmarks for transferring from multiple third-person\nviews to the egocentric view. Our extensive experiments on these benchmarks\ndemonstrate the efficiency and effectiveness of our POV framework and prompt\ntuning techniques in terms of view adaptation and view generalization. Our code\nis available at \\url{https://github.com/xuboshen/pov_acmmm2023}.\n"
    },
    {
        "title": "Multisource Semisupervised Adversarial Domain Generalization Network for\n  Cross-Scene Sea-Land Clutter Classification",
        "published_time": "2024-02-09T10:50:28Z",
        "abstract": "  Deep learning (DL)-based sea\\textendash land clutter classification for\nsky-wave over-the-horizon-radar (OTHR) has become a novel research topic. In\nengineering applications, real-time predictions of sea\\textendash land clutter\nwith existing distribution discrepancies are crucial. To solve this problem,\nthis article proposes a novel Multisource Semisupervised Adversarial Domain\nGeneralization Network (MSADGN) for cross-scene sea\\textendash land clutter\nclassification. MSADGN can extract domain-invariant and domain-specific\nfeatures from one labeled source domain and multiple unlabeled source domains,\nand then generalize these features to an arbitrary unseen target domain for\nreal-time prediction of sea\\textendash land clutter. Specifically, MSADGN\nconsists of three modules: domain-related pseudolabeling module,\ndomain-invariant module, and domain-specific module. The first module\nintroduces an improved pseudolabel method called domain-related pseudolabel,\nwhich is designed to generate reliable pseudolabels to fully exploit unlabeled\nsource domains. The second module utilizes a generative adversarial network\n(GAN) with a multidiscriminator to extract domain-invariant features, to\nenhance the model's transferability in the target domain. The third module\nemploys a parallel multiclassifier branch to extract domain-specific features,\nto enhance the model's discriminability in the target domain. The effectiveness\nof our method is validated in twelve domain generalizations (DG) scenarios.\nMeanwhile, we selected 10 state-of-the-art DG methods for comparison. The\nexperimental results demonstrate the superiority of our method.\n"
    },
    {
        "title": "SSF-Net: Spatial-Spectral Fusion Network with Spectral Angle Awareness\n  for Hyperspectral Object Tracking",
        "published_time": "2024-03-09T09:37:13Z",
        "abstract": "  Hyperspectral video (HSV) offers valuable spatial, spectral, and temporal\ninformation simultaneously, making it highly suitable for handling challenges\nsuch as background clutter and visual similarity in object tracking. However,\nexisting methods primarily focus on band regrouping and rely on RGB trackers\nfor feature extraction, resulting in limited exploration of spectral\ninformation and difficulties in achieving complementary representations of\nobject features. In this paper, a spatial-spectral fusion network with spectral\nangle awareness (SST-Net) is proposed for hyperspectral (HS) object tracking.\nFirstly, to address the issue of insufficient spectral feature extraction in\nexisting networks, a spatial-spectral feature backbone ($S^2$FB) is designed.\nWith the spatial and spectral extraction branch, a joint representation of\ntexture and spectrum is obtained. Secondly, a spectral attention fusion module\n(SAFM) is presented to capture the intra- and inter-modality correlation to\nobtain the fused features from the HS and RGB modalities. It can incorporate\nthe visual information into the HS spectral context to form a robust\nrepresentation. Thirdly, to ensure a more accurate response of the tracker to\nthe object position, a spectral angle awareness module (SAAM) investigates the\nregion-level spectral similarity between the template and search images during\nthe prediction stage. Furthermore, we develop a novel spectral angle awareness\nloss (SAAL) to offer guidance for the SAAM based on similar regions. Finally,\nto obtain the robust tracking results, a weighted prediction method is\nconsidered to combine the HS and RGB predicted motions of objects to leverage\nthe strengths of each modality. Extensive experiments on the HOTC dataset\ndemonstrate the effectiveness of the proposed SSF-Net, compared with\nstate-of-the-art trackers.\n"
    },
    {
        "title": "MirrorAttack: Backdoor Attack on 3D Point Cloud with a Distorting Mirror",
        "published_time": "2024-03-09T09:15:37Z",
        "abstract": "  The widespread deployment of Deep Neural Networks (DNNs) for 3D point cloud\nprocessing starkly contrasts with their susceptibility to security breaches,\nnotably backdoor attacks. These attacks hijack DNNs during training, embedding\ntriggers in the data that, once activated, cause the network to make\npredetermined errors while maintaining normal performance on unaltered data.\nThis vulnerability poses significant risks, especially given the insufficient\nresearch on robust defense mechanisms for 3D point cloud networks against such\nsophisticated threats. Existing attacks either struggle to resist basic point\ncloud pre-processing methods, or rely on delicate manual design. Exploring\nsimple, effective, imperceptible, and difficult-to-defend triggers in 3D point\nclouds is still challenging.To address these challenges, we introduce\nMirrorAttack, a novel effective 3D backdoor attack method, which implants the\ntrigger by simply reconstructing a clean point cloud with an auto-encoder. The\ndata-driven nature of the MirrorAttack obviates the need for complex manual\ndesign. Minimizing the reconstruction loss automatically improves\nimperceptibility. Simultaneously, the reconstruction network endows the trigger\nwith pronounced nonlinearity and sample specificity, rendering traditional\npreprocessing techniques ineffective in eliminating it. A trigger smoothing\nmodule based on spherical harmonic transformation is also attached to regulate\nthe intensity of the attack.Both quantitive and qualitative results verify the\neffectiveness of our method. We achieve state-of-the-art ASR on different types\nof victim models with the intervention of defensive techniques. Moreover, the\nminimal perturbation introduced by our trigger, as assessed by various metrics,\nattests to the method's stealth, ensuring its imperceptibility.\n"
    },
    {
        "title": "Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines",
        "published_time": "2024-03-09T09:11:49Z",
        "abstract": "  Text-to-image diffusion models (T2I) use a latent representation of a text\nprompt to guide the image generation process. However, the process by which the\nencoder produces the text representation is unknown. We propose the Diffusion\nLens, a method for analyzing the text encoder of T2I models by generating\nimages from its intermediate representations. Using the Diffusion Lens, we\nperform an extensive analysis of two recent T2I models. Exploring compound\nprompts, we find that complex scenes describing multiple objects are composed\nprogressively and more slowly compared to simple scenes; Exploring knowledge\nretrieval, we find that representation of uncommon concepts requires further\ncomputation compared to common concepts, and that knowledge retrieval is\ngradual across layers. Overall, our findings provide valuable insights into the\ntext encoder component in T2I pipelines.\n"
    },
    {
        "title": "Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline",
        "published_time": "2024-03-09T08:49:50Z",
        "abstract": "  Current event-/frame-event based trackers undergo evaluation on short-term\ntracking datasets, however, the tracking of real-world scenarios involves\nlong-term tracking, and the performance of existing tracking algorithms in\nthese scenarios remains unclear. In this paper, we first propose a new\nlong-term and large-scale frame-event single object tracking dataset, termed\nFELT. It contains 742 videos and 1,594,474 RGB frames and event stream pairs\nand has become the largest frame-event tracking dataset to date. We re-train\nand evaluate 15 baseline trackers on our dataset for future works to compare.\nMore importantly, we find that the RGB frames and event streams are naturally\nincomplete due to the influence of challenging factors and spatially sparse\nevent flow. In response to this, we propose a novel associative memory\nTransformer network as a unified backbone by introducing modern Hopfield layers\ninto multi-head self-attention blocks to fuse both RGB and event data.\nExtensive experiments on both FELT and RGB-T tracking dataset LasHeR fully\nvalidated the effectiveness of our model. The dataset and source code can be\nfound at \\url{https://github.com/Event-AHU/FELT_SOT_Benchmark}.\n"
    },
    {
        "title": "SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object\n  Detection",
        "published_time": "2024-03-09T06:48:19Z",
        "abstract": "  LiDAR-based 3D object detection plays an essential role in autonomous\ndriving. Existing high-performing 3D object detectors usually build dense\nfeature maps in the backbone network and prediction head. However, the\ncomputational costs introduced by the dense feature maps grow quadratically as\nthe perception range increases, making these models hard to scale up to\nlong-range detection. Some recent works have attempted to construct fully\nsparse detectors to solve this issue; nevertheless, the resulting models either\nrely on a complex multi-stage pipeline or exhibit inferior performance. In this\nwork, we propose SAFDNet, a straightforward yet highly effective architecture,\ntailored for fully sparse 3D object detection. In SAFDNet, an adaptive feature\ndiffusion strategy is designed to address the center feature missing problem.\nWe conducted extensive experiments on Waymo Open, nuScenes, and Argoverse2\ndatasets. SAFDNet performed slightly better than the previous SOTA on the first\ntwo datasets but much better on the last dataset, which features long-range\ndetection, verifying the efficacy of SAFDNet in scenarios where long-range\ndetection is required. Notably, on Argoverse2, SAFDNet surpassed the previous\nbest hybrid detector HEDNet by 2.6% mAP while being 2.1x faster, and yielded\n2.1% mAP gains over the previous best sparse detector FSDv2 while being 1.3x\nfaster. The code will be available at https://github.com/zhanggang001/HEDNet.\n"
    },
    {
        "title": "Recurrent Aligned Network for Generalized Pedestrian Trajectory\n  Prediction",
        "published_time": "2024-03-09T06:17:09Z",
        "abstract": "  Pedestrian trajectory prediction is a crucial component in computer vision\nand robotics, but remains challenging due to the domain shift problem. Previous\nstudies have tried to tackle this problem by leveraging a portion of the\ntrajectory data from the target domain to adapt the model. However, such domain\nadaptation methods are impractical in real-world scenarios, as it is infeasible\nto collect trajectory data from all potential target domains. In this paper, we\nstudy a task named generalized pedestrian trajectory prediction, with the aim\nof generalizing the model to unseen domains without accessing their\ntrajectories. To tackle this task, we introduce a Recurrent Aligned\nNetwork~(RAN) to minimize the domain gap through domain alignment.\nSpecifically, we devise a recurrent alignment module to effectively align the\ntrajectory feature spaces at both time-state and time-sequence levels by the\nrecurrent alignment strategy.Furthermore, we introduce a pre-aligned\nrepresentation module to combine social interactions with the recurrent\nalignment strategy, which aims to consider social interactions during the\nalignment process instead of just target trajectories. We extensively evaluate\nour method and compare it with state-of-the-art methods on three widely used\nbenchmarks. The experimental results demonstrate the superior generalization\ncapability of our method. Our work not only fills the gap in the generalization\nsetting for practical pedestrian trajectory prediction but also sets strong\nbaselines in this field.\n"
    },
    {
        "title": "Adaptive Multi-modal Fusion of Spatially Variant Kernel Refinement with\n  Diffusion Model for Blind Image Super-Resolution",
        "published_time": "2024-03-09T06:01:25Z",
        "abstract": "  Pre-trained diffusion models utilized for image generation encapsulate a\nsubstantial reservoir of a priori knowledge pertaining to intricate textures.\nHarnessing the potential of leveraging this a priori knowledge in the context\nof image super-resolution presents a compelling avenue. Nonetheless, prevailing\ndiffusion-based methodologies presently overlook the constraints imposed by\ndegradation information on the diffusion process. Furthermore, these methods\nfail to consider the spatial variability inherent in the estimated blur kernel,\nstemming from factors such as motion jitter and out-of-focus elements in\nopen-environment scenarios. This oversight results in a notable deviation of\nthe image super-resolution effect from fundamental realities. To address these\nconcerns, we introduce a framework known as Adaptive Multi-modal Fusion of\n\\textbf{S}patially Variant Kernel Refinement with Diffusion Model for Blind\nImage \\textbf{S}uper-\\textbf{R}esolution (SSR). Within the SSR framework, we\npropose a Spatially Variant Kernel Refinement (SVKR) module. SVKR estimates a\nDepth-Informed Kernel, which takes the depth information into account and is\nspatially variant. Additionally, SVKR enhance the accuracy of depth information\nacquired from LR images, allowing for mutual enhancement between the depth map\nand blur kernel estimates. Finally, we introduce the Adaptive Multi-Modal\nFusion (AMF) module to align the information from three modalities:\nlow-resolution images, depth maps, and blur kernels. This alignment can\nconstrain the diffusion model to generate more authentic SR results.\nQuantitative and qualitative experiments affirm the superiority of our\napproach, while ablation experiments corroborate the effectiveness of the\nmodules we have proposed.\n"
    },
    {
        "title": "A self-supervised CNN for image watermark removal",
        "published_time": "2024-03-09T05:59:48Z",
        "abstract": "  Popular convolutional neural networks mainly use paired images in a\nsupervised way for image watermark removal. However, watermarked images do not\nhave reference images in the real world, which results in poor robustness of\nimage watermark removal techniques. In this paper, we propose a self-supervised\nconvolutional neural network (CNN) in image watermark removal (SWCNN). SWCNN\nuses a self-supervised way to construct reference watermarked images rather\nthan given paired training samples, according to watermark distribution. A\nheterogeneous U-Net architecture is used to extract more complementary\nstructural information via simple components for image watermark removal.\nTaking into account texture information, a mixed loss is exploited to improve\nvisual effects of image watermark removal. Besides, a watermark dataset is\nconducted. Experimental results show that the proposed SWCNN is superior to\npopular CNNs in image watermark removal.\n"
    },
    {
        "title": "And Then the Hammer Broke: Reflections on Machine Ethics from Feminist\n  Philosophy of Science",
        "published_time": "2024-03-09T05:50:32Z",
        "abstract": "  Vision is an important metaphor in ethical and political questions of\nknowledge. The feminist philosopher Donna Haraway points out the ``perverse''\nnature of an intrusive, alienating, all-seeing vision (to which we might cry\nout ``stop looking at me!''), but also encourages us to embrace the embodied\nnature of sight and its promises for genuinely situated knowledge. Current\ntechnologies of machine vision -- surveillance cameras, drones (for war or\nrecreation), iPhone cameras -- are usually construed as instances of the former\nrather than the latter, and for good reasons. However, although in no way\nattempting to diminish the real suffering these technologies have brought about\nin the world, I make the case for understanding technologies of computer vision\nas material instances of embodied seeing and situated knowing. Furthermore,\nborrowing from Iris Murdoch's concept of moral vision, I suggest that these\ntechnologies direct our labor towards self-reflection in ethically significant\nways. My approach draws upon paradigms in computer vision research,\nphenomenology, and feminist epistemology. Ultimately, this essay is an argument\nfor directing more philosophical attention from merely criticizing technologies\nof vision as ethically deficient towards embracing them as complex,\nmethodologically and epistemologically important objects.\n"
    },
    {
        "title": "Weakly Supervised Change Detection via Knowledge Distillation and\n  Multiscale Sigmoid Inference",
        "published_time": "2024-03-09T05:01:51Z",
        "abstract": "  Change detection, which aims to detect spatial changes from a pair of\nmulti-temporal images due to natural or man-made causes, has been widely\napplied in remote sensing, disaster management, urban management, etc. Most\nexisting change detection approaches, however, are fully supervised and require\nlabor-intensive pixel-level labels. To address this, we develop a novel weakly\nsupervised change detection technique via Knowledge Distillation and Multiscale\nSigmoid Inference (KD-MSI) that leverages image-level labels. In our approach,\nthe Class Activation Maps (CAM) are utilized not only to derive a change\nprobability map but also to serve as a foundation for the knowledge\ndistillation process. This is done through a joint training strategy of the\nteacher and student networks, enabling the student network to highlight\npotential change areas more accurately than teacher network based on\nimage-level labels. Moreover, we designed a Multiscale Sigmoid Inference (MSI)\nmodule as a post processing step to further refine the change probability map\nfrom the trained student network. Empirical results on three public datasets,\ni.e., WHU-CD, DSIFN-CD, and LEVIR-CD, demonstrate that our proposed technique,\nwith its integrated training strategy, significantly outperforms the\nstate-of-the-art.\n"
    },
    {
        "title": "Hair and scalp disease detection using deep learning",
        "published_time": "2024-03-09T04:49:40Z",
        "abstract": "  In recent years, there has been a notable advancement in the integration of\nhealthcare and technology, particularly evident in the field of medical image\nanalysis. This paper introduces a pioneering approach in dermatology,\npresenting a robust method for the detection of hair and scalp diseases using\nstate-of-the-art deep learning techniques. Our methodology relies on\nConvolutional Neural Networks (CNNs), well-known for their efficacy in image\nrecognition, to meticulously analyze images for various dermatological\nconditions affecting the hair and scalp. Our proposed system represents a\nsignificant advancement in dermatological diagnostics, offering a non-invasive\nand highly efficient means of early detection and diagnosis. By leveraging the\ncapabilities of CNNs, our model holds the potential to revolutionize\ndermatology, providing accessible and timely healthcare solutions. Furthermore,\nthe seamless integration of our trained model into a web-based platform\ndeveloped with the Django framework ensures broad accessibility and usability,\ndemocratizing advanced medical diagnostics. The integration of machine learning\nalgorithms into web applications marks a pivotal moment in healthcare delivery,\npromising empowerment for both healthcare providers and patients. Through the\nsynergy between technology and healthcare, our paper outlines the meticulous\nmethodology, technical intricacies, and promising future prospects of our\nsystem. With a steadfast commitment to advancing healthcare frontiers, our goal\nis to significantly contribute to leveraging technology for improved healthcare\noutcomes globally. This endeavor underscores the profound impact of\ntechnological innovation in shaping the future of healthcare delivery and\npatient care, highlighting the transformative potential of our approach.\n"
    },
    {
        "title": "DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM",
        "published_time": "2024-01-03T05:42:17Z",
        "abstract": "  SLAM systems based on NeRF have demonstrated superior performance in\nrendering quality and scene reconstruction for static environments compared to\ntraditional dense SLAM. However, they encounter tracking drift and mapping\nerrors in real-world scenarios with dynamic interferences. To address these\nissues, we introduce DDN-SLAM, the first real-time dense dynamic neural\nimplicit SLAM system integrating semantic features. To address dynamic tracking\ninterferences, we propose a feature point segmentation method that combines\nsemantic features with a mixed Gaussian distribution model. To avoid incorrect\nbackground removal, we propose a mapping strategy based on sparse point cloud\nsampling and background restoration. We propose a dynamic semantic loss to\neliminate dynamic occlusions. Experimental results demonstrate that DDN-SLAM is\ncapable of robustly tracking and producing high-quality reconstructions in\ndynamic environments, while appropriately preserving potential dynamic objects.\nCompared to existing neural implicit SLAM systems, the tracking results on\ndynamic datasets indicate an average 90% improvement in Average Trajectory\nError (ATE) accuracy.\n"
    },
    {
        "title": "Dynamic Policy-Driven Adaptive Multi-Instance Learning for Whole Slide\n  Image Classification",
        "published_time": "2024-03-09T04:43:24Z",
        "abstract": "  Multi-Instance Learning (MIL) has shown impressive performance for\nhistopathology whole slide image (WSI) analysis using bags or pseudo-bags. It\ninvolves instance sampling, feature representation, and decision-making.\nHowever, existing MIL-based technologies at least suffer from one or more of\nthe following problems: 1) requiring high storage and intensive pre-processing\nfor numerous instances (sampling); 2) potential over-fitting with limited\nknowledge to predict bag labels (feature representation); 3) pseudo-bag counts\nand prior biases affect model robustness and generalizability\n(decision-making). Inspired by clinical diagnostics, using the past sampling\ninstances can facilitate the final WSI analysis, but it is barely explored in\nprior technologies. To break free these limitations, we integrate the dynamic\ninstance sampling and reinforcement learning into a unified framework to\nimprove the instance selection and feature aggregation, forming a novel Dynamic\nPolicy Instance Selection (DPIS) scheme for better and more credible\ndecision-making. Specifically, the measurement of feature distance and reward\nfunction are employed to boost continuous instance sampling. To alleviate the\nover-fitting, we explore the latent global relations among instances for more\nrobust and discriminative feature representation while establishing reward and\npunishment mechanisms to correct biases in pseudo-bags using contrastive\nlearning. These strategies form the final Dynamic Policy-Driven Adaptive\nMulti-Instance Learning (PAMIL) method for WSI tasks. Extensive experiments\nreveal that our PAMIL method outperforms the state-of-the-art by 3.8\\% on\nCAMELYON16 and 4.4\\% on TCGA lung cancer datasets.\n"
    },
    {
        "title": "uniGradICON: A Foundation Model for Medical Image Registration",
        "published_time": "2024-03-09T03:26:35Z",
        "abstract": "  Conventional medical image registration approaches directly optimize over the\nparameters of a transformation model. These approaches have been highly\nsuccessful and are used generically for registrations of different anatomical\nregions. Recent deep registration networks are incredibly fast and accurate but\nare only trained for specific tasks. Hence, they are no longer generic\nregistration approaches. We therefore propose uniGradICON, a first step toward\na foundation model for registration providing 1) great performance\n\\emph{across} multiple datasets which is not feasible for current\nlearning-based registration methods, 2) zero-shot capabilities for new\nregistration tasks suitable for different acquisitions, anatomical regions, and\nmodalities compared to the training dataset, and 3) a strong initialization for\nfinetuning on out-of-distribution registration tasks. UniGradICON unifies the\nspeed and accuracy benefits of learning-based registration algorithms with the\ngeneric applicability of conventional non-deep-learning approaches. We\nextensively trained and evaluated uniGradICON on twelve different public\ndatasets. Our code and the uniGradICON model are available at\nhttps://github.com/uncbiag/uniGradICON.\n"
    },
    {
        "title": "Pan-Mamba: Effective pan-sharpening with State Space Model",
        "published_time": "2024-02-19T14:54:54Z",
        "abstract": "  Pan-sharpening involves integrating information from low-resolution\nmulti-spectral and high-resolution panchromatic images to generate\nhigh-resolution multi-spectral counterparts. While recent advancements in the\nstate space model, particularly the efficient long-range dependency modeling\nachieved by Mamba, have revolutionized computer vision community, its untapped\npotential in pan-sharpening motivates our exploration. Our contribution,\nPan-Mamba, represents a novel pan-sharpening network that leverages the\nefficiency of the Mamba model in global information modeling. In Pan-Mamba, we\ncustomize two core components: channel swapping Mamba and cross-modal Mamba,\nstrategically designed for efficient cross-modal information exchange and\nfusion. The former initiates a lightweight cross-modal interaction through the\nexchange of partial panchromatic and multi-spectral channels, while the latter\nfacilities the information representation capability by exploiting inherent\ncross-modal relationships. Through extensive experiments across diverse\ndatasets, our proposed approach surpasses state-of-the-art methods, showcasing\nsuperior fusion results in pan-sharpening. To the best of our knowledge, this\nwork is the first attempt in exploring the potential of the Mamba model and\nestablishes a new frontier in the pan-sharpening techniques. The source code is\navailable at \\url{https://github.com/alexhe101/Pan-Mamba}.\n"
    },
    {
        "title": "Unveiling Ancient Maya Settlements Using Aerial LiDAR Image Segmentation",
        "published_time": "2024-03-09T02:59:48Z",
        "abstract": "  Manual identification of archaeological features in LiDAR imagery is\nlabor-intensive, costly, and requires archaeological expertise. This paper\nshows how recent advancements in deep learning (DL) present efficient solutions\nfor accurately segmenting archaeological structures in aerial LiDAR images\nusing the YOLOv8 neural network. The proposed approach uses novel\npre-processing of the raw LiDAR data and dataset augmentation methods to\nproduce trained YOLOv8 networks to improve accuracy, precision, and recall for\nthe segmentation of two important Maya structure types: annular structures and\nplatforms. The results show an IoU performance of 0.842 for platforms and 0.809\nfor annular structures which outperform existing approaches. Further, analysis\nvia domain experts considers the topological consistency of segmented regions\nand performance vs. area providing important insights. The approach automates\ntime-consuming LiDAR image labeling which significantly accelerates accurate\nanalysis of historical landscapes.\n"
    },
    {
        "title": "Towards Deviation-Robust Agent Navigation via Perturbation-Aware\n  Contrastive Learning",
        "published_time": "2024-03-09T02:34:13Z",
        "abstract": "  Vision-and-language navigation (VLN) asks an agent to follow a given language\ninstruction to navigate through a real 3D environment. Despite significant\nadvances, conventional VLN agents are trained typically under disturbance-free\nenvironments and may easily fail in real-world scenarios, since they are\nunaware of how to deal with various possible disturbances, such as sudden\nobstacles or human interruptions, which widely exist and may usually cause an\nunexpected route deviation. In this paper, we present a model-agnostic training\nparadigm, called Progressive Perturbation-aware Contrastive Learning (PROPER)\nto enhance the generalization ability of existing VLN agents, by requiring them\nto learn towards deviation-robust navigation. Specifically, a simple yet\neffective path perturbation scheme is introduced to implement the route\ndeviation, with which the agent is required to still navigate successfully\nfollowing the original instruction. Since directly enforcing the agent to learn\nperturbed trajectories may lead to inefficient training, a progressively\nperturbed trajectory augmentation strategy is designed, where the agent can\nself-adaptively learn to navigate under perturbation with the improvement of\nits navigation performance for each specific trajectory. For encouraging the\nagent to well capture the difference brought by perturbation, a\nperturbation-aware contrastive learning mechanism is further developed by\ncontrasting perturbation-free trajectory encodings and perturbation-based\ncounterparts. Extensive experiments on R2R show that PROPER can benefit\nmultiple VLN baselines in perturbation-free scenarios. We further collect the\nperturbed path data to construct an introspection subset based on the R2R,\ncalled Path-Perturbed R2R (PP-R2R). The results on PP-R2R show unsatisfying\nrobustness of popular VLN agents and the capability of PROPER in improving the\nnavigation robustness.\n"
    },
    {
        "title": "Deep Contrastive Multi-view Clustering under Semantic Feature Guidance",
        "published_time": "2024-03-09T02:33:38Z",
        "abstract": "  Contrastive learning has achieved promising performance in the field of\nmulti-view clustering recently. However, the positive and negative sample\nconstruction mechanisms ignoring semantic consistency lead to false negative\npairs, limiting the performance of existing algorithms from further\nimprovement. To solve this problem, we propose a multi-view clustering\nframework named Deep Contrastive Multi-view Clustering under Semantic feature\nguidance (DCMCS) to alleviate the influence of false negative pairs.\nSpecifically, view-specific features are firstly extracted from raw features\nand fused to obtain fusion view features according to view importance. To\nmitigate the interference of view-private information, specific view and fusion\nview semantic features are learned by cluster-level contrastive learning and\nconcatenated to measure the semantic similarity of instances. By minimizing\ninstance-level contrastive loss weighted by semantic similarity, DCMCS\nadaptively weakens contrastive leaning between false negative pairs.\nExperimental results on several public datasets demonstrate the proposed\nframework outperforms the state-of-the-art methods.\n"
    }
]