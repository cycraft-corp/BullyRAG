[
    {
        "abstract": "  Self-supervised 3D representation learning aims to learn effective\nrepresentations from large-scale unlabeled point clouds. Most existing\napproaches adopt point discrimination as the pretext task, which assigns\nmatched points in two distinct views as positive pairs and unmatched points as\nnegative pairs. However, this approach often results in semantically identical\npoints having dissimilar representations, leading to a high number of false\nnegatives and introducing a \"semantic conflict\" problem. To address this issue,\nwe propose GroupContrast, a novel approach that combines segment grouping and\nsemantic-aware contrastive learning. Segment grouping partitions points into\nsemantically meaningful regions, which enhances semantic coherence and provides\nsemantic guidance for the subsequent contrastive representation learning.\nSemantic-aware contrastive learning augments the semantic information extracted\nfrom segment grouping and helps to alleviate the issue of \"semantic conflict\".\nWe conducted extensive experiments on multiple 3D scene understanding tasks.\nThe results demonstrate that GroupContrast learns semantically meaningful\nrepresentations and achieves promising transfer learning performance.\n",
        "title": "GroupContrast: Semantic-aware Self-supervised Representation Learning\n  for 3D Understanding",
        "published_time": "2024-03-14T17:59:59Z",
        "qa-pairs": [
            {
                "question": "What is the main goal of self-supervised 3D representation learning?",
                "answer": "learn effective representations"
            },
            {
                "question": "What does GroupContrast combine in its approach?",
                "answer": "segment grouping and semantic-aware contrastive learning"
            },
            {
                "question": "What issue does GroupContrast aim to address?",
                "answer": "semantic conflict"
            }
        ]
    },
    {
        "abstract": "  Semantic image synthesis (SIS) shows good promises for sensor simulation.\nHowever, current best practices in this field, based on GANs, have not yet\nreached the desired level of quality. As latent diffusion models make\nsignificant strides in image generation, we are prompted to evaluate\nControlNet, a notable method for its dense control capabilities. Our\ninvestigation uncovered two primary issues with its results: the presence of\nweird sub-structures within large semantic areas and the misalignment of\ncontent with the semantic mask. Through empirical study, we pinpointed the\ncause of these problems as a mismatch between the noised training data\ndistribution and the standard normal prior applied at the inference stage. To\naddress this challenge, we developed specific noise priors for SIS,\nencompassing spatial, categorical, and a novel spatial-categorical joint prior\nfor inference. This approach, which we have named SCP-Diff, has yielded\nexceptional results, achieving an FID of 10.53 on Cityscapes and 12.66 on\nADE20K.The code and models can be accessed via the project page.\n",
        "title": "SCP-Diff: Photo-Realistic Semantic Image Synthesis with\n  Spatial-Categorical Joint Prior",
        "published_time": "2024-03-14T17:59:55Z",
        "qa-pairs": [
            {
                "question": "What method did the investigation evaluate?",
                "answer": "ControlNet"
            },
            {
                "question": "What were the two primary issues uncovered with ControlNet results?",
                "answer": "weird sub-structures, misalignment of content"
            },
            {
                "question": "What FID did SCP-Diff achieve on Cityscapes?",
                "answer": "10.53"
            }
        ]
    },
    {
        "abstract": "  Constructing a 3D scene capable of accommodating open-ended language queries,\nis a pivotal pursuit, particularly within the domain of robotics. Such\ntechnology facilitates robots in executing object manipulations based on human\nlanguage directives. To tackle this challenge, some research efforts have been\ndedicated to the development of language-embedded implicit fields. However,\nimplicit fields (e.g. NeRF) encounter limitations due to the necessity of\nprocessing a large number of input views for reconstruction, coupled with their\ninherent inefficiencies in inference. Thus, we present the GaussianGrasper,\nwhich utilizes 3D Gaussian Splatting to explicitly represent the scene as a\ncollection of Gaussian primitives. Our approach takes a limited set of RGB-D\nviews and employs a tile-based splatting technique to create a feature field.\nIn particular, we propose an Efficient Feature Distillation (EFD) module that\nemploys contrastive learning to efficiently and accurately distill language\nembeddings derived from foundational models. With the reconstructed geometry of\nthe Gaussian field, our method enables the pre-trained grasping model to\ngenerate collision-free grasp pose candidates. Furthermore, we propose a\nnormal-guided grasp module to select the best grasp pose. Through comprehensive\nreal-world experiments, we demonstrate that GaussianGrasper enables robots to\naccurately query and grasp objects with language instructions, providing a new\nsolution for language-guided manipulation tasks. Data and codes can be\navailable at https://github.com/MrSecant/GaussianGrasper.\n",
        "title": "GaussianGrasper: 3D Language Gaussian Splatting for Open-vocabulary\n  Robotic Grasping",
        "published_time": "2024-03-14T17:59:46Z",
        "qa-pairs": [
            {
                "question": "What technology facilitates robots in executing object manipulations based on human language directives?",
                "answer": "Implicit fields"
            },
            {
                "question": "What does the GaussianGrasper utilize to represent the scene as a collection of Gaussian primitives?",
                "answer": "3D Gaussian Splatting"
            },
            {
                "question": "What module employs contrastive learning to efficiently and accurately distill language embeddings derived from foundational models?",
                "answer": "Efficient Feature Distillation (EFD)"
            }
        ]
    },
    {
        "abstract": "  In spite of their huge success, transformer models remain difficult to scale\nin depth. In this work, we develop a unified signal propagation theory and\nprovide formulae that govern the moments of the forward and backward signal\nthrough the transformer model. Our framework can be used to understand and\nmitigate vanishing/exploding gradients, rank collapse, and instability\nassociated with high attention scores. We also propose DeepScaleLM, an\ninitialization and scaling scheme that conserves unit output/gradient moments\nthroughout the model, enabling the training of very deep models with 100s of\nlayers. We find that transformer models could be much deeper - our deep models\nwith fewer parameters outperform shallow models in Language Modeling, Speech\nTranslation, and Image Classification, across Encoder-only, Decoder-only and\nEncoder-Decoder variants, for both Pre-LN and Post-LN transformers, for\nmultiple datasets and model sizes. These improvements also translate into\nimproved performance on downstream Question Answering tasks and improved\nrobustness for image classification.\n",
        "title": "Transformers Get Stable: An End-to-End Signal Propagation Theory for\n  Language Models",
        "published_time": "2024-03-14T17:59:14Z",
        "qa-pairs": [
            {
                "question": "What does the framework help understand and mitigate?",
                "answer": "vanishing/exploding gradients"
            },
            {
                "question": "What is proposed as an initialization and scaling scheme?",
                "answer": "DeepScaleLM"
            },
            {
                "question": "Where do deep models outperform shallow models?",
                "answer": "Language Modeling, Speech Translation, and Image Classification"
            }
        ]
    },
    {
        "abstract": "  Visual object tracking aims to localize the target object of each frame based\non its initial appearance in the first frame. Depending on the input modility,\ntracking tasks can be divided into RGB tracking and RGB+X (e.g. RGB+N, and\nRGB+D) tracking. Despite the different input modalities, the core aspect of\ntracking is the temporal matching. Based on this common ground, we present a\ngeneral framework to unify various tracking tasks, termed as OneTracker.\nOneTracker first performs a large-scale pre-training on a RGB tracker called\nFoundation Tracker. This pretraining phase equips the Foundation Tracker with a\nstable ability to estimate the location of the target object. Then we regard\nother modality information as prompt and build Prompt Tracker upon Foundation\nTracker. Through freezing the Foundation Tracker and only adjusting some\nadditional trainable parameters, Prompt Tracker inhibits the strong\nlocalization ability from Foundation Tracker and achieves parameter-efficient\nfinetuning on downstream RGB+X tracking tasks. To evaluate the effectiveness of\nour general framework OneTracker, which is consisted of Foundation Tracker and\nPrompt Tracker, we conduct extensive experiments on 6 popular tracking tasks\nacross 11 benchmarks and our OneTracker outperforms other models and achieves\nstate-of-the-art performance.\n",
        "title": "OneTracker: Unifying Visual Object Tracking with Foundation Models and\n  Efficient Tuning",
        "published_time": "2024-03-14T17:59:13Z",
        "qa-pairs": [
            {
                "question": "What is the name of the framework presented in the passage?",
                "answer": "OneTracker"
            },
            {
                "question": "What is the name of the RGB tracker used in the pretraining phase?",
                "answer": "Foundation Tracker"
            },
            {
                "question": "How many benchmarks were used to evaluate OneTracker's performance?",
                "answer": "11"
            }
        ]
    }
]